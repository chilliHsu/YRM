{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data positive feedback dynamic (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "from sklearn.metrics import average_precision_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineNotifyMessage(token, msg):\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer \" + token, \n",
    "        \"Content-Type\" : \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "\n",
    "    payload = {'message': msg}\n",
    "    r = requests.post(\"https://notify-api.line.me/api/notify\", headers = headers, params = payload)\n",
    "    return r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_following = np.load('../Data/npy/user_following_1489.npy')\n",
    "#all_3374 = np.load('../Data/npy/all_2939D_img0.5.npy')\n",
    "all_3374 = np.load('../Data/npy/Following_64D.npy')\n",
    "user_category = np.load('../Data/npy/user_category_1489.npy')\n",
    "YouTuber_category = np.load('../Data/npy/YouTuber_category_0.7.npy')\n",
    "active_users = np.load('../Data/npy/active_userID_1489.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test_amount = 150\n",
    "yt_test_amount = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test(user_following,feature,user_category,item_category,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    print('user_category shape ',user_category.shape)\n",
    "    print('YouTuber_category shape ',item_category.shape)\n",
    "    user_category_norm = np.zeros(user_category.shape)\n",
    "    for i in range(len(user_category)):\n",
    "        user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "    print('user_category after normalized by max...')\n",
    "    print('user_category_norm shape ',user_category_norm.shape)\n",
    "    print('user cateogory norm',user_category_norm)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = item_category[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(5)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    test_pos = -1\n",
    "    for i in range(num_of_users):\n",
    "        t_for_train = []\n",
    "        f_for_train = []\n",
    "        if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    t_for_train.append(j)\n",
    "                else:\n",
    "                    f_for_train.append(j)\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "\n",
    "        else: #if in test id, choose 2 true and other \n",
    "            test_pos += 1\n",
    "            temp_t = []\n",
    "            temp_f = []\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    temp_t.append(j)\n",
    "                else:\n",
    "                    temp_f.append(j)\n",
    "            t_for_test = sorted(random.sample(temp_t,math.ceil(0.5*len(temp_t))))\n",
    "            f_for_test  = sorted(random.sample(temp_f,yt_test_amount-len(t_for_test)))\n",
    "\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,user_category_norm,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test_t, test_f, train_t, train_f, hopefully to get the similar score\n",
    "def generate_train_test(user_following,feature,user_category,item_category,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    print('user_category shape ',user_category.shape)\n",
    "    print('YouTuber_category shape ',item_category.shape)\n",
    "    user_category_norm = np.zeros(user_category.shape)\n",
    "    for i in range(len(user_category)):\n",
    "        user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "    print('user_category after normalized by max...')\n",
    "    print('user_category_norm shape ',user_category_norm.shape)\n",
    "    print('user cateogory norm',user_category_norm)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = item_category[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(5)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    print('test_idx',test_idx)\n",
    "    \n",
    "    data_split_path = 'D:/ChilliHsu/Data/Result_Matrix/Dims200_Text300/'\n",
    "    with open(data_split_path+'test_t.json') as json_file:\n",
    "        test_t = json.load(json_file)\n",
    "    with open(data_split_path+'test_f.json') as json_file:\n",
    "        test_f = json.load(json_file)\n",
    "    with open(data_split_path+'train_t.json') as json_file:\n",
    "        train_t = json.load(json_file)\n",
    "    with open(data_split_path+'train_f.json') as json_file:\n",
    "        train_f = json.load(json_file)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,user_category_norm,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't leave half positive for train, just put all testing user data into testing \n",
    "def generate_train_test(user_following,feature,user_category,item_category,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    print('user_category shape ',user_category.shape)\n",
    "    print('YouTuber_category shape ',item_category.shape)\n",
    "    user_category_norm = np.zeros(user_category.shape)\n",
    "    for i in range(len(user_category)):\n",
    "        user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "    print('user_category after normalized by max...')\n",
    "    print('user_category_norm shape ',user_category_norm.shape)\n",
    "    print('user cateogory norm',user_category_norm)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = item_category[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(33)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    test_pos = -1\n",
    "    for i in range(num_of_users):\n",
    "        t_for_train = []\n",
    "        f_for_train = []\n",
    "        if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    t_for_train.append(j)\n",
    "                else:\n",
    "                    f_for_train.append(j)\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "\n",
    "        else: #if in test id, choose 2 true and other \n",
    "            test_pos += 1\n",
    "            temp_t = []\n",
    "            temp_f = []\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    temp_t.append(j)\n",
    "                else:\n",
    "                    temp_f.append(j)\n",
    "            t_for_test = random.sample(temp_t,math.ceil(len(temp_t)))\n",
    "            f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,user_category_norm,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate_train_test----\n",
      "user_following shape  (1489, 88)\n",
      "feature shape  (88, 0)\n",
      "user_category shape  (1489, 17)\n",
      "YouTuber_category shape  (88, 17)\n",
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n",
      "user cateogory norm [[0.         1.         0.         ... 0.05714286 0.         0.        ]\n",
      " [0.24390244 0.         0.02439024 ... 0.         0.         0.09756098]\n",
      " [0.04210526 0.04210526 0.05263158 ... 0.02105263 0.         0.02105263]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01408451 0.01408451 0.04225352 ... 0.02816901 0.         0.02816901]\n",
      " [0.03703704 0.22222222 0.14814815 ... 0.         0.         0.        ]]\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "test_idx [3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "train_t,train_f,test_t,test_f,user_category_norm,test_idx = generate_train_test(user_following,all_3374,user_category,YouTuber_category,active_users,user_test_amount,yt_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 14, 17, 19, 20, 22, 41, 51, 56, 57, 59, 62, 78, 87],\n",
       " [17, 21, 26, 39, 42, 46, 50, 55, 58, 67, 68, 70, 77],\n",
       " [1, 5, 26, 37, 39, 42, 44, 48, 50, 51, 71, 83],\n",
       " [24, 38, 39, 50, 51, 52, 59, 64, 66, 72, 80, 82, 87],\n",
       " [3, 11, 22, 23, 29, 38, 40, 43, 59, 69, 76, 84],\n",
       " [6, 7, 17, 20, 24, 27, 28, 33, 46, 68, 69, 87],\n",
       " [1, 16, 31, 37, 42, 52, 55, 59, 61, 80],\n",
       " [18, 20, 44, 50, 52, 54, 60, 61, 69, 76, 79, 80],\n",
       " [0, 12, 14, 19, 20, 28, 44, 47, 51, 57, 66, 76, 78, 81],\n",
       " [2, 38, 39, 46, 47, 48, 62, 70, 76, 77],\n",
       " [1, 6, 18, 20, 44, 62, 73, 77],\n",
       " [2, 17, 18, 39, 41, 42, 47, 68, 71, 75, 86, 87],\n",
       " [0, 1, 18, 29, 32, 39, 62, 71],\n",
       " [0, 8, 19, 27, 38, 39, 41, 42, 53, 55, 60, 64, 65, 86],\n",
       " [3, 6, 8, 11, 20, 34, 39, 48, 62, 79, 81, 86],\n",
       " [6, 8, 14, 15, 18, 20, 21, 24, 30, 42, 57, 61, 76, 86],\n",
       " [1, 9, 12, 15, 19, 21, 43, 48, 60, 65, 72],\n",
       " [7, 9, 18, 20, 29, 31, 33, 34, 38, 49, 67, 73, 75, 76, 84],\n",
       " [2, 4, 10, 11, 15, 16, 22, 31, 44, 48, 50, 59, 65, 77, 86],\n",
       " [3, 6, 10, 16, 21, 30, 31, 58, 63, 72, 78, 81],\n",
       " [4, 20, 31, 41, 46, 48, 50, 51, 53, 54, 61, 77],\n",
       " [0, 5, 6, 13, 26, 28, 41, 47, 57, 63, 64, 65, 75, 83, 86],\n",
       " [9, 15, 20, 21, 26, 34, 37, 39, 52, 54, 65, 77, 85, 86],\n",
       " [3, 16, 20, 37, 46, 50, 52, 70, 73],\n",
       " [9, 12, 23, 27, 34, 42, 46, 53, 59, 69, 81, 84],\n",
       " [0, 4, 5, 7, 13, 23, 36, 47, 55, 59, 61, 63, 70, 83, 87],\n",
       " [4, 6, 7, 11, 31, 32, 39, 52, 54, 59, 62, 65, 78, 83, 85],\n",
       " [2, 13, 14, 15, 17, 24, 28, 31, 33, 45, 56, 61, 74, 75],\n",
       " [1, 12, 15, 17, 18, 31, 44, 53, 64, 68, 74, 78, 83],\n",
       " [6, 7, 10, 17, 34, 38, 58, 64, 67, 73, 78, 79, 82, 84, 87],\n",
       " [0, 2, 3, 6, 9, 13, 17, 19, 38, 42, 51, 56, 59, 78],\n",
       " [4, 9, 17, 18, 22, 36, 39, 42, 45, 52, 69, 77, 83, 84],\n",
       " [1, 2, 19, 30, 34, 38, 48, 51, 52, 61, 62, 74, 86, 87],\n",
       " [19, 20, 39, 53, 55, 68, 75],\n",
       " [0, 4, 6, 14, 17, 21, 28, 57, 60, 65, 68, 74, 81, 86],\n",
       " [27, 29, 32, 33, 37, 40, 48, 49, 53, 62, 70, 76, 78, 83, 87],\n",
       " [4, 13, 19, 22, 29, 34, 40, 45, 48, 52, 54, 55, 67, 78, 85],\n",
       " [1, 2, 3, 15, 30, 39, 49, 50, 60, 80],\n",
       " [2, 7, 9, 19, 28, 39, 49, 54, 56, 60, 69, 71, 73, 74, 78],\n",
       " [1, 4, 12, 14, 29, 34, 40, 44, 50, 72],\n",
       " [0, 14, 23, 27, 43, 46, 51, 52, 53, 58, 72, 75, 77, 82],\n",
       " [9, 10, 23, 26, 32, 38, 41, 45, 59, 60, 80, 83],\n",
       " [3, 12, 16, 21, 27, 40, 42, 48, 60, 62, 83],\n",
       " [9, 12, 18, 26, 31, 46, 55, 68, 70, 72, 77],\n",
       " [5, 20, 34, 56, 62, 69, 74, 84],\n",
       " [0, 12, 18, 19, 36, 54, 55, 58, 68, 73, 80, 81],\n",
       " [13, 18, 22, 31, 45, 51, 53, 57, 72, 75, 81],\n",
       " [6, 9, 12, 18, 23, 28, 30, 34, 54, 56, 63, 69, 75, 80, 86],\n",
       " [0, 2, 11, 17, 19, 36, 50, 51, 54, 64, 72, 78, 85],\n",
       " [1, 2, 4, 6, 11, 22, 39, 55, 60, 66, 71, 72, 73, 86],\n",
       " [1, 9, 19, 22, 26, 29, 40, 46, 50, 51, 53, 57, 58],\n",
       " [2, 10, 19, 26, 30, 55, 75, 82],\n",
       " [0, 4, 13, 16, 23, 39, 59, 76],\n",
       " [11, 13, 14, 16, 19, 29, 39, 40, 45, 50, 54, 70, 76, 86],\n",
       " [2, 3, 8, 19, 37, 40, 57, 70, 80, 81, 87],\n",
       " [3, 5, 9, 33, 41, 42, 47, 52, 59, 60, 68, 85, 86, 87],\n",
       " [7, 9, 12, 23, 30, 44, 54, 56, 66, 83, 86, 87],\n",
       " [0, 5, 7, 9, 13, 16, 19, 29, 47, 62, 68, 77, 80, 81, 86],\n",
       " [3, 16, 19, 22, 32, 33, 34, 36, 44, 50, 57, 62, 78, 81, 86],\n",
       " [3, 5, 17, 42, 57, 65, 81, 84],\n",
       " [5, 8, 11, 15, 35, 39, 54, 62, 73, 76, 83, 84, 87],\n",
       " [0, 12, 20, 27, 28, 34, 39, 42, 44, 51, 61, 62, 68, 71, 87],\n",
       " [2, 18, 19, 22, 23, 26, 46, 53, 57, 59, 60, 66, 68, 70, 87],\n",
       " [15, 23, 27, 29, 32, 34, 35, 41, 44, 57, 58, 69, 73, 76, 77],\n",
       " [7, 15, 16, 17, 26, 39, 44, 51, 52, 58, 82],\n",
       " [5, 9, 13, 26, 42, 44, 49, 58, 73, 81, 82, 84, 87],\n",
       " [3, 10, 14, 45, 49, 54, 56, 61, 69, 76, 81, 84],\n",
       " [6, 11, 13, 15, 30, 40, 46, 55, 62, 63, 66, 76, 77],\n",
       " [6, 9, 17, 24, 33, 37, 40, 42, 58, 61, 66, 70, 78],\n",
       " [14, 15, 16, 20, 24, 30, 34, 36, 65, 68, 86],\n",
       " [3, 6, 14, 17, 29, 36, 43, 49, 51, 69, 70],\n",
       " [0, 4, 19, 20, 23, 27, 39, 40, 51, 57, 66, 73, 83, 84],\n",
       " [0, 1, 23, 45, 47, 51, 58, 64, 66, 68, 69, 72, 73, 81, 85],\n",
       " [1, 16, 18, 24, 29, 33, 34, 39, 55, 57, 59, 68, 71, 84],\n",
       " [0, 8, 13, 30, 35, 41, 49, 57, 58, 68, 72, 75, 76, 79, 86],\n",
       " [3, 9, 13, 23, 25, 41, 62, 64, 65, 66, 68, 73, 87],\n",
       " [0, 4, 8, 12, 13, 25, 28, 31, 47, 51, 54, 60, 69, 73, 85],\n",
       " [4, 16, 18, 20, 28, 34, 42, 59, 62, 63, 68],\n",
       " [6, 23, 29, 30, 34, 37, 47, 59, 60, 62, 81, 84],\n",
       " [0, 15, 21, 49, 54, 63, 69, 86, 87],\n",
       " [0, 9, 23, 36, 48, 53, 56, 58, 60, 63, 68, 75, 81, 85],\n",
       " [10, 11, 13, 15, 33, 47, 48, 60, 71, 75, 78, 81],\n",
       " [5, 12, 17, 21, 31, 42, 44, 61, 66, 69, 72, 73, 84],\n",
       " [9, 11, 12, 20, 32, 38, 39, 49, 53, 58, 68, 77, 79, 81, 85],\n",
       " [4, 7, 13, 15, 34, 41, 47, 51, 65, 68, 72, 84],\n",
       " [8, 13, 17, 18, 29, 58, 61, 63, 66, 76, 77],\n",
       " [1, 5, 6, 9, 15, 20, 25, 43, 50, 57, 61, 83, 84, 85],\n",
       " [4, 21, 32, 39, 40, 46, 54, 59, 62, 64, 72, 74, 77, 86],\n",
       " [12, 16, 18, 36, 37, 43, 46, 50, 52, 54, 59, 60, 63, 83, 84],\n",
       " [1, 17, 21, 25, 27, 36, 46, 59, 60, 64, 68, 77],\n",
       " [3, 12, 14, 24, 31, 40, 44, 45, 52, 65, 66, 73, 75, 80, 87],\n",
       " [2, 4, 5, 27, 28, 30, 36, 43, 45, 50, 62, 64, 67, 71, 78],\n",
       " [9, 16, 23, 34, 35, 37, 42, 48, 49, 55, 58, 86],\n",
       " [0, 4, 7, 13, 30, 32, 36, 39, 43, 54, 67, 69, 78, 80],\n",
       " [0, 34, 37, 41, 48, 49, 53, 55, 58, 72, 77, 84],\n",
       " [0, 13, 30, 31, 32, 34, 35, 44, 57, 86],\n",
       " [11, 13, 16, 26, 32, 36, 45, 50, 64, 68, 69, 71, 85],\n",
       " [1, 13, 18, 21, 22, 25, 36, 40, 45, 51, 60, 63, 74, 82, 86],\n",
       " [3, 11, 15, 21, 29, 35, 38, 39, 42, 45, 51, 57, 75, 83],\n",
       " [5, 7, 8, 22, 35, 52, 54, 55, 60, 65, 66, 73, 81, 83],\n",
       " [7, 24, 26, 34, 42, 53, 55, 59, 61, 68, 69, 80, 82],\n",
       " [1, 12, 18, 21, 33, 47, 66, 69, 78, 85],\n",
       " [2, 5, 9, 17, 22, 25, 30, 45, 46, 57, 58, 59, 66, 70, 72],\n",
       " [1, 5, 6, 28, 30, 33, 36, 39, 42, 45, 46, 69, 77, 82],\n",
       " [0, 8, 15, 19, 28, 30, 34, 41, 43, 47, 54, 56, 62, 64, 75],\n",
       " [5, 14, 16, 19, 33, 34, 36, 41, 49, 67, 73, 75, 78, 84],\n",
       " [0, 5, 12, 18, 30, 42, 59, 60, 65, 66],\n",
       " [0, 13, 20, 22, 38, 41, 49, 58, 59, 62, 63, 71, 76, 77],\n",
       " [10, 14, 15, 29, 35, 36, 52, 55, 60, 68, 73, 75, 76],\n",
       " [1, 18, 29, 32, 36, 43, 46, 54, 55, 63, 68, 77, 78, 86],\n",
       " [6, 7, 23, 27, 34, 44, 63, 64, 68, 81, 82, 86],\n",
       " [18, 21, 26, 30, 35, 56, 68, 69, 71, 73, 75],\n",
       " [5, 27, 35, 43, 47, 52, 56, 77, 78, 80, 85],\n",
       " [4, 6, 13, 24, 27, 30, 36, 45, 49, 54, 55, 56, 62, 70, 83],\n",
       " [5, 16, 22, 34, 41, 46, 48, 51, 56, 58, 61, 83],\n",
       " [16, 23, 39, 47, 51, 55, 80, 84, 87],\n",
       " [2, 3, 13, 21, 25, 26, 33, 35, 44, 52, 55, 58, 69, 80, 81],\n",
       " [16, 19, 22, 25, 28, 30, 38, 42, 48, 53, 59, 66, 73, 80],\n",
       " [4, 10, 11, 15, 34, 37, 40, 45, 47, 56, 68, 76, 87],\n",
       " [4, 15, 16, 18, 26, 35, 44, 49, 50, 56, 64, 68, 72, 85],\n",
       " [0, 6, 9, 15, 17, 29, 31, 38, 53, 62, 63, 77, 85],\n",
       " [14, 18, 31, 42, 52, 61, 69, 70, 72, 83, 85, 87],\n",
       " [5, 22, 32, 45, 49, 53, 59, 64, 68, 69, 84, 87],\n",
       " [3, 4, 11, 17, 19, 27, 39, 46, 47, 48, 57, 62, 73, 85],\n",
       " [16, 20, 34, 52, 58, 63],\n",
       " [1, 6, 17, 20, 27, 31, 47, 56, 59, 64, 78, 86],\n",
       " [4, 7, 10, 11, 26, 50, 51, 63, 81],\n",
       " [2, 8, 16, 64, 68],\n",
       " [2, 20, 27, 29, 30, 31, 35, 39, 56, 68, 69, 86],\n",
       " [3, 15, 20, 21, 24, 39, 46, 49, 55, 61, 77, 78, 80],\n",
       " [9, 16, 20, 24, 36, 37, 42, 48, 63, 71, 76, 86],\n",
       " [7, 20, 22, 31, 38, 39, 53, 65, 69, 70, 83],\n",
       " [0, 17, 18, 34, 39, 47, 49, 53, 61, 68, 86],\n",
       " [5, 12, 32, 38, 46, 50, 53, 54, 61, 71, 83, 85],\n",
       " [10, 12, 20, 22, 26, 30, 39, 42, 57, 65, 86],\n",
       " [20, 24, 34, 37, 39, 43, 57, 58, 78, 84],\n",
       " [3, 9, 13, 17, 23, 26, 42, 47, 49, 55, 56, 73],\n",
       " [1, 10, 26, 36, 45, 49, 66, 71, 75, 83, 85],\n",
       " [22, 23, 26, 27, 37, 39, 53, 60, 69, 75, 78, 82, 83],\n",
       " [0, 1, 19, 21, 26, 28, 30, 42, 52, 55, 64],\n",
       " [8, 9, 19, 21, 26, 29, 37, 44, 48, 63, 65, 70, 77],\n",
       " [9, 31, 32, 36, 43, 55, 59, 60, 65, 66, 69, 85],\n",
       " [10, 11, 33, 36, 50, 56, 59, 68, 71, 75],\n",
       " [10, 20, 26, 29, 32, 41, 46, 49, 60, 68, 73, 75, 77, 79, 85],\n",
       " [1, 6, 17, 22, 25, 42, 44, 46, 50, 51, 52, 53, 77, 85],\n",
       " [18, 19, 21, 31, 36, 46, 47, 59, 76, 81, 87],\n",
       " [3, 5, 21, 28, 38, 47, 51, 54, 58, 64, 71, 82, 83, 85],\n",
       " [2, 3, 5, 13, 16, 26, 39, 48, 62, 64, 72, 75, 81, 87],\n",
       " [2, 13, 14, 19, 20, 23, 28, 29, 36, 38, 47, 51, 70, 84],\n",
       " [7, 18, 23, 26, 29, 34, 35, 42, 49, 62, 70, 84, 86]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 9.758226997985226\n",
      "testing 5.466666666666667\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train/len(user_following)\n",
    "print('training',avg)\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test/user_test_amount\n",
    "print('testing',avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(88)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation  Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(user_following)\n",
    "m = 88  \n",
    "k = 64\n",
    "#l = all_3374.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    token = 'lk9WjNIjAuMTd94qdDC1oDszn38qjfCY9PiD3nvrot0'\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.trainable_variables()])\n",
    "    print('trainable_vars:',sess.run(all_trainable_vars))\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(5):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #user degree of category favor \n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            train_f_sample = random.sample(train_f[z],5)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                #train_f_sample = random.sample(train_f[z],10)\n",
    "                \n",
    "                #print('Train_f_sample:', train_f_sample)\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    \n",
    "                    _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])\n",
    "        msg = test_result(U, Y, A, E,Au, Ay, Aa, Av,B,save_name+str(q))\n",
    "        lineNotifyMessage(token, msg)\n",
    "        np.savez('../Data/grid_search_weight/Our/'+save_name+str(q)+'.npz',U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "        print('Current time:',time.ctime())\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])\n",
    "    np.savez('../Data/grid_search_weight/Our/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    lineNotifyMessage(token, '../Data/grid_search_weight/Our/'+save_name+'.npz')\n",
    "    #np.savez('../Data/npy/mask_feature/result300/'+save_name+'.npz', \n",
    "    #                    U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    \n",
    "    return U, Y, A, E, Au, Ay, Aa, Av,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_feature_training(save_name): \n",
    "    token = 'lk9WjNIjAuMTd94qdDC1oDszn38qjfCY9PiD3nvrot0'\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.trainable_variables()])\n",
    "    print('trainable_vars:',sess.run(all_trainable_vars))\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(8):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #user degree of category favor \n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            \n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "    \n",
    "                train_f_sample = random.sample(train_f[z],5)\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    \n",
    "                    _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],r:r_3,\n",
    "                                       })\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        U, Y, A, Au, Ay, Aa =sess.run([user_latent, item_latent, aux_item, Wu, Wy, Wa])\n",
    "        msg = no_feature_test_result(U, Y, A, Au, Ay, Aa,save_name+str(q))\n",
    "        lineNotifyMessage(token, msg)\n",
    "        np.savez('../Data/grid_search_weight/Our/'+save_name+str(q)+'.npz',U=U, Y=Y, A=A,Wu=Au, Wy=Ay, Wa=Aa)\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "        print('Current time:',time.ctime())\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A, Au, Ay, Aa =sess.run([user_latent, item_latent, aux_item, Wu, Wy, Wa])\n",
    "    np.savez('../Data/grid_search_weight/Our/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A,Wu=Au, Wy=Ay, Wa=Aa)\n",
    "    lineNotifyMessage(token, '../Data/grid_search_weight/Our/'+save_name+'.npz')\n",
    "    #np.savez('../Data/npy/mask_feature/result300/'+save_name+'.npz', \n",
    "    #                    U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    \n",
    "    return U, Y, A, Au, Ay, Aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amount = user_test_amount\n",
    "def getScoreMatrix(RS):\n",
    "    #取出test的資料\n",
    "    testRS = np.zeros((test_amount,yt_test_amount)) #shape 150*18\n",
    "    target = np.zeros((test_amount,yt_test_amount))\n",
    "    print(testRS.shape)\n",
    "    #test_t 是true的\n",
    "    #test_f 是false的\n",
    "\n",
    "    for z in range(test_amount):\n",
    "        user_id = test_idx[z]\n",
    "        #positive target YouTuber list\n",
    "        youtube_t = test_t[z] \n",
    "        #not target YouTuber list\n",
    "        youtube_f = test_f[z]\n",
    "        #print(user_id)\n",
    "        #print(youtube_t)\n",
    "        #print(user_following[user_id])\n",
    "        #前兩個放target的RS\n",
    "        for i in range(len(youtube_t)):\n",
    "            testRS[z][i] = RS[z][youtube_t[i]]\n",
    "            target[z][i] = user_following[user_id][youtube_t[i]]\n",
    "        for i in range(len(youtube_f)):\n",
    "            testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "            target[z][i+len(youtube_t)] = user_following[user_id][youtube_f[i]]\n",
    "        #print('testRS for user',user_id,testRS[z],youtube_t+youtube_f)\n",
    "        \n",
    "    sumtarget = 0\n",
    "    for i in range(len(target)):\n",
    "        #print(np.sum(target[i]))\n",
    "        sumtarget += np.sum(target[i])\n",
    "    print('num of positive data in testing:',sumtarget)\n",
    "    print('total testing data:',test_amount*yt_test_amount)\n",
    "    \n",
    "    return target, testRS,sumtarget\n",
    "\n",
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList\n",
    "\n",
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "def getTOP1(target,testRS,sumtarget):\n",
    "    print('top1')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_0 = topN(list(testRS[i]),1) #取一個\n",
    "        \n",
    "        #print(top_0)\n",
    "        if top_0[0] < int(np.sum(target[i])):\n",
    "            correct += 1\n",
    "    top1_prec = correct/len(testRS)\n",
    "    top1_recall = correct/(sumtarget)\n",
    "    print('prec ',top1_prec,'recall ',top1_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top1_prec,top1_recall))\n",
    "    return top1_prec,top1_recall,F1_score(top1_prec,top1_recall)\n",
    "def getTOP3(target,testRS,sumtarget):\n",
    "    print('top3')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_3 = topN(list(testRS[i]),3) #取一個\n",
    "        \n",
    "        #print(top_3)\n",
    "        for j in range(len(top_3)):\n",
    "            if top_3[j] < int(np.sum(target[i])):\n",
    "                correct += 1\n",
    "    top3_prec = correct/(len(testRS)*3)\n",
    "    top3_recall = correct/(sumtarget)\n",
    "    print('prec ',top3_prec,'recall ',top3_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top3_prec,top3_recall))\n",
    "    return top3_prec,top3_recall,F1_score(top3_prec,top3_recall)\n",
    "def getTOP5(target,testRS,sumtarget):\n",
    "    print('top5')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_5 = topN(list(testRS[i]),5) #取一個\n",
    "       \n",
    "        #print(top_5)\n",
    "        for j in range(len(top_5)):\n",
    "            if top_5[j] < int(np.sum(target[i])):\n",
    "                correct += 1\n",
    "    top5_prec = correct/(len(testRS)*5)\n",
    "    top5_recall = correct/(sumtarget)\n",
    "    print('prec ',top5_prec,'recall ',top5_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top5_prec,top5_recall))\n",
    "    return top5_prec,top5_recall,F1_score(top5_prec,top5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NDCG\n",
    "https://daiwk.github.io/posts/nlp-ndcg.html\n",
    "\"\"\"\n",
    "# pre_list\n",
    "\"\"\"\n",
    "test_amount = 150\n",
    "yt_test_amount = 18\n",
    "\"\"\"\n",
    "def NDCG(target,testRS): #target是真正的喜好\n",
    "    all_sort = []\n",
    "    num_ndcg = 10\n",
    "    pre_matrix = np.zeros(shape=(test_amount,yt_test_amount)) #(150,18)\n",
    "    for i in range(test_amount): #user amount = 150\n",
    "        top_n = topN(list(testRS[i]),num_ndcg) #取10個\n",
    "        #print(top_n)\n",
    "        all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "        #print('all_sort',topN(list(testRS[i]),len(testRS[i])))\n",
    "        for j in range(len(top_n)):\n",
    "            pre_matrix[i][top_n[j]] = 1\n",
    "\n",
    "    #Ideal DCG，理想状况下的DCG。也就是说，相关性完全由高到低排序时算出的DCG：\n",
    "    def IDCG(ideal_list): #ideal_list example = [1,1,1,1,1,0,0,....]\n",
    "        idcg=0\n",
    "        #print('ideal',ideal_list)\n",
    "        for i in range(len(ideal_list)):\n",
    "            #print((2**true_list[i]-1),math.log2(i+2))\n",
    "            idcg+= (2**ideal_list[i]-1)/ math.log2(i+2)\n",
    "        #print('idcg',idcg)\n",
    "        return idcg\n",
    "    def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "        dcg=0\n",
    "        #print('prec',prec_list)\n",
    "        for i in range(len(prec_list)):\n",
    "            dcg+= (2**prec_list[i]-1)/ math.log2(i+2)\n",
    "        #print('dcg',dcg)\n",
    "        return dcg\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    for m in range(test_amount): # the number of testing users\n",
    "        #print('all-sort',all_sort[m][:num_ndcg])\n",
    "        idcg = IDCG(target[m][:num_ndcg])\n",
    "        pre_list = []\n",
    "        least_pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "            pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        for s in all_sort[m][num_ndcg:]:\n",
    "            #print(s)\n",
    "            #print(target[m][s])\n",
    "            least_pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "        #print(ndcg)\n",
    "        total_ndcg += ndcg\n",
    "    avg_ndcg = total_ndcg/test_amount\n",
    "    print('NDCG:',avg_ndcg)\n",
    "    return pre_matrix,avg_ndcg\n",
    "\n",
    "# MAP\n",
    "\"\"\"\n",
    ">>> y_true = np.array([0, 0, 1, 1])\n",
    ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    ">>> average_precision_score(y_true, y_scores)\n",
    "\"\"\"\n",
    "def MAP(target,testRS):\n",
    "    print('target:',target)\n",
    "    print('testRS:',testRS)\n",
    "    total_prec = 0\n",
    "    for u in range(test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec+=average_precision_score(y_true, y_scores)\n",
    "    Map_value = total_prec/test_amount\n",
    "    print('MAP',Map_value)\n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(U, Y, A, E,Au, Ay, Aa, Av,B):\n",
    "    test_amount = 150\n",
    "    yt_test_amount = 18\n",
    "    result=np.zeros((test_amount,88))\n",
    "    RS=np.zeros((test_amount,88))\n",
    "    #test_idx --> Test 的 index\n",
    "    print(test_idx)\n",
    "    sum_alpha = 0\n",
    "    for s in range(test_amount):\n",
    "        #print(s,test_idx[s])\n",
    "\n",
    "        yes=[]\n",
    "        sample = [i for i in range(88)]\n",
    "        alpha=np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "            alpha_a = np.dot(Au[test_idx[s]][sample[a]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[test_idx[s]][sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]][sample[a]],\n",
    "                    np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]][sample[a]],np.dot(E,np.expand_dims(all_3374[sample[a]],0).T))\n",
    "            \"\"\"\n",
    "            relu part ...\n",
    "            \"\"\"\n",
    "            alpha[a]=np.sum((relu(alpha_a)))*r\n",
    "            \"\"\"\n",
    "            tanh part ...\n",
    "            \"\"\"\n",
    "            #alpha[a]=np.sum((np.tanh(alpha_a)))*r\n",
    "            \n",
    "            \n",
    "        mul=np.zeros((1,64))\n",
    "        #print('alpha--------',alpha)\n",
    "        #print('add alpha------------',np.add(alpha,0.000000001))\n",
    "        added_alpha = np.add(alpha,0.0000000001)\n",
    "        norm_alpha = added_alpha/np.sum(added_alpha)\n",
    "        #print('alpha-----------',alpha)\n",
    "        #print('norm alpha--------------',norm_alpha)\n",
    "        sum_alpha += np.sum(alpha)\n",
    "        for i in range(len(sample)):\n",
    "            mul+=norm_alpha[i]*A[sample[i]] #attention alpha*Ai part\n",
    "        new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "        \n",
    "        \n",
    "        for k in range(88):\n",
    "            result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]],np.dot(E, all_3374[k].T))\n",
    "            \n",
    "    return RS\n",
    "def no_feature_testing(U, Y, A, Au, Ay, Aa):\n",
    "    test_amount = 150\n",
    "    yt_test_amount = 18\n",
    "    result=np.zeros((test_amount,88))\n",
    "    RS=np.zeros((test_amount,88))\n",
    "    #test_idx --> Test 的 index\n",
    "    print(test_idx)\n",
    "    sum_alpha = 0\n",
    "    for s in range(test_amount):\n",
    "        #print(s,test_idx[s])\n",
    "\n",
    "        sample = [i for i in range(88)]\n",
    "        alpha=np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "            alpha_a = np.dot(Au[test_idx[s]][sample[a]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[test_idx[s]][sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]][sample[a]],\n",
    "                    np.expand_dims(A[sample[a]],0).T)\n",
    "            \"\"\"\n",
    "            relu part ...\n",
    "            \"\"\"\n",
    "            alpha[a]=np.sum((relu(alpha_a)))*r\n",
    "            \"\"\"\n",
    "            tanh part ...\n",
    "            \"\"\"\n",
    "            #alpha[a]=np.sum((np.tanh(alpha_a)))*r\n",
    "            \n",
    "            \n",
    "        mul=np.zeros((1,64))\n",
    "        #print('alpha--------',alpha)\n",
    "        #print('add alpha------------',np.add(alpha,0.000000001))\n",
    "        added_alpha = np.add(alpha,0.0000000001)\n",
    "        norm_alpha = added_alpha/np.sum(added_alpha)\n",
    "        #print('alpha-----------',alpha)\n",
    "        #print('norm alpha--------------',norm_alpha)\n",
    "        sum_alpha += np.sum(alpha)\n",
    "        for i in range(len(sample)):\n",
    "            mul+=norm_alpha[i]*A[sample[i]] #attention alpha*Ai part\n",
    "        new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "        \n",
    "        \n",
    "        for k in range(88):\n",
    "            result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T)\n",
    "            \n",
    "    return RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_result(U, Y, A, E,Wu, Wy, Wa, Wv,B,save_name):\n",
    "    RS = testing(U, Y, A, E,Wu, Wy, Wa, Wv,B)\n",
    "    target,testRS,sumtarget = getScoreMatrix(RS)\n",
    "    prec_1,recall_1,f1_1 = getTOP1(target,testRS,sumtarget)\n",
    "    prec_3,recall_3,f1_3 = getTOP3(target,testRS,sumtarget)\n",
    "    prec_5,recall_5,f1_5 = getTOP5(target,testRS,sumtarget)\n",
    "    pre_matrix,avg_ndcg = NDCG(target,testRS)\n",
    "    Map_value = MAP(target,testRS)\n",
    "    msg = save_name+'\\nNDCG:'+str(round(avg_ndcg,3))+'\\n'+'MAP:'+str(round(Map_value,3))+'\\nTop1: prec'+str(round(prec_1,3))+' rec'+str(round(recall_1,3))+' f1'+str(round(f1_1,3))+'\\nTop3: prec'+str(round(prec_3,3))+' rec'+str(round(recall_3,3))+' f1'+str(round(f1_3,3))+'\\nTop5: prec'+str(round(prec_5,3))+' rec'+str(round(recall_5,3))+' f1'+str(round(f1_5,3))\n",
    "    \n",
    "    return msg\n",
    "def no_feature_test_result(U, Y, A,Wu, Wy, Wa,save_name):\n",
    "    RS = no_feature_testing(U, Y, A, Wu, Wy, Wa)\n",
    "    target,testRS,sumtarget = getScoreMatrix(RS)\n",
    "    prec_1,recall_1,f1_1 = getTOP1(target,testRS,sumtarget)\n",
    "    prec_3,recall_3,f1_3 = getTOP3(target,testRS,sumtarget)\n",
    "    prec_5,recall_5,f1_5 = getTOP5(target,testRS,sumtarget)\n",
    "    pre_matrix,avg_ndcg = NDCG(target,testRS)\n",
    "    Map_value = MAP(target,testRS)\n",
    "    msg = save_name+'\\nNDCG:'+str(round(avg_ndcg,3))+'\\n'+'MAP:'+str(round(Map_value,3))+'\\nTop1: prec'+str(round(prec_1,3))+' rec'+str(round(recall_1,3))+' f1'+str(round(f1_1,3))+'\\nTop3: prec'+str(round(prec_3,3))+' rec'+str(round(recall_3,3))+' f1'+str(round(f1_3,3))+'\\nTop5: prec'+str(round(prec_5,3))+' rec'+str(round(recall_5,3))+' f1'+str(round(f1_5,3))\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished []\n",
      "trainable_vars: 25264768\n",
      "Iteraction: 0\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.4666666666666667 recall  0.08536585365853659\n",
      "F1_score: 0.1443298969072165\n",
      "top3\n",
      "prec  0.44 recall  0.24146341463414633\n",
      "F1_score: 0.31181102362204727\n",
      "top5\n",
      "prec  0.3973333333333333 recall  0.36341463414634145\n",
      "F1_score: 0.3796178343949045\n",
      "NDCG: 0.5388229228324837\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.87184244e-01 -6.45279216e-02  1.00701557e-01 ... -6.30510847e-02\n",
      "   2.02575450e-02 -1.14888714e-01]\n",
      " [ 1.49091025e-01  1.73214777e-01 -6.74876327e-02 ...  1.28845638e-01\n",
      "  -2.40741143e-02  7.60238515e-02]\n",
      " [ 2.24377028e-02 -8.19033976e-03 -3.41421188e-05 ...  2.59636822e-03\n",
      "  -1.24456522e-02  1.23413063e-01]\n",
      " ...\n",
      " [-7.20020114e-02  4.95194096e-02  1.82987659e-02 ...  4.30752040e-02\n",
      "   6.25640193e-02 -1.62591535e-01]\n",
      " [ 1.18010402e-02  3.56428529e-03  6.72186471e-02 ... -5.02153332e-02\n",
      "  -3.73897788e-02  3.97029433e-02]\n",
      " [ 1.27159056e-01 -1.21301390e-01  9.76443027e-02 ... -7.54082040e-02\n",
      "   3.45351625e-02 -1.64791484e-01]]\n",
      "MAP 0.48930956736941594\n",
      "total_loss:----------------- [[0.6834011]]\n",
      "train_auc:------------------- 0.5697040605643496\n",
      "time: 1581.5456840991974  sec\n",
      "Current time: Wed Jul 22 17:26:41 2020\n",
      "Iteraction: 1\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.5066666666666667 recall  0.09268292682926829\n",
      "F1_score: 0.15670103092783505\n",
      "top3\n",
      "prec  0.47333333333333333 recall  0.2597560975609756\n",
      "F1_score: 0.3354330708661417\n",
      "top5\n",
      "prec  0.44533333333333336 recall  0.4073170731707317\n",
      "F1_score: 0.42547770700636944\n",
      "NDCG: 0.5872397340328732\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.24128034 -0.01617753  0.11922314 ... -0.08863621 -0.0243848\n",
      "  -0.14348571]\n",
      " [ 0.13551923  0.17012171 -0.06042838 ...  0.12350504 -0.04866762\n",
      "   0.06356893]\n",
      " [ 0.03453583  0.00871147 -0.00226423 ... -0.00781245  0.00647391\n",
      "   0.0953338 ]\n",
      " ...\n",
      " [-0.06614995  0.04161347  0.01840077 ...  0.03576007  0.06114382\n",
      "  -0.15687037]\n",
      " [ 0.00978019  0.02916482  0.04894515 ... -0.03533069 -0.0461591\n",
      "   0.04209698]\n",
      " [ 0.12631811 -0.10196106  0.04353607 ... -0.03122952  0.01627386\n",
      "  -0.16795657]]\n",
      "MAP 0.5310896571067503\n",
      "total_loss:----------------- [[0.67391469]]\n",
      "train_auc:------------------- 0.6234686854783207\n",
      "time: 3160.655017852783  sec\n",
      "Current time: Wed Jul 22 17:53:00 2020\n",
      "Iteraction: 2\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.6066666666666667 recall  0.11097560975609756\n",
      "F1_score: 0.18762886597938144\n",
      "top3\n",
      "prec  0.5288888888888889 recall  0.29024390243902437\n",
      "F1_score: 0.3748031496062992\n",
      "top5\n",
      "prec  0.4786666666666667 recall  0.4378048780487805\n",
      "F1_score: 0.4573248407643312\n",
      "NDCG: 0.6428364068392307\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 3.17854295e-01  3.49356429e-02  1.45579202e-01 ... -1.28807026e-01\n",
      "  -4.67027040e-02 -1.68803600e-01]\n",
      " [ 1.03572839e-01  1.96366684e-01 -4.41678995e-02 ...  1.28462589e-01\n",
      "  -9.77558196e-02  5.76641618e-02]\n",
      " [ 2.23735701e-02 -7.91763123e-02 -7.75618825e-04 ...  2.65847973e-04\n",
      "   1.98347038e-02  4.42182849e-02]\n",
      " ...\n",
      " [-5.03720768e-02  5.23496086e-02  2.56220999e-02 ...  2.23188834e-02\n",
      "   6.45225642e-02 -1.53378253e-01]\n",
      " [ 1.46305941e-02  1.03084043e-02  4.76334705e-02 ... -6.15876233e-02\n",
      "  -4.06289345e-02  1.75766483e-02]\n",
      " [ 4.03903601e-01 -1.39858869e-02  1.08362079e-01 ...  1.69027900e-01\n",
      "  -2.54339761e-02 -2.10847860e-01]]\n",
      "MAP 0.57691429693263\n",
      "total_loss:----------------- [[0.665152]]\n",
      "train_auc:------------------- 0.6659876118375774\n",
      "time: 4737.290873289108  sec\n",
      "Current time: Wed Jul 22 18:19:17 2020\n",
      "Iteraction: 3\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.5933333333333334 recall  0.10853658536585366\n",
      "F1_score: 0.18350515463917527\n",
      "top3\n",
      "prec  0.5644444444444444 recall  0.3097560975609756\n",
      "F1_score: 0.4\n",
      "top5\n",
      "prec  0.5253333333333333 recall  0.48048780487804876\n",
      "F1_score: 0.5019108280254777\n",
      "NDCG: 0.6713297447161515\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.29724615  0.08107221  0.14777895 ... -0.12135308 -0.05695929\n",
      "  -0.17754143]\n",
      " [ 0.16355269  0.1502426  -0.07584708 ...  0.14871577 -0.14231327\n",
      "   0.05559515]\n",
      " [ 0.10010449 -0.01024475  0.04192241 ... -0.00112049  0.00993984\n",
      "  -0.01414865]\n",
      " ...\n",
      " [-0.05115312  0.09172424  0.04620251 ...  0.03898657  0.08533133\n",
      "  -0.15383545]\n",
      " [-0.00725275  0.07277429  0.20196652 ... -0.124385    0.04609781\n",
      "   0.07163687]\n",
      " [ 0.11369586 -0.13917165  0.0530706  ... -0.09097156  0.09281262\n",
      "  -0.20389933]]\n",
      "MAP 0.6065372987073704\n",
      "total_loss:----------------- [[0.65752662]]\n",
      "train_auc:------------------- 0.6974535443909153\n",
      "time: 6326.965560913086  sec\n",
      "Current time: Wed Jul 22 18:45:47 2020\n",
      "Iteraction: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.6666666666666666 recall  0.12195121951219512\n",
      "F1_score: 0.2061855670103093\n",
      "top3\n",
      "prec  0.5933333333333334 recall  0.325609756097561\n",
      "F1_score: 0.42047244094488195\n",
      "top5\n",
      "prec  0.54 recall  0.49390243902439024\n",
      "F1_score: 0.5159235668789809\n",
      "NDCG: 0.7049165733413231\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 2.88113654e-01  3.02736860e-01  1.84681265e-01 ... -9.42162611e-02\n",
      "  -8.40175662e-03 -2.55752928e-01]\n",
      " [ 1.36712650e-01  1.76311909e-01 -3.29867167e-02 ...  9.28043092e-02\n",
      "  -1.21162532e-01  1.70264557e-02]\n",
      " [ 9.53526383e-02  2.20874928e-04 -3.36494974e-02 ... -2.61311768e-02\n",
      "   7.94663888e-03  1.54061865e-02]\n",
      " ...\n",
      " [ 5.58982135e-02  2.70687944e-01  1.08215815e-01 ...  2.19676327e-02\n",
      "  -9.28586047e-03 -2.50027961e-01]\n",
      " [ 2.41982175e-02  4.00198966e-02  1.75813176e-01 ... -8.39623870e-02\n",
      "  -1.52780361e-02  3.37711877e-02]\n",
      " [ 4.20483797e-01  4.30389745e-02  9.79200678e-02 ...  1.87330273e-01\n",
      "  -3.09126945e-02 -2.30461072e-01]]\n",
      "MAP 0.6410668515269445\n",
      "total_loss:----------------- [[0.65426897]]\n",
      "train_auc:------------------- 0.7015278733654507\n",
      "time: 7890.710386514664  sec\n",
      "Current time: Wed Jul 22 19:11:50 2020\n",
      "Iteraction: 5\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.6866666666666666 recall  0.12560975609756098\n",
      "F1_score: 0.21237113402061858\n",
      "top3\n",
      "prec  0.6177777777777778 recall  0.33902439024390246\n",
      "F1_score: 0.43779527559055115\n",
      "top5\n",
      "prec  0.5573333333333333 recall  0.5097560975609756\n",
      "F1_score: 0.5324840764331211\n",
      "NDCG: 0.7216306594191548\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.4276508   0.11627914  0.15779724 ... -0.19841042 -0.02776696\n",
      "  -0.15457917]\n",
      " [ 0.1899419   0.13876329 -0.07096477 ...  0.13207328 -0.16748046\n",
      "   0.0226891 ]\n",
      " [ 0.10539021  0.01695978 -0.04685028 ... -0.0378795   0.00428861\n",
      "   0.00639217]\n",
      " ...\n",
      " [-0.04729647  0.12886251  0.07319108 ...  0.03139505  0.07548826\n",
      "  -0.16015592]\n",
      " [ 0.00473541  0.09617729  0.307339   ... -0.1786748   0.08266569\n",
      "   0.07739497]\n",
      " [ 0.13946404 -0.1580231   0.05255471 ... -0.06710976  0.09442978\n",
      "  -0.20589199]]\n",
      "MAP 0.6549598827211197\n",
      "total_loss:----------------- [[0.65517329]]\n",
      "train_auc:------------------- 0.6992429456297315\n",
      "time: 9410.997699737549  sec\n",
      "Current time: Wed Jul 22 19:37:11 2020\n",
      "Iteraction: 6\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7066666666666667 recall  0.12926829268292683\n",
      "F1_score: 0.21855670103092786\n",
      "top3\n",
      "prec  0.6022222222222222 recall  0.3304878048780488\n",
      "F1_score: 0.42677165354330704\n",
      "top5\n",
      "prec  0.5613333333333334 recall  0.5134146341463415\n",
      "F1_score: 0.5363057324840764\n",
      "NDCG: 0.7261365957971657\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.35707627  0.35718983  0.22136423 ... -0.12047938 -0.03567559\n",
      "  -0.2860384 ]\n",
      " [ 0.16703559  0.1587604  -0.03168861 ...  0.07184782 -0.13233484\n",
      "  -0.01270275]\n",
      " [ 0.09973844  0.02087405 -0.0465604  ... -0.04231913  0.00964613\n",
      "  -0.00082171]\n",
      " ...\n",
      " [ 0.05687525  0.31090317  0.12996644 ...  0.03594636 -0.0397935\n",
      "  -0.27285492]\n",
      " [ 0.03209715  0.05329948  0.22871465 ... -0.11073263 -0.00510938\n",
      "   0.04462058]\n",
      " [ 0.43374778  0.06732353  0.09026091 ...  0.19487195 -0.02846762\n",
      "  -0.24608317]]\n",
      "MAP 0.6583408081759948\n",
      "total_loss:----------------- [[0.6569941]]\n",
      "train_auc:------------------- 0.6913282863041982\n",
      "time: 10920.586822032928  sec\n",
      "Current time: Wed Jul 22 20:02:20 2020\n",
      "Iteraction: 7\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.6866666666666666 recall  0.12560975609756098\n",
      "F1_score: 0.21237113402061858\n",
      "top3\n",
      "prec  0.6066666666666667 recall  0.3329268292682927\n",
      "F1_score: 0.42992125984251967\n",
      "top5\n",
      "prec  0.548 recall  0.501219512195122\n",
      "F1_score: 0.5235668789808917\n",
      "NDCG: 0.7238059309951753\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.47418573  0.1589085   0.18475708 ... -0.21643876 -0.04792388\n",
      "  -0.17783222]\n",
      " [ 0.22367055  0.11857612 -0.06808448 ...  0.10128527 -0.17379773\n",
      "  -0.00768375]\n",
      " [ 0.06822245  0.0103327  -0.02793476 ... -0.04103747  0.02857708\n",
      "  -0.00399026]\n",
      " ...\n",
      " [-0.04311386  0.16734203  0.11208368 ...  0.03522573  0.0474333\n",
      "  -0.17819379]\n",
      " [ 0.02455569  0.11294877  0.40800335 ... -0.23308324  0.10802355\n",
      "   0.09494319]\n",
      " [ 0.14750352 -0.16217043  0.05790115 ... -0.05692866  0.10254142\n",
      "  -0.20912328]]\n",
      "MAP 0.6576849361107738\n",
      "total_loss:----------------- [[0.65738456]]\n",
      "train_auc:------------------- 0.6918788713007571\n",
      "time: 12501.178585767746  sec\n",
      "Current time: Wed Jul 22 20:28:41 2020\n",
      "Total cost  12501.178585767746  sec\n",
      "Finish Training: no_feature_18_10f_lr5\n",
      "Start Testing:\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.6866666666666666 recall  0.12560975609756098\n",
      "F1_score: 0.21237113402061858\n",
      "top3\n",
      "prec  0.6066666666666667 recall  0.3329268292682927\n",
      "F1_score: 0.42992125984251967\n",
      "top5\n",
      "prec  0.548 recall  0.501219512195122\n",
      "F1_score: 0.5235668789808917\n",
      "NDCG: 0.7238059309951753\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.47418573  0.1589085   0.18475708 ... -0.21643876 -0.04792388\n",
      "  -0.17783222]\n",
      " [ 0.22367055  0.11857612 -0.06808448 ...  0.10128527 -0.17379773\n",
      "  -0.00768375]\n",
      " [ 0.06822245  0.0103327  -0.02793476 ... -0.04103747  0.02857708\n",
      "  -0.00399026]\n",
      " ...\n",
      " [-0.04311386  0.16734203  0.11208368 ...  0.03522573  0.0474333\n",
      "  -0.17819379]\n",
      " [ 0.02455569  0.11294877  0.40800335 ... -0.23308324  0.10802355\n",
      "   0.09494319]\n",
      " [ 0.14750352 -0.16217043  0.05790115 ... -0.05692866  0.10254142\n",
      "  -0.20912328]]\n",
      "MAP 0.6576849361107738\n"
     ]
    }
   ],
   "source": [
    "# no feature\n",
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "feature_name = ['no_feature']\n",
    "token = 'lk9WjNIjAuMTd94qdDC1oDszn38qjfCY9PiD3nvrot0'\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for feature in feature_name:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(feature)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    lineNotifyMessage(token,feature)\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "                    \n",
    "                    \n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) \n",
    "                                )[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                \n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    save_name = feature+'_18_10f_lr5'\n",
    "                    Ur, Yr, Ar, Aur, Ayr, Aar= no_feature_training(save_name)\n",
    "                    #np.savez(save_name+'.npz', \n",
    "                    #    U=Ur, Y=Yr, A=Ar, E=Er, Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr,B=Br)\n",
    "                    print('Finish Training:',save_name)\n",
    "                    print('Start Testing:')\n",
    "                    msg = no_feature_test_result(Ur, Yr, Ar,Aur, Ayr, Aar,save_name)\n",
    "                    lineNotifyMessage(token, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished []\n",
      "2112\n",
      "Now Dims: 200\n",
      "trainable_vars: 52191368\n",
      "Iteraction: 0\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7733333333333333 recall  0.14146341463414633\n",
      "F1_score: 0.23917525773195872\n",
      "top3\n",
      "prec  0.6066666666666667 recall  0.3329268292682927\n",
      "F1_score: 0.42992125984251967\n",
      "top5\n",
      "prec  0.5773333333333334 recall  0.5280487804878049\n",
      "F1_score: 0.5515923566878981\n",
      "NDCG: 0.7889327739004667\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.78787852  1.26461072 -0.0707484  ... -3.86581184 -0.61172655\n",
      "  -3.46294803]\n",
      " [ 2.61154402 -1.82095374  0.69944315 ... -1.43593109  0.72860627\n",
      "  -3.32980389]\n",
      " [ 1.3940751   2.60464969 -1.82659942 ... -2.52746057  0.56022535\n",
      "   1.67142552]\n",
      " ...\n",
      " [ 0.27425747  2.58876742  1.20022544 ... -0.47036183 -2.47552256\n",
      "  -3.43782718]\n",
      " [ 1.35997978 -0.38716926  2.80787051 ... -2.52462259  0.68793126\n",
      "   2.72518772]\n",
      " [ 0.75610069  0.46764865 -0.33911501 ...  0.74585951  2.73864051\n",
      "  -2.1382973 ]]\n",
      "MAP 0.6901665732679149\n",
      "total_loss:----------------- [[0.99097159]]\n",
      "train_auc:------------------- 0.8649415003441157\n",
      "time: 4289.3508858680725  sec\n",
      "Current time: Sat Jul 18 03:33:15 2020\n",
      "Iteraction: 1\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7666666666666667 recall  0.1402439024390244\n",
      "F1_score: 0.2371134020618557\n",
      "top3\n",
      "prec  0.6933333333333334 recall  0.3804878048780488\n",
      "F1_score: 0.4913385826771654\n",
      "top5\n",
      "prec  0.6253333333333333 recall  0.5719512195121951\n",
      "F1_score: 0.597452229299363\n",
      "NDCG: 0.8217743390459145\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.07212212  0.9283132   0.10496867 ... -4.81578974 -1.70081636\n",
      "  -5.03557987]\n",
      " [ 1.25937358 -3.12467009 -0.60937334 ... -3.27689926  0.29591107\n",
      "  -4.75369767]\n",
      " [ 0.39642185  1.29029198 -3.12830818 ... -4.04923892 -0.32862487\n",
      "   0.16603589]\n",
      " ...\n",
      " [-0.9140951   1.22230954  0.15050586 ... -1.44713754 -4.18601342\n",
      "  -4.89527034]\n",
      " [ 0.23724212 -1.36079969  2.06663518 ... -3.86776387  0.23534983\n",
      "   0.90809151]\n",
      " [ 0.93659055  0.5422885  -1.22064782 ...  0.45791831  0.84765307\n",
      "  -3.75496439]]\n",
      "MAP 0.737822459489477\n",
      "total_loss:----------------- [[0.34074321]]\n",
      "train_auc:------------------- 0.8765932553337922\n",
      "time: 8545.457632541656  sec\n",
      "Current time: Sat Jul 18 04:44:11 2020\n",
      "Iteraction: 2\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.78 recall  0.14268292682926828\n",
      "F1_score: 0.24123711340206183\n",
      "top3\n",
      "prec  0.6777777777777778 recall  0.3719512195121951\n",
      "F1_score: 0.48031496062992135\n",
      "top5\n",
      "prec  0.632 recall  0.5780487804878048\n",
      "F1_score: 0.6038216560509554\n",
      "NDCG: 0.8126469208783185\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-0.04915364  0.33043418 -1.06672599 ... -4.33393319 -2.19619178\n",
      "  -4.48346428]\n",
      " [ 1.21501627 -2.95279592 -0.15544133 ... -2.87048808 -0.63721006\n",
      "  -4.67263247]\n",
      " [ 0.64134483  1.32177537 -2.90543618 ... -4.47822917 -0.12352013\n",
      "  -0.55142101]\n",
      " ...\n",
      " [-0.12669282  1.17840036  0.32836103 ... -1.43821405 -3.80245247\n",
      "  -4.25582311]\n",
      " [ 0.26834707 -2.04809671  1.18000934 ... -3.99506552 -0.76320839\n",
      "   1.19553443]\n",
      " [-0.21685784 -0.70557039 -1.71115833 ... -0.18770225  0.9443988\n",
      "  -3.17733906]]\n",
      "MAP 0.7287709605809863\n",
      "total_loss:----------------- [[0.30864631]]\n",
      "train_auc:------------------- 0.8858568479008947\n",
      "time: 12727.84466290474  sec\n",
      "Current time: Sat Jul 18 05:53:53 2020\n",
      "Iteraction: 3\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8133333333333334 recall  0.14878048780487804\n",
      "F1_score: 0.2515463917525773\n",
      "top3\n",
      "prec  0.7244444444444444 recall  0.3975609756097561\n",
      "F1_score: 0.5133858267716537\n",
      "top5\n",
      "prec  0.6653333333333333 recall  0.6085365853658536\n",
      "F1_score: 0.6356687898089173\n",
      "NDCG: 0.8463013050277762\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-0.16764388 -0.44174388 -0.89097123 ... -6.18875516 -3.06649109\n",
      "  -6.86798629]\n",
      " [ 0.16461117 -4.38211946 -2.74114957 ... -5.34382937 -1.27374048\n",
      "  -5.86008336]\n",
      " [-0.58864363  0.28130192 -4.32938312 ... -6.84638828 -1.72273936\n",
      "  -2.94225505]\n",
      " ...\n",
      " [-1.69696645  0.17673202 -0.96376929 ... -2.67081863 -5.55618013\n",
      "  -6.51837437]\n",
      " [-0.98735286 -2.61056943  0.07966644 ... -6.29977401 -1.26111887\n",
      "  -1.17892202]\n",
      " [-0.57929251 -0.76058604 -2.27754099 ... -0.63072282 -1.43779716\n",
      "  -4.57362154]]\n",
      "MAP 0.7663967822419744\n",
      "total_loss:----------------- [[0.28781237]]\n",
      "train_auc:------------------- 0.8964693737095664\n",
      "time: 17025.738156080246  sec\n",
      "Current time: Sat Jul 18 07:05:31 2020\n",
      "Iteraction: 4\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.76 recall  0.13902439024390245\n",
      "F1_score: 0.2350515463917526\n",
      "top3\n",
      "prec  0.7044444444444444 recall  0.38658536585365855\n",
      "F1_score: 0.49921259842519683\n",
      "top5\n",
      "prec  0.6493333333333333 recall  0.5939024390243902\n",
      "F1_score: 0.6203821656050955\n",
      "NDCG: 0.8329743538871839\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-2.20896066 -2.17407752 -3.19903272 ... -9.13701716 -4.91308971\n",
      "  -9.33984266]\n",
      " [-1.57486521 -7.02015921 -4.73778773 ... -6.9959544  -3.59180441\n",
      "  -8.56285867]\n",
      " [-2.41663111 -1.48866382 -7.00785236 ... -8.72694302 -3.96033023\n",
      "  -4.40778605]\n",
      " ...\n",
      " [-4.06441704 -1.59268076 -3.35780533 ... -5.17550889 -7.44564836\n",
      "  -8.80430499]\n",
      " [-2.86053311 -5.30146724 -2.31085399 ... -8.09450426 -3.57686667\n",
      "  -2.70063284]\n",
      " [-2.66742028 -2.82117586 -4.85756505 ... -2.73527759 -3.04574086\n",
      "  -7.4926244 ]]\n",
      "MAP 0.7499685164072141\n",
      "total_loss:----------------- [[0.27723199]]\n",
      "train_auc:------------------- 0.9032484514796971\n",
      "time: 21235.619526147842  sec\n",
      "Current time: Sat Jul 18 08:15:41 2020\n",
      "Total cost  21235.619526147842  sec\n",
      "Finish Training: Image_50_2048D_18_10f_lr5\n",
      "Start Testing:\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.76 recall  0.13902439024390245\n",
      "F1_score: 0.2350515463917526\n",
      "top3\n",
      "prec  0.7044444444444444 recall  0.38658536585365855\n",
      "F1_score: 0.49921259842519683\n",
      "top5\n",
      "prec  0.6493333333333333 recall  0.5939024390243902\n",
      "F1_score: 0.6203821656050955\n",
      "NDCG: 0.8329743538871839\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-2.20896066 -2.17407752 -3.19903272 ... -9.13701716 -4.91308971\n",
      "  -9.33984266]\n",
      " [-1.57486521 -7.02015921 -4.73778773 ... -6.9959544  -3.59180441\n",
      "  -8.56285867]\n",
      " [-2.41663111 -1.48866382 -7.00785236 ... -8.72694302 -3.96033023\n",
      "  -4.40778605]\n",
      " ...\n",
      " [-4.06441704 -1.59268076 -3.35780533 ... -5.17550889 -7.44564836\n",
      "  -8.80430499]\n",
      " [-2.86053311 -5.30146724 -2.31085399 ... -8.09450426 -3.57686667\n",
      "  -2.70063284]\n",
      " [-2.66742028 -2.82117586 -4.85756505 ... -2.73527759 -3.04574086\n",
      "  -7.4926244 ]]\n",
      "MAP 0.7499685164072141\n",
      "Finished ['Image_50_2048D']\n",
      "2412\n",
      "Now Dims: 200\n",
      "trainable_vars: 52251368\n",
      "Iteraction: 0\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.62 recall  0.11341463414634147\n",
      "F1_score: 0.19175257731958764\n",
      "top3\n",
      "prec  0.62 recall  0.3402439024390244\n",
      "F1_score: 0.43937007874015754\n",
      "top5\n",
      "prec  0.5893333333333334 recall  0.5390243902439025\n",
      "F1_score: 0.5630573248407644\n",
      "NDCG: 0.7566042266486968\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 3.56940938  3.79178137  2.90262061 ... -0.24323782  2.03055971\n",
      "   0.14545023]\n",
      " [ 7.07222985  1.51741421  3.4191206  ...  1.4680235   3.59697802\n",
      "   0.11722245]\n",
      " [ 5.47669699  7.08463842  1.52114342 ...  0.62812701  3.97307585\n",
      "   4.87281235]\n",
      " ...\n",
      " [ 4.59867074  7.08067098  4.5104556  ...  2.4929176   0.5324046\n",
      "   0.14465044]\n",
      " [ 5.46001728  3.41238148  5.76324373 ...  0.66650434  3.59466248\n",
      "   5.03335524]\n",
      " [ 3.56975749  2.79742673  3.44857011 ...  3.6606902   5.02381898\n",
      "   1.19449927]]\n",
      "MAP 0.6704636567194694\n",
      "total_loss:----------------- [[1.02176273]]\n",
      "train_auc:------------------- 0.8644322092222987\n",
      "time: 4209.364263057709  sec\n",
      "Current time: Sat Jul 18 09:26:32 2020\n",
      "Iteraction: 1\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.72 recall  0.13170731707317074\n",
      "F1_score: 0.22268041237113403\n",
      "top3\n",
      "prec  0.6066666666666667 recall  0.3329268292682927\n",
      "F1_score: 0.42992125984251967\n",
      "top5\n",
      "prec  0.5946666666666667 recall  0.5439024390243903\n",
      "F1_score: 0.5681528662420383\n",
      "NDCG: 0.7818968036026861\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 2.88511299  3.02161013  2.047078   ... -1.7726238   1.48149643\n",
      "  -1.39294473]\n",
      " [ 4.82761779  0.38538737  2.30197267 ...  0.40190844  2.57870654\n",
      "  -1.46540069]\n",
      " [ 3.51006449  4.8859719   0.40591238 ... -1.22905418  2.70034343\n",
      "   3.81817418]\n",
      " ...\n",
      " [ 2.92887942  4.86330322  2.87762275 ...  0.95383233 -1.35126561\n",
      "  -1.3518567 ]\n",
      " [ 3.39530256  2.05799923  4.85622126 ... -1.0541532   2.60001271\n",
      "   3.99989363]\n",
      " [ 2.7764888   1.85599976  2.14630072 ...  2.77252892  3.9397769\n",
      "  -0.17909423]]\n",
      "MAP 0.6852121293788042\n",
      "total_loss:----------------- [[0.34550787]]\n",
      "train_auc:------------------- 0.8739022711631108\n",
      "time: 8363.762917041779  sec\n",
      "Current time: Sat Jul 18 10:35:46 2020\n",
      "Iteraction: 2\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7333333333333333 recall  0.13414634146341464\n",
      "F1_score: 0.2268041237113402\n",
      "top3\n",
      "prec  0.6555555555555556 recall  0.3597560975609756\n",
      "F1_score: 0.4645669291338582\n",
      "top5\n",
      "prec  0.6146666666666667 recall  0.5621951219512196\n",
      "F1_score: 0.5872611464968154\n",
      "NDCG: 0.8033509760409049\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.12394026  0.27043    -0.80219655 ... -6.07133834 -2.17204697\n",
      "  -5.3878446 ]\n",
      " [ 1.06179747 -3.61970227 -1.32297075 ... -3.44970027 -0.41452611\n",
      "  -5.22646631]\n",
      " [ 0.38033393  1.15350556 -3.62093542 ... -4.43877545 -0.8976653\n",
      "  -0.35755329]\n",
      " ...\n",
      " [-1.0576637   1.10747454 -0.5694757  ... -2.32872646 -4.46488111\n",
      "  -5.18135676]\n",
      " [ 0.06936806 -1.9160646   1.10179632 ... -3.9287321  -0.39781467\n",
      "   0.48601363]\n",
      " [-0.13038312 -0.18646576 -1.7006395  ...  0.00613369  0.29334814\n",
      "  -3.59530212]]\n",
      "MAP 0.7158777872760557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:----------------- [[0.30660748]]\n",
      "train_auc:------------------- 0.8860977288368892\n",
      "time: 12528.202760457993  sec\n",
      "Current time: Sat Jul 18 11:45:11 2020\n",
      "Iteraction: 3\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8066666666666666 recall  0.1475609756097561\n",
      "F1_score: 0.2494845360824742\n",
      "top3\n",
      "prec  0.7311111111111112 recall  0.401219512195122\n",
      "F1_score: 0.5181102362204725\n",
      "top5\n",
      "prec  0.6573333333333333 recall  0.6012195121951219\n",
      "F1_score: 0.6280254777070063\n",
      "NDCG: 0.8440759678349916\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.44198966  1.67793764  0.67294248 ... -4.75082994 -1.33405096\n",
      "  -5.56573334]\n",
      " [ 1.50716355 -3.36151734 -1.98230338 ... -3.63740719  0.46186515\n",
      "  -4.4625171 ]\n",
      " [ 1.39780566  1.5772039  -3.36076312 ... -5.71196811 -0.22023562\n",
      "  -1.08576512]\n",
      " ...\n",
      " [-0.10966283  1.50753645  0.37414475 ... -0.89402572 -4.10290471\n",
      "  -5.21081219]\n",
      " [ 0.9887324  -1.45724485  1.77557844 ... -4.9937281   0.35424316\n",
      "   0.34594032]\n",
      " [ 1.23753121  1.08915309 -1.06675195 ...  1.13926997 -0.0985021\n",
      "  -3.5728198 ]]\n",
      "MAP 0.7668417288399434\n",
      "total_loss:----------------- [[0.28838247]]\n",
      "train_auc:------------------- 0.8959119064005506\n",
      "time: 16724.442472219467  sec\n",
      "Current time: Sat Jul 18 12:55:07 2020\n",
      "Iteraction: 4\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8266666666666667 recall  0.15121951219512195\n",
      "F1_score: 0.25567010309278354\n",
      "top3\n",
      "prec  0.7488888888888889 recall  0.41097560975609754\n",
      "F1_score: 0.5307086614173229\n",
      "top5\n",
      "prec  0.6733333333333333 recall  0.6158536585365854\n",
      "F1_score: 0.6433121019108281\n",
      "NDCG: 0.8513438805369163\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.81927424  1.02006621  0.06652946 ... -4.79313864 -1.94215985\n",
      "  -5.93299065]\n",
      " [ 1.40159159 -3.56273115 -1.11335388 ... -4.50189891 -0.62772066\n",
      "  -4.54425969]\n",
      " [ 0.83769056  1.45937248 -3.55679523 ... -6.87427237 -0.51573871\n",
      "  -2.14039507]\n",
      " ...\n",
      " [-0.35051155  1.39504915 -0.35623665 ... -1.8185929  -5.05564812\n",
      "  -5.42996651]\n",
      " [ 0.33974753 -1.98894446  1.54251514 ... -5.91844568 -0.83747386\n",
      "  -0.6483905 ]\n",
      " [ 0.35984294 -0.25655887 -1.54685193 ...  0.07504684 -1.20205957\n",
      "  -3.998661  ]]\n",
      "MAP 0.7756009317432078\n",
      "total_loss:----------------- [[0.27618177]]\n",
      "train_auc:------------------- 0.901933929800413\n",
      "time: 20850.30333995819  sec\n",
      "Current time: Sat Jul 18 14:03:53 2020\n",
      "Total cost  20850.30333995819  sec\n",
      "Finish Training: Text_300D_18_10f_lr5\n",
      "Start Testing:\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8266666666666667 recall  0.15121951219512195\n",
      "F1_score: 0.25567010309278354\n",
      "top3\n",
      "prec  0.7488888888888889 recall  0.41097560975609754\n",
      "F1_score: 0.5307086614173229\n",
      "top5\n",
      "prec  0.6733333333333333 recall  0.6158536585365854\n",
      "F1_score: 0.6433121019108281\n",
      "NDCG: 0.8513438805369163\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.81927424  1.02006621  0.06652946 ... -4.79313864 -1.94215985\n",
      "  -5.93299065]\n",
      " [ 1.40159159 -3.56273115 -1.11335388 ... -4.50189891 -0.62772066\n",
      "  -4.54425969]\n",
      " [ 0.83769056  1.45937248 -3.55679523 ... -6.87427237 -0.51573871\n",
      "  -2.14039507]\n",
      " ...\n",
      " [-0.35051155  1.39504915 -0.35623665 ... -1.8185929  -5.05564812\n",
      "  -5.42996651]\n",
      " [ 0.33974753 -1.98894446  1.54251514 ... -5.91844568 -0.83747386\n",
      "  -0.6483905 ]\n",
      " [ 0.35984294 -0.25655887 -1.54685193 ...  0.07504684 -1.20205957\n",
      "  -3.998661  ]]\n",
      "MAP 0.7756009317432078\n",
      "Finished ['Image_50_2048D', 'Text_300D']\n",
      "2924\n",
      "Now Dims: 200\n",
      "trainable_vars: 52353768\n",
      "Iteraction: 0\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.6533333333333333 recall  0.11951219512195121\n",
      "F1_score: 0.2020618556701031\n",
      "top3\n",
      "prec  0.5933333333333334 recall  0.325609756097561\n",
      "F1_score: 0.42047244094488195\n",
      "top5\n",
      "prec  0.5813333333333334 recall  0.5317073170731708\n",
      "F1_score: 0.5554140127388536\n",
      "NDCG: 0.7669726875076371\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 2.62128583  3.38014034  1.82235829 ... -0.9427872   1.11141345\n",
      "  -1.01574251]\n",
      " [ 5.36687167  0.24188336  2.59672799 ...  0.17602959  2.74506028\n",
      "  -1.10558272]\n",
      " [ 3.7434415   5.35430356  0.229795   ... -0.09332703  3.11283761\n",
      "   3.73368967]\n",
      " ...\n",
      " [ 2.96011175  5.35954758  3.12813913 ...  1.12355072 -0.72550218\n",
      "  -1.0053674 ]\n",
      " [ 3.75184049  1.66301054  4.68691184 ... -0.08356092  2.73388718\n",
      "   4.48018769]\n",
      " [ 2.64028566  2.2263926   1.69689242 ...  2.80172953  4.45987256\n",
      "  -0.45289198]]\n",
      "MAP 0.6711472795986222\n",
      "total_loss:----------------- [[0.9381165]]\n",
      "train_auc:------------------- 0.865216792842395\n",
      "time: 4362.08395075798  sec\n",
      "Current time: Sat Jul 18 15:17:15 2020\n",
      "Iteraction: 1\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.76 recall  0.13902439024390245\n",
      "F1_score: 0.2350515463917526\n",
      "top3\n",
      "prec  0.6266666666666667 recall  0.3439024390243902\n",
      "F1_score: 0.44409448818897634\n",
      "top5\n",
      "prec  0.6026666666666667 recall  0.551219512195122\n",
      "F1_score: 0.575796178343949\n",
      "NDCG: 0.7891217896030742\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 3.36872491e-02  1.30970680e+00 -1.74861324e-01 ... -4.65037856e+00\n",
      "  -1.05218571e+00 -4.15413556e+00]\n",
      " [ 2.96588186e+00 -1.95080363e+00  8.11327671e-02 ... -2.17978283e+00\n",
      "   4.85539915e-01 -4.01890805e+00]\n",
      " [ 1.28625749e+00  2.96966957e+00 -1.95701315e+00 ... -3.06455522e+00\n",
      "   4.69004837e-01  1.34565659e+00]\n",
      " ...\n",
      " [ 1.71473172e-01  2.96712129e+00  5.69309227e-01 ... -1.71732429e+00\n",
      "  -3.84553564e+00 -4.05685239e+00]\n",
      " [ 1.23427039e+00 -8.77575806e-01  2.47239012e+00 ... -2.99082651e+00\n",
      "   5.16670112e-01  1.86531746e+00]\n",
      " [ 3.20416067e-03 -5.10827364e-01 -7.88160604e-01 ...  6.86880158e-01\n",
      "   1.80232682e+00 -3.01652701e+00]]\n",
      "MAP 0.6950082174572795\n",
      "total_loss:----------------- [[0.34318326]]\n",
      "train_auc:------------------- 0.8750791465932554\n",
      "time: 8669.298092365265  sec\n",
      "Current time: Sat Jul 18 16:29:02 2020\n",
      "Iteraction: 2\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.76 recall  0.13902439024390245\n",
      "F1_score: 0.2350515463917526\n",
      "top3\n",
      "prec  0.7022222222222222 recall  0.3853658536585366\n",
      "F1_score: 0.4976377952755905\n",
      "top5\n",
      "prec  0.648 recall  0.5926829268292683\n",
      "F1_score: 0.6191082802547772\n",
      "NDCG: 0.8295828031634145\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.92963463  1.95797434  0.28242813 ... -4.2929391  -0.17910629\n",
      "  -4.22120211]\n",
      " [ 2.50995217 -1.85682753 -0.82836772 ... -2.64385284  1.00823159\n",
      "  -3.75906348]\n",
      " [ 1.62710727  2.49887702 -1.86682283 ... -4.07417114  0.59377715\n",
      "   0.63381896]\n",
      " ...\n",
      " [ 0.41556507  2.48286778  0.74245285 ... -0.96569507 -3.64720105\n",
      "  -3.99897097]\n",
      " [ 1.43243591 -0.18205241  2.59001875 ... -3.75429985  0.83522965\n",
      "   1.32380502]\n",
      " [ 0.77745064  0.76627602  0.04641075 ...  1.31076398  1.08875723\n",
      "  -2.52584316]]\n",
      "MAP 0.7457786137335282\n",
      "total_loss:----------------- [[0.30634888]]\n",
      "train_auc:------------------- 0.8857811424638679\n",
      "time: 12995.934145450592  sec\n",
      "Current time: Sat Jul 18 17:41:09 2020\n",
      "Iteraction: 3\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7866666666666666 recall  0.14390243902439023\n",
      "F1_score: 0.24329896907216492\n",
      "top3\n",
      "prec  0.7111111111111111 recall  0.3902439024390244\n",
      "F1_score: 0.5039370078740157\n",
      "top5\n",
      "prec  0.6386666666666667 recall  0.5841463414634146\n",
      "F1_score: 0.6101910828025477\n",
      "NDCG: 0.8312228786664725\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.68204784  0.77143728 -0.41303855 ... -4.92223899 -1.74058592\n",
      "  -5.30917053]\n",
      " [ 1.30358435 -3.23984033 -0.97649194 ... -4.0979361  -0.16249901\n",
      "  -4.47660572]\n",
      " [ 0.67677568  1.27694107 -3.25718416 ... -5.48042553 -0.54407206\n",
      "  -1.39297217]\n",
      " ...\n",
      " [-0.29169229  1.25476674 -0.36600144 ... -2.15578585 -4.85345931\n",
      "  -4.91385159]\n",
      " [ 0.34931104 -2.08129819  1.07357188 ... -4.91765726 -0.43554599\n",
      "  -0.09655974]\n",
      " [ 0.30494638 -0.2483157  -1.7784801  ...  0.19769581 -0.42676127\n",
      "  -3.53992283]]\n",
      "MAP 0.7474629630687307\n",
      "total_loss:----------------- [[0.28810645]]\n",
      "train_auc:------------------- 0.895196145905024\n",
      "time: 17333.68672633171  sec\n",
      "Current time: Sat Jul 18 18:53:26 2020\n",
      "Iteraction: 4\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7733333333333333 recall  0.14146341463414633\n",
      "F1_score: 0.23917525773195872\n",
      "top3\n",
      "prec  0.72 recall  0.3951219512195122\n",
      "F1_score: 0.5102362204724409\n",
      "top5\n",
      "prec  0.652 recall  0.5963414634146341\n",
      "F1_score: 0.6229299363057326\n",
      "NDCG: 0.8368936286351608\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 2.81105957  2.99844133  2.23583597 ... -4.72711779  0.54956978\n",
      "  -4.7077509 ]\n",
      " [ 2.69951965 -2.30073177 -0.1710163  ... -2.28121843  1.75128671\n",
      "  -3.29452346]\n",
      " [ 2.67879707  2.67547802 -2.30197601 ... -5.38140137  0.82636928\n",
      "  -0.55956012]\n",
      " ...\n",
      " [ 1.20247414  2.64392005  0.82970171 ... -0.55083696 -3.46066227\n",
      "  -4.13379045]\n",
      " [ 2.3118719   0.04860686  3.07003388 ... -4.65844749  1.52989631\n",
      "   0.84402679]\n",
      " [ 2.21601114  1.89330793  0.39419691 ...  2.31180771  0.39202607\n",
      "  -2.58181877]]\n",
      "MAP 0.7558773708257367\n",
      "total_loss:----------------- [[0.27699646]]\n",
      "train_auc:------------------- 0.9024776324845148\n",
      "time: 21582.135874271393  sec\n",
      "Current time: Sat Jul 18 20:04:15 2020\n",
      "Total cost  21582.135874271393  sec\n",
      "Finish Training: Video_512D_6650_18_10f_lr5\n",
      "Start Testing:\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7733333333333333 recall  0.14146341463414633\n",
      "F1_score: 0.23917525773195872\n",
      "top3\n",
      "prec  0.72 recall  0.3951219512195122\n",
      "F1_score: 0.5102362204724409\n",
      "top5\n",
      "prec  0.652 recall  0.5963414634146341\n",
      "F1_score: 0.6229299363057326\n",
      "NDCG: 0.8368936286351608\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 2.81105957  2.99844133  2.23583597 ... -4.72711779  0.54956978\n",
      "  -4.7077509 ]\n",
      " [ 2.69951965 -2.30073177 -0.1710163  ... -2.28121843  1.75128671\n",
      "  -3.29452346]\n",
      " [ 2.67879707  2.67547802 -2.30197601 ... -5.38140137  0.82636928\n",
      "  -0.55956012]\n",
      " ...\n",
      " [ 1.20247414  2.64392005  0.82970171 ... -0.55083696 -3.46066227\n",
      "  -4.13379045]\n",
      " [ 2.3118719   0.04860686  3.07003388 ... -4.65844749  1.52989631\n",
      "   0.84402679]\n",
      " [ 2.21601114  1.89330793  0.39419691 ...  2.31180771  0.39202607\n",
      "  -2.58181877]]\n",
      "MAP 0.7558773708257367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ['Image_50_2048D', 'Text_300D', 'Video_512D_6650']\n",
      "2939\n",
      "Now Dims: 200\n",
      "trainable_vars: 52356768\n",
      "Iteraction: 0\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.58 recall  0.10609756097560975\n",
      "F1_score: 0.17938144329896907\n",
      "top3\n",
      "prec  0.5377777777777778 recall  0.2951219512195122\n",
      "F1_score: 0.3811023622047244\n",
      "top5\n",
      "prec  0.5533333333333333 recall  0.5060975609756098\n",
      "F1_score: 0.5286624203821655\n",
      "NDCG: 0.7338700247144881\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.76354114  2.3683926   1.40761859 ... -2.27144808  0.69685736\n",
      "  -1.64804494]\n",
      " [ 3.84959308 -0.44473075  2.17089299 ... -0.1128158   1.78322957\n",
      "  -1.35207731]\n",
      " [ 2.80479243  3.85213638 -0.45185656 ... -0.04734025  2.0334772\n",
      "   4.77297572]\n",
      " ...\n",
      " [ 2.42844773  3.84213346  2.34241815 ...  0.19502564 -1.52888028\n",
      "  -1.63166581]\n",
      " [ 2.76881703  1.065953    3.97907265 ... -0.01033345  1.73442113\n",
      "   3.85520898]\n",
      " [ 1.74780488  1.1625278   1.09789557 ...  1.78988535  3.83835194\n",
      "  -0.3400091 ]]\n",
      "MAP 0.6306739267052759\n",
      "total_loss:----------------- [[0.93741784]]\n",
      "train_auc:------------------- 0.8651204404679973\n",
      "time: 4366.321794271469  sec\n",
      "Current time: Sat Jul 18 21:17:49 2020\n",
      "Iteraction: 1\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8066666666666666 recall  0.1475609756097561\n",
      "F1_score: 0.2494845360824742\n",
      "top3\n",
      "prec  0.7044444444444444 recall  0.38658536585365855\n",
      "F1_score: 0.49921259842519683\n",
      "top5\n",
      "prec  0.628 recall  0.574390243902439\n",
      "F1_score: 0.6\n",
      "NDCG: 0.8236668609589046\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 3.25784007  2.81405472  2.19177267 ... -2.02463639  0.87610015\n",
      "  -1.81677656]\n",
      " [ 4.5597438  -0.46200838  1.07649774 ... -0.77566276  2.55114824\n",
      "  -1.91380696]\n",
      " [ 3.545977    4.57418174 -0.48054776 ... -1.98745622  2.50449938\n",
      "   2.45377476]\n",
      " ...\n",
      " [ 2.23777688  4.52818732  2.72916214 ...  0.70332128 -2.10153208\n",
      "  -1.72815072]\n",
      " [ 3.41105949  1.26865165  4.41677311 ... -1.7884824   2.31261921\n",
      "   3.0800582 ]\n",
      " [ 3.20647606  2.02917802  1.43103177 ...  2.617662    2.9367743\n",
      "  -0.34310409]]\n",
      "MAP 0.7406039375248771\n",
      "total_loss:----------------- [[0.34484794]]\n",
      "train_auc:------------------- 0.8743977976600138\n",
      "time: 8672.917977809906  sec\n",
      "Current time: Sat Jul 18 22:29:36 2020\n",
      "Iteraction: 2\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7533333333333333 recall  0.1378048780487805\n",
      "F1_score: 0.2329896907216495\n",
      "top3\n",
      "prec  0.6644444444444444 recall  0.3646341463414634\n",
      "F1_score: 0.4708661417322834\n",
      "top5\n",
      "prec  0.6173333333333333 recall  0.5646341463414634\n",
      "F1_score: 0.5898089171974522\n",
      "NDCG: 0.8075050411993995\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.46486329  1.9251687   0.53632991 ... -4.45546976 -1.42208734\n",
      "  -3.88463165]\n",
      " [ 2.34164315 -2.64261559  0.46423436 ... -2.15231356  1.07645367\n",
      "  -3.72264747]\n",
      " [ 1.38160327  2.34968213 -2.67745191 ... -3.42659067  0.16795402\n",
      "   1.23148838]\n",
      " ...\n",
      " [ 0.53510555  2.27684109  0.6619297  ... -1.22223669 -3.27084459\n",
      "  -3.58252513]\n",
      " [ 1.17173134 -1.1255791   2.58031246 ... -3.05551213  0.82559625\n",
      "   1.9263308 ]\n",
      " [ 1.33735958  1.3275389  -0.83799205 ...  1.41643999  1.59299754\n",
      "  -2.86064392]]\n",
      "MAP 0.7176745160146416\n",
      "total_loss:----------------- [[0.30746191]]\n",
      "train_auc:------------------- 0.8860839642119752\n",
      "time: 12987.239378929138  sec\n",
      "Current time: Sat Jul 18 23:41:30 2020\n",
      "Iteraction: 3\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.78 recall  0.14268292682926828\n",
      "F1_score: 0.24123711340206183\n",
      "top3\n",
      "prec  0.7244444444444444 recall  0.3975609756097561\n",
      "F1_score: 0.5133858267716537\n",
      "top5\n",
      "prec  0.6613333333333333 recall  0.6048780487804878\n",
      "F1_score: 0.6318471337579618\n",
      "NDCG: 0.8438266466489848\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.82240518  1.96551092  1.3603473  ... -4.82070255 -0.53472743\n",
      "  -4.99990078]\n",
      " [ 2.37692283 -2.03212053 -1.86852215 ... -3.95396083  0.65162266\n",
      "  -4.00095841]\n",
      " [ 1.56262711  2.37955998 -2.05614825 ... -4.91846046  0.49860703\n",
      "  -1.61663   ]\n",
      " ...\n",
      " [ 0.49013238  2.27570881  0.85185605 ... -0.94497813 -3.83426533\n",
      "  -4.48051368]\n",
      " [ 1.2896172  -0.43798902  2.34455426 ... -4.42262797  0.32575403\n",
      "   0.39477112]\n",
      " [ 1.57261979  1.23366148 -0.013454   ...  1.19897317 -0.09522198\n",
      "  -2.33759758]]\n",
      "MAP 0.7641616777667907\n",
      "total_loss:----------------- [[0.28711782]]\n",
      "train_auc:------------------- 0.8967102546455609\n",
      "time: 17319.323795080185  sec\n",
      "Current time: Sun Jul 19 00:53:42 2020\n",
      "Iteraction: 4\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.76 recall  0.13902439024390245\n",
      "F1_score: 0.2350515463917526\n",
      "top3\n",
      "prec  0.7177777777777777 recall  0.39390243902439026\n",
      "F1_score: 0.5086614173228347\n",
      "top5\n",
      "prec  0.6613333333333333 recall  0.6048780487804878\n",
      "F1_score: 0.6318471337579618\n",
      "NDCG: 0.8342283742826041\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-0.23850986 -0.57702147 -1.07160173 ... -6.45436235 -2.85158385\n",
      "  -7.0292534 ]\n",
      " [ 0.59232444 -4.68321561 -2.83002433 ... -5.38406507 -1.42547514\n",
      "  -6.02658679]\n",
      " [-0.74077937  0.61346571 -4.71259955 ... -7.36918821 -1.85868389\n",
      "  -3.00091514]\n",
      " ...\n",
      " [-1.49261265  0.5201114  -1.89550748 ... -3.6911265  -6.46899971\n",
      "  -6.39148658]\n",
      " [-1.2127133  -3.46549927  0.15145101 ... -6.44493911 -1.78648433\n",
      "  -1.75319961]\n",
      " [-0.6946472  -1.65352939 -2.96471713 ... -0.71664882 -2.39422709\n",
      "  -4.96026535]]\n",
      "MAP 0.7546508247661158\n",
      "total_loss:----------------- [[0.27675572]]\n",
      "train_auc:------------------- 0.9015760495526497\n",
      "time: 21606.28313279152  sec\n",
      "Current time: Sun Jul 19 02:05:09 2020\n",
      "Total cost  21606.28313279152  sec\n",
      "Finish Training: social_Norm15D_18_10f_lr5\n",
      "Start Testing:\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.76 recall  0.13902439024390245\n",
      "F1_score: 0.2350515463917526\n",
      "top3\n",
      "prec  0.7177777777777777 recall  0.39390243902439026\n",
      "F1_score: 0.5086614173228347\n",
      "top5\n",
      "prec  0.6613333333333333 recall  0.6048780487804878\n",
      "F1_score: 0.6318471337579618\n",
      "NDCG: 0.8342283742826041\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-0.23850986 -0.57702147 -1.07160173 ... -6.45436235 -2.85158385\n",
      "  -7.0292534 ]\n",
      " [ 0.59232444 -4.68321561 -2.83002433 ... -5.38406507 -1.42547514\n",
      "  -6.02658679]\n",
      " [-0.74077937  0.61346571 -4.71259955 ... -7.36918821 -1.85868389\n",
      "  -3.00091514]\n",
      " ...\n",
      " [-1.49261265  0.5201114  -1.89550748 ... -3.6911265  -6.46899971\n",
      "  -6.39148658]\n",
      " [-1.2127133  -3.46549927  0.15145101 ... -6.44493911 -1.78648433\n",
      "  -1.75319961]\n",
      " [-0.6946472  -1.65352939 -2.96471713 ... -0.71664882 -2.39422709\n",
      "  -4.96026535]]\n",
      "MAP 0.7546508247661158\n"
     ]
    }
   ],
   "source": [
    "# two feature \n",
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "#l = all_3374.shape[1]\n",
    "try_count = [str(try_i) for try_i in range(3)]\n",
    "feature_name = ['Following_64D','Image_50_2048D','Text_300D','Video_512D_6650','social_Norm15D']\n",
    "token = 'lk9WjNIjAuMTd94qdDC1oDszn38qjfCY9PiD3nvrot0'\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "all_3374 = np.load('../Data/npy/'+'Following_64D'+'.npy')\n",
    "for feature in feature_name[1:]:\n",
    "    #added_feature = np.load('../Data/npy/'+'Image_50_2048D'+'.npy')\n",
    "    #all_3374 = np.concatenate((all_3374,added_feature),axis=1)\n",
    "    if 'Following' not in feature:\n",
    "        added_feature = np.load('../Data/npy/'+feature+'.npy')\n",
    "        all_3374 = np.concatenate((all_3374,added_feature),axis=1)\n",
    "        l = all_3374.shape[1]\n",
    "    else:\n",
    "        l = all_3374.shape[1]\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(feature)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    lineNotifyMessage(token,feature)\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    \n",
    "                    \n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.1,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    merge_f = ''\n",
    "                    for fname in finish_list:\n",
    "                        merge_f=merge_f+fname\n",
    "                    save_name = feature+'_18_10f_lr5'\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(save_name)\n",
    "                    #np.savez(save_name+'.npz', \n",
    "                    #    U=Ur, Y=Yr, A=Ar, E=Er, Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr,B=Br)\n",
    "                    print('Finish Training:',save_name)\n",
    "                    print('Start Testing:')\n",
    "                    msg = test_result(Ur, Yr, Ar, Er,Aur, Ayr, Aar, Avr,Br,save_name)\n",
    "                    lineNotifyMessage(token, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished []\n",
      "2048\n",
      "Now Dims: 200\n",
      "trainable_vars: 52178568\n",
      "Iteraction: 0\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.6266666666666667 recall  0.11463414634146342\n",
      "F1_score: 0.19381443298969073\n",
      "top3\n",
      "prec  0.5866666666666667 recall  0.32195121951219513\n",
      "F1_score: 0.415748031496063\n",
      "top5\n",
      "prec  0.5813333333333334 recall  0.5317073170731708\n",
      "F1_score: 0.5554140127388536\n",
      "NDCG: 0.7651092031050446\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.93154554  2.08326199  1.32240786 ... -2.90300798 -0.11151684\n",
      "  -1.52538293]\n",
      " [ 3.14985001 -1.33870784  0.27987005 ... -0.77977237  1.7959981\n",
      "  -2.76370986]\n",
      " [ 2.04806389  3.14805168 -1.35584065 ... -1.37809967  1.34212422\n",
      "   2.96120487]\n",
      " ...\n",
      " [ 0.78803712  3.13326468  1.89499384 ...  0.07925555 -1.50649535\n",
      "  -1.50642853]\n",
      " [ 1.99940141  0.01450718  2.97145928 ... -1.33942796  1.7347252\n",
      "   3.30359149]\n",
      " [ 1.88948513  1.60854548  0.06714302 ...  1.80496791  3.30289315\n",
      "  -1.47312595]]\n",
      "MAP 0.6695435524397372\n",
      "total_loss:----------------- [[0.99875355]]\n",
      "train_auc:------------------- 0.8643289745354439\n",
      "time: 4107.015435934067  sec\n",
      "Current time: Mon Jul 20 02:40:40 2020\n",
      "Iteraction: 1\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.84 recall  0.15365853658536585\n",
      "F1_score: 0.2597938144329897\n",
      "top3\n",
      "prec  0.7044444444444444 recall  0.38658536585365855\n",
      "F1_score: 0.49921259842519683\n",
      "top5\n",
      "prec  0.6266666666666667 recall  0.573170731707317\n",
      "F1_score: 0.5987261146496816\n",
      "NDCG: 0.8363572405385736\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.68973136e+00  2.12223131e+00  9.01729523e-01 ... -3.52706824e+00\n",
      "  -1.32875935e-01 -2.34405368e+00]\n",
      " [ 3.21293606e+00  2.40500925e-03  6.31829024e-01 ... -1.29125726e+00\n",
      "   1.35899433e+00 -2.71155674e+00]\n",
      " [ 2.19882238e+00  3.25838066e+00  1.50914668e-02 ... -2.33668448e+00\n",
      "   1.15040227e+00  1.64902209e+00]\n",
      " ...\n",
      " [ 1.33633423e+00  3.22373290e+00  2.03228684e+00 ...  2.38117222e-01\n",
      "  -1.94224689e+00 -2.26749459e+00]\n",
      " [ 2.07947611e+00  7.78213574e-01  3.02024775e+00 ... -2.19266661e+00\n",
      "   1.31417566e+00  2.44665053e+00]\n",
      " [ 1.64384710e+00  1.59456824e+00  9.52673580e-01 ...  1.62256267e+00\n",
      "   2.31273171e+00 -1.08785617e+00]]\n",
      "MAP 0.7499648133584179\n",
      "total_loss:----------------- [[0.34447802]]\n",
      "train_auc:------------------- 0.8749346180316586\n",
      "time: 8212.574449300766  sec\n",
      "Current time: Mon Jul 20 03:49:05 2020\n",
      "Iteraction: 2\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8133333333333334 recall  0.14878048780487804\n",
      "F1_score: 0.2515463917525773\n",
      "top3\n",
      "prec  0.72 recall  0.3951219512195122\n",
      "F1_score: 0.5102362204724409\n",
      "top5\n",
      "prec  0.644 recall  0.5890243902439024\n",
      "F1_score: 0.6152866242038216\n",
      "NDCG: 0.8392537012716365\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-1.08398253 -1.55217571 -1.96256245 ... -6.33345505 -3.91290555\n",
      "  -6.31557154]\n",
      " [ 0.27537889 -4.84240785 -3.22953824 ... -5.60926166 -2.12592852\n",
      "  -6.67441274]\n",
      " [-1.13675828  0.34202118 -4.84329661 ... -6.89484943 -2.50019786\n",
      "  -1.98731881]\n",
      " ...\n",
      " [-1.98810214  0.3098572  -1.66333388 ... -3.01886433 -5.85755615\n",
      "  -6.10454214]\n",
      " [-1.30494011 -3.32169254 -0.22223722 ... -6.67359473 -2.05915953\n",
      "  -1.17379052]\n",
      " [-1.24175232 -1.79831671 -3.06051307 ... -1.605212   -1.35586027\n",
      "  -5.09141912]]\n",
      "MAP 0.7545517860314557\n",
      "total_loss:----------------- [[0.30621357]]\n",
      "train_auc:------------------- 0.8861390227116311\n",
      "time: 12290.924375772476  sec\n",
      "Current time: Mon Jul 20 04:57:04 2020\n",
      "Iteraction: 3\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8266666666666667 recall  0.15121951219512195\n",
      "F1_score: 0.25567010309278354\n",
      "top3\n",
      "prec  0.7244444444444444 recall  0.3975609756097561\n",
      "F1_score: 0.5133858267716537\n",
      "top5\n",
      "prec  0.6493333333333333 recall  0.5939024390243902\n",
      "F1_score: 0.6203821656050955\n",
      "NDCG: 0.8413782894505244\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-0.38192768 -0.75353835 -1.12678852 ... -6.10812329 -3.27548808\n",
      "  -6.05284248]\n",
      " [ 0.17515844 -4.98102038 -2.71689447 ... -4.93325533 -2.03617367\n",
      "  -6.19758036]\n",
      " [-0.85276382  0.30305982 -4.93680338 ... -6.45513978 -2.05473302\n",
      "  -2.99561014]\n",
      " ...\n",
      " [-2.35342072  0.18229922 -1.42290834 ... -3.16957497 -5.3906408\n",
      "  -5.64171783]\n",
      " [-1.11929016 -3.4794087  -0.16812916 ... -6.08527067 -1.86064374\n",
      "  -0.81642122]\n",
      " [-0.7005121  -0.63887223 -3.10401799 ... -1.18949943 -1.12411436\n",
      "  -5.04263691]]\n",
      "MAP 0.7634802791814832\n",
      "total_loss:----------------- [[0.28909836]]\n",
      "train_auc:------------------- 0.8960908465244322\n",
      "time: 16380.37128329277  sec\n",
      "Current time: Mon Jul 20 06:05:13 2020\n",
      "Iteraction: 4\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8133333333333334 recall  0.14878048780487804\n",
      "F1_score: 0.2515463917525773\n",
      "top3\n",
      "prec  0.7177777777777777 recall  0.39390243902439026\n",
      "F1_score: 0.5086614173228347\n",
      "top5\n",
      "prec  0.6493333333333333 recall  0.5939024390243902\n",
      "F1_score: 0.6203821656050955\n",
      "NDCG: 0.835328215559256\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-1.76962352 -1.52421885 -2.56249789 ... -7.67691065 -4.85507415\n",
      "  -7.72449818]\n",
      " [-1.67750619 -6.81757622 -4.06531283 ... -5.841784   -3.48538437\n",
      "  -7.86953964]\n",
      " [-2.10216055 -1.50534786 -6.76242454 ... -8.16477875 -3.47825136\n",
      "  -3.26565506]\n",
      " ...\n",
      " [-4.00221991 -1.65662427 -2.99125746 ... -4.18007112 -6.55149544\n",
      "  -7.19324136]\n",
      " [-2.52893865 -5.49970329 -1.14426179 ... -7.55479767 -3.3028019\n",
      "  -1.84156843]\n",
      " [-2.13082787 -2.44187293 -4.98891402 ... -2.39350854 -2.27054896\n",
      "  -7.04139806]]\n",
      "MAP 0.7530241670586522\n",
      "total_loss:----------------- [[0.27780941]]\n",
      "train_auc:------------------- 0.9018513420509291\n",
      "time: 20478.330744743347  sec\n",
      "Current time: Mon Jul 20 07:13:31 2020\n",
      "Total cost  20478.330744743347  sec\n",
      "Finish Training: Image_50_2048D2f_lr5\n",
      "Start Testing:\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8133333333333334 recall  0.14878048780487804\n",
      "F1_score: 0.2515463917525773\n",
      "top3\n",
      "prec  0.7177777777777777 recall  0.39390243902439026\n",
      "F1_score: 0.5086614173228347\n",
      "top5\n",
      "prec  0.6493333333333333 recall  0.5939024390243902\n",
      "F1_score: 0.6203821656050955\n",
      "NDCG: 0.835328215559256\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[-1.76962352 -1.52421885 -2.56249789 ... -7.67691065 -4.85507415\n",
      "  -7.72449818]\n",
      " [-1.67750619 -6.81757622 -4.06531283 ... -5.841784   -3.48538437\n",
      "  -7.86953964]\n",
      " [-2.10216055 -1.50534786 -6.76242454 ... -8.16477875 -3.47825136\n",
      "  -3.26565506]\n",
      " ...\n",
      " [-4.00221991 -1.65662427 -2.99125746 ... -4.18007112 -6.55149544\n",
      "  -7.19324136]\n",
      " [-2.52893865 -5.49970329 -1.14426179 ... -7.55479767 -3.3028019\n",
      "  -1.84156843]\n",
      " [-2.13082787 -2.44187293 -4.98891402 ... -2.39350854 -2.27054896\n",
      "  -7.04139806]]\n",
      "MAP 0.7530241670586522\n"
     ]
    }
   ],
   "source": [
    "# two feature \n",
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "#l = all_3374.shape[1]\n",
    "try_count = [str(try_i) for try_i in range(3)]\n",
    "feature_name = ['Image_50_2048D','Following_64D','Text_300D','Video_512D_6650','social_Norm15D']\n",
    "token = 'lk9WjNIjAuMTd94qdDC1oDszn38qjfCY9PiD3nvrot0'\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "all_3374 = np.load('../Data/npy/'+'Image_50_2048D'+'.npy')\n",
    "for feature in feature_name[:1]:\n",
    "    #added_feature = np.load('../Data/npy/'+'Image_50_2048D'+'.npy')\n",
    "    #all_3374 = np.concatenate((all_3374,added_feature),axis=1)\n",
    "    if 'Image' not in feature:\n",
    "        added_feature = np.load('../Data/npy/'+feature+'.npy')\n",
    "        all_3374 = np.concatenate((all_3374,added_feature),axis=1)\n",
    "        l = all_3374.shape[1]\n",
    "    else:\n",
    "        l = all_3374.shape[1]\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(feature)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    lineNotifyMessage(token,feature)\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    \n",
    "                    \n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.1,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    merge_f = ''\n",
    "                    for fname in finish_list:\n",
    "                        merge_f=merge_f+fname\n",
    "                    save_name = feature+'2f_lr5'\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(save_name)\n",
    "                    #np.savez(save_name+'.npz', \n",
    "                    #    U=Ur, Y=Yr, A=Ar, E=Er, Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr,B=Br)\n",
    "                    print('Finish Training:',save_name)\n",
    "                    print('Start Testing:')\n",
    "                    msg = test_result(Ur, Yr, Ar, Er,Aur, Ayr, Aar, Avr,Br,save_name)\n",
    "                    lineNotifyMessage(token, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished []\n",
      "2412\n",
      "Now Dims: 200\n",
      "trainable_vars: 52251368\n",
      "Iteraction: 0\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.76 recall  0.13902439024390245\n",
      "F1_score: 0.2350515463917526\n",
      "top3\n",
      "prec  0.6755555555555556 recall  0.37073170731707317\n",
      "F1_score: 0.47874015748031495\n",
      "top5\n",
      "prec  0.6133333333333333 recall  0.5609756097560976\n",
      "F1_score: 0.5859872611464968\n",
      "NDCG: 0.807870115968919\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 3.89105998  4.71045011  3.44757846 ... -1.3784976   2.3919469\n",
      "  -0.85313245]\n",
      " [ 5.60259738  1.35754166  3.42157922 ...  0.72195657  3.76438557\n",
      "  -0.42103322]\n",
      " [ 4.44808289  5.61171707  1.36454939 ...  0.01375345  3.24774363\n",
      "   3.71948599]\n",
      " ...\n",
      " [ 3.89339791  5.59880204  3.67383495 ...  1.98135459 -0.1867937\n",
      "  -0.84950725]\n",
      " [ 4.41414475  2.20341532  5.5073417  ...  0.04393209  3.74122997\n",
      "   4.35113247]\n",
      " [ 3.92015022  3.47509333  2.2583844  ...  3.83901443  4.35154406\n",
      "   0.65909555]]\n",
      "MAP 0.7222743775507797\n",
      "total_loss:----------------- [[1.02802542]]\n",
      "train_auc:------------------- 0.8650722642807983\n",
      "time: 4246.5062074661255  sec\n",
      "Current time: Sun Jul 19 20:49:54 2020\n",
      "Iteraction: 1\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7666666666666667 recall  0.1402439024390244\n",
      "F1_score: 0.2371134020618557\n",
      "top3\n",
      "prec  0.6755555555555556 recall  0.37073170731707317\n",
      "F1_score: 0.47874015748031495\n",
      "top5\n",
      "prec  0.6026666666666667 recall  0.551219512195122\n",
      "F1_score: 0.575796178343949\n",
      "NDCG: 0.8044095095101099\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 2.10088578  2.97752009  1.72553026 ... -3.40820553  0.20005282\n",
      "  -2.23721105]\n",
      " [ 3.46641431 -0.35465111  1.73500378 ... -0.80448207  2.01872373\n",
      "  -2.21721328]\n",
      " [ 2.98737941  3.49756663 -0.33538192 ... -2.28682733  1.48181842\n",
      "   2.25668657]\n",
      " ...\n",
      " [ 1.99653553  3.47223199  1.77225225 ...  0.42924642 -1.95160232\n",
      "  -2.17527141]\n",
      " [ 2.87832285  0.74222733  3.7845434  ... -2.12519615  2.03395897\n",
      "   2.87162748]\n",
      " [ 2.11317405  1.85841781  0.86747308 ...  2.2823929   2.76726008\n",
      "  -0.75009209]]\n",
      "MAP 0.7151973132026704\n",
      "total_loss:----------------- [[0.34604971]]\n",
      "train_auc:------------------- 0.875712319339298\n",
      "time: 8464.953199863434  sec\n",
      "Current time: Sun Jul 19 22:00:13 2020\n",
      "Iteraction: 2\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7466666666666667 recall  0.13658536585365855\n",
      "F1_score: 0.2309278350515464\n",
      "top3\n",
      "prec  0.6755555555555556 recall  0.37073170731707317\n",
      "F1_score: 0.47874015748031495\n",
      "top5\n",
      "prec  0.6386666666666667 recall  0.5841463414634146\n",
      "F1_score: 0.6101910828025477\n",
      "NDCG: 0.8158928686320632\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 2.23779237  1.83790275  1.52625333 ... -4.0220566  -0.70629275\n",
      "  -3.40185255]\n",
      " [ 2.55990282 -1.0122672  -0.16307479 ... -1.60966985  0.8940582\n",
      "  -3.56403295]\n",
      " [ 1.7863466   2.64485153 -0.95170125 ... -3.10817706  1.14902288\n",
      "   1.62398072]\n",
      " ...\n",
      " [ 1.10669875  2.60452261  1.37918414 ... -0.22195161 -2.63915978\n",
      "  -3.22691404]\n",
      " [ 1.50582923  0.31548473  3.1368364  ... -2.65221432  0.96242322\n",
      "   2.61405876]\n",
      " [ 2.04665425  1.41784884  0.56104296 ...  1.45520478  2.37546317\n",
      "  -2.01693624]]\n",
      "MAP 0.7320630506443537\n",
      "total_loss:----------------- [[0.30489228]]\n",
      "train_auc:------------------- 0.8870887818306951\n",
      "time: 12697.400237560272  sec\n",
      "Current time: Sun Jul 19 23:10:45 2020\n",
      "Iteraction: 3\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.7733333333333333 recall  0.14146341463414633\n",
      "F1_score: 0.23917525773195872\n",
      "top3\n",
      "prec  0.7133333333333334 recall  0.39146341463414636\n",
      "F1_score: 0.505511811023622\n",
      "top5\n",
      "prec  0.6453333333333333 recall  0.5902439024390244\n",
      "F1_score: 0.6165605095541401\n",
      "NDCG: 0.8328481961891757\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.35956193  1.65075131  0.54098393 ... -5.68728837 -1.42397532\n",
      "  -5.41949415]\n",
      " [ 1.28645819 -3.5862144  -1.02098942 ... -3.33340562  0.02130854\n",
      "  -4.77037372]\n",
      " [ 0.55136064  1.34106502 -3.57528383 ... -5.9671795  -0.6658188\n",
      "  -0.37654635]\n",
      " ...\n",
      " [-0.93492752  1.32857794 -0.14695648 ... -1.22919596 -4.47135028\n",
      "  -5.07119246]\n",
      " [ 0.12882937 -1.48540127  1.54930788 ... -5.21210395 -0.10993666\n",
      "   0.50691267]\n",
      " [ 0.94981279  0.93417314 -1.08187995 ...  0.74203303  0.03711171\n",
      "  -3.45118999]]\n",
      "MAP 0.7481125049965862\n",
      "total_loss:----------------- [[0.28970589]]\n",
      "train_auc:------------------- 0.8955677907777013\n",
      "time: 16916.51829957962  sec\n",
      "Current time: Mon Jul 20 00:21:04 2020\n",
      "Iteraction: 4\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.86 recall  0.1573170731707317\n",
      "F1_score: 0.26597938144329897\n",
      "top3\n",
      "prec  0.7555555555555555 recall  0.4146341463414634\n",
      "F1_score: 0.5354330708661418\n",
      "top5\n",
      "prec  0.6706666666666666 recall  0.6134146341463415\n",
      "F1_score: 0.640764331210191\n",
      "NDCG: 0.8649949423123504\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.67253308  1.28890084  0.34500027 ... -5.28102254 -0.93442341\n",
      "  -5.63436202]\n",
      " [ 1.93804503 -2.97462024 -1.66658278 ... -3.96029103 -0.31731038\n",
      "  -4.53345011]\n",
      " [ 1.00170385  1.97112089 -2.95044126 ... -6.32150166 -0.22406124\n",
      "  -2.01839518]\n",
      " ...\n",
      " [-0.08758896  1.96427623  0.31634389 ... -1.39481196 -4.29208083\n",
      "  -5.16120636]\n",
      " [ 0.65358184 -1.14844968  1.88426936 ... -5.66205359 -0.28194287\n",
      "   0.03623545]\n",
      " [ 1.12748387  0.71646609 -0.77200063 ...  0.53000097 -0.41554377\n",
      "  -3.32275519]]\n",
      "MAP 0.786158776005006\n",
      "total_loss:----------------- [[0.27449305]]\n",
      "train_auc:------------------- 0.9043702684101859\n",
      "time: 21144.301100730896  sec\n",
      "Current time: Mon Jul 20 01:31:32 2020\n",
      "Total cost  21144.30209827423  sec\n",
      "Finish Training: Text_300D2f_lr5\n",
      "Start Testing:\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.86 recall  0.1573170731707317\n",
      "F1_score: 0.26597938144329897\n",
      "top3\n",
      "prec  0.7555555555555555 recall  0.4146341463414634\n",
      "F1_score: 0.5354330708661418\n",
      "top5\n",
      "prec  0.6706666666666666 recall  0.6134146341463415\n",
      "F1_score: 0.640764331210191\n",
      "NDCG: 0.8649949423123504\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.67253308  1.28890084  0.34500027 ... -5.28102254 -0.93442341\n",
      "  -5.63436202]\n",
      " [ 1.93804503 -2.97462024 -1.66658278 ... -3.96029103 -0.31731038\n",
      "  -4.53345011]\n",
      " [ 1.00170385  1.97112089 -2.95044126 ... -6.32150166 -0.22406124\n",
      "  -2.01839518]\n",
      " ...\n",
      " [-0.08758896  1.96427623  0.31634389 ... -1.39481196 -4.29208083\n",
      "  -5.16120636]\n",
      " [ 0.65358184 -1.14844968  1.88426936 ... -5.66205359 -0.28194287\n",
      "   0.03623545]\n",
      " [ 1.12748387  0.71646609 -0.77200063 ...  0.53000097 -0.41554377\n",
      "  -3.32275519]]\n",
      "MAP 0.786158776005006\n"
     ]
    }
   ],
   "source": [
    "# two feature \n",
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "#l = all_3374.shape[1]\n",
    "try_count = [str(try_i) for try_i in range(3)]\n",
    "feature_name = ['Image_50_2048D','Following_64D','Text_300D','Video_512D_6650','social_Norm15D']\n",
    "token = 'lk9WjNIjAuMTd94qdDC1oDszn38qjfCY9PiD3nvrot0'\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "all_3374 = np.load('../Data/npy/'+'Image_50_2048D'+'.npy')\n",
    "for feature in feature_name[2:3]:\n",
    "    added_feature = np.load('../Data/npy/'+'Following_64D'+'.npy')\n",
    "    all_3374 = np.concatenate((all_3374,added_feature),axis=1)\n",
    "    if 'Image' not in feature:\n",
    "        added_feature = np.load('../Data/npy/'+feature+'.npy')\n",
    "        all_3374 = np.concatenate((all_3374,added_feature),axis=1)\n",
    "        l = all_3374.shape[1]\n",
    "    else:\n",
    "        l = all_3374.shape[1]\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(feature)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    lineNotifyMessage(token,feature)\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    \n",
    "                    \n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.01,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.1,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    merge_f = ''\n",
    "                    for fname in finish_list:\n",
    "                        merge_f=merge_f+fname\n",
    "                    save_name = feature+'2f_lr5'\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(save_name)\n",
    "                    #np.savez(save_name+'.npz', \n",
    "                    #    U=Ur, Y=Yr, A=Ar, E=Er, Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr,B=Br)\n",
    "                    print('Finish Training:',save_name)\n",
    "                    print('Start Testing:')\n",
    "                    msg = test_result(Ur, Yr, Ar, Er,Aur, Ayr, Aar, Avr,Br,save_name)\n",
    "                    lineNotifyMessage(token, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing:\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 88)\n",
      "num of positive data in testing: 1566.0\n",
      "total testing data: 13200\n",
      "top1\n",
      "prec  0.6666666666666666 recall  0.06385696040868455\n",
      "F1_score: 0.11655011655011654\n",
      "top3\n",
      "prec  0.6288888888888889 recall  0.18071519795657726\n",
      "F1_score: 0.28075396825396826\n",
      "top5\n",
      "prec  0.5533333333333333 recall  0.26500638569604085\n",
      "F1_score: 0.35837651122625214\n",
      "NDCG: 0.5554036307895681\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 3.58257432 -0.03989087  1.26652611 ...  1.12680865 -2.8498265\n",
      "   1.51779637]\n",
      " [-0.15788601  3.04633398  1.80696653 ... -0.62859013 -1.93904234\n",
      "  -1.2463929 ]\n",
      " [-0.43799217  1.33742655  2.10023494 ... -3.30077979 -4.71472627\n",
      "  -4.46295095]\n",
      " ...\n",
      " [ 2.20092114 -1.24650185  1.10214703 ... -0.63112077 -3.47804188\n",
      "   2.06511248]\n",
      " [ 3.57297056  2.59130073  3.63515366 ...  1.80716184  1.25810559\n",
      "  -4.71762112]\n",
      " [ 1.39391975  1.71722844  2.06646979 ... -1.24310031 -2.07240378\n",
      "  -1.10408825]]\n",
      "MAP 0.5266694403828617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Start Testing:')\n",
    "msg = test_result(Ur, Yr, Ar, Er,Aur, Ayr, Aar, Avr,Br,save_name)\n",
    "lineNotifyMessage(token, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "all_3374 = np.load('../Data/npy/mask_feature/no_300image.npy')\n",
    "l = all_3374.shape[1]\n",
    "only_features = ['no_300image']\n",
    "try_count = [str(try_i) for try_i in range(10)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for only_feature in only_features:\n",
    "    all_3374 = np.load('../Data/npy/mask_feature/'+only_feature+'.npy')\n",
    "    l = all_3374.shape[1]\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(only_feature)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=4))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=6)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=8))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    \n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(only_feature+'9_E'+str(embedding_dims))\n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('../Data/npy/Image_50_2048D.npy')\n",
    "b = np.load('../Data/npy/Video_512D_6650.npy')\n",
    "#c = np.load('../Data/npy/social_Norm15D.npy')\n",
    "\n",
    "all_3374 = np.concatenate((a, b), axis=1)\n",
    "#all_3374=np.concatenate((all_3374, c), axis=1)\n",
    "l =all_3374.shape[1]\n",
    "print('all 3374 shape',all_3374.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finish list:[image+video,image+text,video+text,social_text,social_image,social_video, following_video, following_social,following_image,following_text]\n",
    "#finish list:[following_image_text,following_image_video,following_image_social,social_image_video,social_image_text]\n",
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "#Embedding_dims = [150,160,170,180,190,210,220,230,240,250]\n",
    "Embedding_dims = [200]\n",
    "single_features = ['Video','Text','Social','Following','Image']\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for paru_weight in par_weights:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for single_feature  in single_features:\n",
    "                    if 'Video' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/Video_512D_6650.npy')\n",
    "                    elif 'Text' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/Text_300D.npy')\n",
    "                    elif 'Social' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/social_15D.npy')\n",
    "                    elif 'Following' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/Following_64D.npy')\n",
    "                    elif 'Image' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/Image_50_2048D.npy')\n",
    "                        \n",
    "                    l = all_3374.shape[1]\n",
    "                    \n",
    "                    embedding_dims = 200\n",
    "                    clear_output()\n",
    "                    print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    print('Now feature:',single_feature,l)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    \n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                paru_weight * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(single_feature+'1_Edims200')\n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('../Data/npy/Image_50_2048D.npy')\n",
    "b = np.load('../Data/npy/Video_512D_6650.npy')\n",
    "c = np.load('../Data/npy/Text_300D.npy')\n",
    "d = np.load('../Data/npy/social_Norm15D.npy')\n",
    "temp1 = np.concatenate((a, b), axis=1)\n",
    "temp2=np.concatenate((c,d), axis=1)\n",
    "all_3374 = np.concatenate((temp1,temp2), axis=1)\n",
    "\n",
    "l =all_3374.shape[1]\n",
    "#finish list:[image+video,image+text,video+text,social_text,social_image,social_video, following_video, following_social,following_image,following_text]\n",
    "#finish list:[following_image_text,following_image_video,following_image_social,social_image_video,social_image_text,text_image_video,]\n",
    "#following_text_video,following_text_social, following_video_social\n",
    "#following,image,text,video,social\n",
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "#Embedding_dims = [150,160,170,180,190,210,220,230,240,250]\n",
    "Embedding_dims = [200]\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for paru_weight in par_weights:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    \n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                paru_weight * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training('Image_Video_Text_Social_Edims200')\n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(U, Y, A, E, A1, Au, Ay, Aa, Av,B):\n",
    "    #print(A1)\n",
    "    result=np.zeros((test_amount,88))\n",
    "    RS=np.zeros((test_amount,88))\n",
    "    #test_idx --> Test 的 index\n",
    "    max1 = 0\n",
    "    maxu = 0\n",
    "    maxy = 0\n",
    "    maxa = 0\n",
    "    maxv = 0\n",
    "    min1 = 100000000000000000\n",
    "    minu = 100000000000000000\n",
    "    miny = 100000000000000000\n",
    "    mina = 100000000000000000\n",
    "    minv = 100000000000000000\n",
    "    test_yes_id=[]\n",
    "    for s in range(test_amount):\n",
    "        print(s,test_idx[s])\n",
    "\n",
    "        yes=[]\n",
    "        sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "        #sample=result_yes_id[now]\n",
    "        test_yes_id.append(sample)\n",
    "        alpha=np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "            #print(test_idx[s])\n",
    "            #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "\n",
    "            #Observe each part in attention , below par are all (128,1)\n",
    "            testW1 = np.sum(A1[test_idx[s]])\n",
    "            #print(A1[test_idx[s]])\n",
    "            WuUu = np.sum(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T))\n",
    "            WyYy = np.sum(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T))\n",
    "            WaAa = np.sum(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T))\n",
    "            WvVy = np.sum(np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))\n",
    "            print('The sum of each par -->\\nw1:',testW1,'\\nWuU:',WuUu,'\\nwyY:',WyYy,'\\nWaA:',WaAa,'\\nWvV:',WvVy)\n",
    "            if testW1 > max1:\n",
    "                max1 = testW1\n",
    "            if testW1 < min1:\n",
    "                min1 = testW1\n",
    "            if WuUu > maxu:\n",
    "                maxu = WuUu\n",
    "            if WuUu < minu:\n",
    "                minu = WuUu\n",
    "            if WyYy > maxy:\n",
    "                maxy = WyYy\n",
    "            if WyYy < miny:\n",
    "                miny = WyYy\n",
    "            if WaAa > maxa:\n",
    "                maxa = WaAa\n",
    "            if WaAa < mina:\n",
    "                mina = WaAa\n",
    "            if WvVy > maxv:\n",
    "                mxv = WvVy\n",
    "            if WvVy < minv:\n",
    "                minv = WvVy\n",
    "            #Have w1\n",
    "            #alpha[a]=np.dot(A1[test_idx[s]],(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "            #                    np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "            #Without w1\n",
    "            alpha[a]=np.sum((relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                                np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "        mul=np.zeros((1,128))\n",
    "        print('alpha------------',alpha)\n",
    "        print('softmax alpha--------------',softmax(alpha))\n",
    "        for i in range(len(sample)):\n",
    "            mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "        new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "        for k in range(88):\n",
    "            result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]],np.dot(E, all_3374[k].T))\n",
    "    #print(max1,maxu,maxy,maxa,maxv)\n",
    "    #print(min1,minu,miny,mina,minv)\n",
    "    return RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "#從grid_search_weight中找尋不同的file \n",
    "path = '../Data/grid_search_weight/0105/'\n",
    "files = os.listdir(path)\n",
    "#U, Y, A, E, A1, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, W1, Wu, Wy, Wa, Wv,Beta])\n",
    "for file in files:\n",
    "    par_data = np.load(path+file)\n",
    "    U = par_data['U']\n",
    "    Y = par_data['Y']\n",
    "    A = par_data['A']\n",
    "    E = par_data['E']\n",
    "    W1 = par_data['W1']\n",
    "    Wu = par_data['Wu']\n",
    "    Wy = par_data['Wy']\n",
    "    Wa = par_data['Wa']\n",
    "    Wv = par_data['Wv']\n",
    "    B = par_data['B']\n",
    "    RS = testing(U, Y, A, E, W1, Wu, Wy, Wa, Wv,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Y, A, E, A1, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, W1, Wu, Wy, Wa, Wv,Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('W1 weight shape: ',A1.shape)\n",
    "print('Wu weight shape:',Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:',Aa.shape)\n",
    "print('Wv weight shape:',Av.shape)\n",
    "print('Beta shape:',B.shape)\n",
    "print('Embedding shape:',E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=np.zeros((test_amount,88))\n",
    "RS=np.zeros((test_amount,88))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id=[]\n",
    "for s in range(test_amount):\n",
    "    print(s,test_idx[s])\n",
    "\n",
    "    yes=[]\n",
    "    sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha=np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        #print(test_idx[s])\n",
    "        #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "        \n",
    "        #Observe each part in attention , below par are all (128,1)\n",
    "        testW1 = np.sum(np.multiply(A1[test_idx[s]],A1[test_idx[s]]))\n",
    "        WuUu = np.sum(np.multiply(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T),np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)))\n",
    "        WyYy = np.sum(np.multiply(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T),np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)))\n",
    "        WaAa = np.sum(np.multiply(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T),np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T)))\n",
    "        WvVy = np.sum(np.multiply(np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T),np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T)))\n",
    "        print('w1:',testW1,'\\nWuU:',WuUu,'\\nwyY:',WyYy,'\\nWaA:',WaAa,'\\nWvV:',WvVy)\n",
    "        \n",
    "        alpha[a]=np.dot(A1[test_idx[s]],(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                            np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "    mul=np.zeros((1,128))\n",
    "    print('alpha------------',alpha)\n",
    "    print('softmax alpha--------------',softmax(alpha))\n",
    "    for i in range(len(sample)):\n",
    "        mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    for k in range(88):\n",
    "        result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]],np.dot(E, all_3374[k].T))\n",
    "#print(RS[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取出test的資料\n",
    "testRS = np.zeros((test_amount,yt_test_amount)) #shape 150*20\n",
    "target = np.zeros((test_amount,yt_test_amount))\n",
    "#test_t 是true的\n",
    "#test_f 是false的\n",
    "        \n",
    "for z in range(test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    #positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    #not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "    #前兩個放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumtarget = 0\n",
    "for i in range(len(target)):\n",
    "    #print(np.sum(target[i]))\n",
    "    sumtarget += np.sum(target[i])\n",
    "print('num of positive data in testing:',sumtarget)\n",
    "print('total testing data:',test_amount*yt_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),int(np.sum(target[i])))\n",
    "    count_0_all.append(top_0)\n",
    "    print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = 0\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        #print(int(np.sum(target[i])))\n",
    "        total+=int(np.sum(target[i]))\n",
    "        if count_0_all[i][j] < int(np.sum(target[i])): #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "avg_acc = acc_0/100\n",
    "print('avg_accuarcy for count_0:',avg_acc)\n",
    "print(acc_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),1) #取一個\n",
    "    count_0_all.append(top_0)\n",
    "    print(top_0)\n",
    "    if top_0[0] < int(np.sum(target[i])):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_prec = correct/len(testRS)\n",
    "top1_recall = correct/(sumtarget)\n",
    "print('prec ',top1_prec,'recall ',top1_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top1_prec,top1_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_3 = topN(list(testRS[i]),3) #取一個\n",
    "    count_0_all.append(top_3)\n",
    "    #print(top_3)\n",
    "    for j in range(len(top_3)):\n",
    "        if top_3[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_prec = correct/(len(testRS)*3)\n",
    "top3_recall = correct/(sumtarget)\n",
    "print('prec ',top3_prec,'recall ',top3_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top3_prec,top3_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    count_0_all.append(top_5)\n",
    "    #print(top_5)\n",
    "    for j in range(len(top_5)):\n",
    "        if top_5[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_prec = correct/(len(testRS)*5)\n",
    "top5_recall = correct/(sumtarget)\n",
    "print('prec ',top5_prec,'recall ',top5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top5_prec,top5_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User_latent_factor = loaddata['User']\n",
    "#YouTuber_latent_factor = loaddata['YouTuber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('../Data/latent_factor/YRM_up10_ALL/Final/1226.npz', User=U, YouTuber=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax([-0.000000000000000000000000000000000000001,0.00000000000000000000000000000000000001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = tf.constant([1.1,2.7,6.1,4.7,5.5,6.5])\n",
    "B = tf.divide(C,tf.reduce_sum(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryy = np.array([1,2,3,4,5])\n",
    "np.add(tryy,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.prod((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
