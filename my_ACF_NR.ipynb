{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "from sklearn.metrics import average_precision_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def lineNotifyMessage(token, msg):\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer \" + token, \n",
    "        \"Content-Type\" : \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "\n",
    "    payload = {'message': msg}\n",
    "    r = requests.post(\"https://notify-api.line.me/api/notify\", headers = headers, params = payload)\n",
    "    return r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (88, 2048)\n"
     ]
    }
   ],
   "source": [
    "user_following = np.load('../Data/npy/user_following_1489.npy')\n",
    "all_3374 = np.load('../Data/npy/Image_50_2048D.npy')\n",
    "active_users = np.load('../Data/npy/active_userID_1489.npy')\n",
    "print('Feature shape:',all_3374.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test_amount = 150\n",
    "yt_test_amount = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive 0.2\n",
    "def generate_train_test(user_following,feature,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = feature.shape[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(5)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    test_pos = -1\n",
    "    for i in range(num_of_users):\n",
    "        t_for_train = []\n",
    "        f_for_train = []\n",
    "        if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    t_for_train.append(j)\n",
    "                else:\n",
    "                    f_for_train.append(j)\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "\n",
    "        else: #if in test id, choose 2 true and other \n",
    "            test_pos += 1\n",
    "            temp_t = []\n",
    "            temp_f = []\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    temp_t.append(j)\n",
    "                else:\n",
    "                    temp_f.append(j)\n",
    "                \n",
    "            t_for_test = random.sample(temp_t,math.ceil(0.2*len(temp_t)))\n",
    "            f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 0.2\n",
    "def generate_train_test(user_following,feature,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(33)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    test_pos = -1\n",
    "    for i in range(num_of_users):\n",
    "        t_for_train = []\n",
    "        f_for_train = []\n",
    "        if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    t_for_train.append(j)\n",
    "                else:\n",
    "                    f_for_train.append(j)\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "\n",
    "        else: #if in test id, choose 2 true and other \n",
    "            all_yts = [x for x in range(88)]\n",
    "            test_yt = random.sample(all_yts,math.ceil(0.2*len(all_yts)))\n",
    "            print(test_yt)\n",
    "            test_pos += 1\n",
    "            temp_t = []\n",
    "            temp_f = []\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    temp_t.append(j)\n",
    "                else:\n",
    "                    temp_f.append(j)\n",
    "            #print(temp_t)\n",
    "            #print(temp_f)\n",
    "            #t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "            #f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "            t_for_test = [item for item in test_yt if item in temp_t]\n",
    "            f_for_test = [item for item in test_yt if item in temp_f]\n",
    "            print(i,len(t_for_test)+len(f_for_test))\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original \n",
    "def generate_train_test(user_following,feature,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = user_following[1]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(5)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    test_pos = -1\n",
    "    for i in range(num_of_users):\n",
    "        t_for_train = []\n",
    "        f_for_train = []\n",
    "        if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    t_for_train.append(j)\n",
    "                else:\n",
    "                    f_for_train.append(j)\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "\n",
    "        else: #if in test id, choose 2 true and other \n",
    "            test_pos += 1\n",
    "            temp_t = []\n",
    "            temp_f = []\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    temp_t.append(j)\n",
    "                else:\n",
    "                    temp_f.append(j)\n",
    "            t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "            f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LHO \n",
    "#original \n",
    "def generate_train_test(user_following,feature,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = user_following[1]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(5)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    \n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    \n",
    "    data_split_path = 'D:/ChilliHsu/Data/Result_Matrix/Dims200_Text300/'\n",
    "    with open(data_split_path+'test_t.json') as json_file:\n",
    "        test_t = json.load(json_file)\n",
    "    with open(data_split_path+'test_f.json') as json_file:\n",
    "        test_f = json.load(json_file)\n",
    "    with open(data_split_path+'train_t.json') as json_file:\n",
    "        train_t = json.load(json_file)\n",
    "    with open(data_split_path+'train_f.json') as json_file:\n",
    "        train_f = json.load(json_file)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate_train_test----\n",
      "user_following shape  (1489, 88)\n",
      "feature shape  (88, 2048)\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "test_idx [3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "train_t,train_f,test_t,test_f,test_idx = generate_train_test(user_following,all_3374,active_users,user_test_amount,yt_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 9.758226997985226\n",
      "testing 5.466666666666667\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train/len(user_following)\n",
    "print('training',avg)\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test/user_test_amount\n",
    "print('testing',avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(88)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(user_following)\n",
    "m = 88  \n",
    "k = 64\n",
    "l = all_3374.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                train_f_sample = random.sample(train_f[z],10)\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    _a_list,r3,_auc, _loss,_=sess.run([a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "        print('Current time:',time.ctime())\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A,E,Au, Ay, Aa, Av =sess.run([user_latent, item_latent, aux_item, embedding,Wu, Wy, Wa, Wv])\n",
    "    #np.savez('../Data/grid_search_weight/acf/'+save_name+'.npz', \n",
    "    #                    U=U, Y=Y, A=A,E=E,Wu=Au, Wy=Ay, Wa=Aa, Wv=Av)\n",
    "    return U, Y, A, E, Au, Ay, Aa, Av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amount = user_test_amount\n",
    "def getScoreMatrix(RS):\n",
    "    #取出test的資料\n",
    "    testRS = np.zeros((test_amount,yt_test_amount)) #shape 150*18\n",
    "    target = np.zeros((test_amount,yt_test_amount))\n",
    "    print(testRS.shape)\n",
    "    #test_t 是true的\n",
    "    #test_f 是false的\n",
    "\n",
    "    for z in range(test_amount):\n",
    "        user_id = test_idx[z]\n",
    "        #positive target YouTuber list\n",
    "        youtube_t = test_t[z] \n",
    "        #not target YouTuber list\n",
    "        youtube_f = test_f[z]\n",
    "        #print(user_id)\n",
    "        #print(youtube_t)\n",
    "        #print(user_following[user_id])\n",
    "        #前兩個放target的RS\n",
    "        for i in range(len(youtube_t)):\n",
    "            testRS[z][i] = RS[z][youtube_t[i]]\n",
    "            target[z][i] = user_following[user_id][youtube_t[i]]\n",
    "        for i in range(len(youtube_f)):\n",
    "            testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "            target[z][i+len(youtube_t)] = user_following[user_id][youtube_f[i]]\n",
    "        #print('testRS for user',user_id,testRS[z],youtube_t+youtube_f)\n",
    "        \n",
    "    sumtarget = 0\n",
    "    for i in range(len(target)):\n",
    "        #print(np.sum(target[i]))\n",
    "        sumtarget += np.sum(target[i])\n",
    "    print('num of positive data in testing:',sumtarget)\n",
    "    print('total testing data:',test_amount*yt_test_amount)\n",
    "    \n",
    "    return target, testRS,sumtarget\n",
    "\n",
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList\n",
    "\n",
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "def getTOP1(target,testRS,sumtarget):\n",
    "    print('top1')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_0 = topN(list(testRS[i]),1) #取一個\n",
    "        \n",
    "        #print(top_0)\n",
    "        if top_0[0] < int(np.sum(target[i])):\n",
    "            correct += 1\n",
    "    top1_prec = correct/len(testRS)\n",
    "    top1_recall = correct/(sumtarget)\n",
    "    print('prec ',top1_prec,'recall ',top1_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top1_prec,top1_recall))\n",
    "    return top1_prec,top1_recall,F1_score(top1_prec,top1_recall)\n",
    "def getTOP3(target,testRS,sumtarget):\n",
    "    print('top3')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_3 = topN(list(testRS[i]),3) #取一個\n",
    "        \n",
    "        #print(top_3)\n",
    "        for j in range(len(top_3)):\n",
    "            if top_3[j] < int(np.sum(target[i])):\n",
    "                correct += 1\n",
    "    top3_prec = correct/(len(testRS)*3)\n",
    "    top3_recall = correct/(sumtarget)\n",
    "    print('prec ',top3_prec,'recall ',top3_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top3_prec,top3_recall))\n",
    "    return top3_prec,top3_recall,F1_score(top3_prec,top3_recall)\n",
    "def getTOP5(target,testRS,sumtarget):\n",
    "    print('top5')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_5 = topN(list(testRS[i]),5) #取一個\n",
    "       \n",
    "        #print(top_5)\n",
    "        for j in range(len(top_5)):\n",
    "            if top_5[j] < int(np.sum(target[i])):\n",
    "                correct += 1\n",
    "    top5_prec = correct/(len(testRS)*5)\n",
    "    top5_recall = correct/(sumtarget)\n",
    "    print('prec ',top5_prec,'recall ',top5_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top5_prec,top5_recall))\n",
    "    return top5_prec,top5_recall,F1_score(top5_prec,top5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NDCG\n",
    "https://daiwk.github.io/posts/nlp-ndcg.html\n",
    "\"\"\"\n",
    "# pre_list\n",
    "\"\"\"\n",
    "test_amount = 150\n",
    "yt_test_amount = 18\n",
    "\"\"\"\n",
    "def NDCG(target,testRS): #target是真正的喜好\n",
    "    all_sort = []\n",
    "    num_ndcg = 10\n",
    "    pre_matrix = np.zeros(shape=(test_amount,yt_test_amount)) #(150,18)\n",
    "    for i in range(test_amount): #user amount = 150\n",
    "        top_n = topN(list(testRS[i]),num_ndcg) #取10個\n",
    "        #print(top_n)\n",
    "        all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "        #print('all_sort',topN(list(testRS[i]),len(testRS[i])))\n",
    "        for j in range(len(top_n)):\n",
    "            pre_matrix[i][top_n[j]] = 1\n",
    "\n",
    "    #Ideal DCG，理想状况下的DCG。也就是说，相关性完全由高到低排序时算出的DCG：\n",
    "    def IDCG(ideal_list): #ideal_list example = [1,1,1,1,1,0,0,....]\n",
    "        idcg=0\n",
    "        #print('ideal',ideal_list)\n",
    "        for i in range(len(ideal_list)):\n",
    "            #print((2**true_list[i]-1),math.log2(i+2))\n",
    "            idcg+= (2**ideal_list[i]-1)/ math.log2(i+2)\n",
    "        #print('idcg',idcg)\n",
    "        return idcg\n",
    "    def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "        dcg=0\n",
    "        #print('prec',prec_list)\n",
    "        for i in range(len(prec_list)):\n",
    "            dcg+= (2**prec_list[i]-1)/ math.log2(i+2)\n",
    "        #print('dcg',dcg)\n",
    "        return dcg\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    for m in range(test_amount): # the number of testing users\n",
    "        #print('all-sort',all_sort[m][:num_ndcg])\n",
    "        idcg = IDCG(target[m][:num_ndcg])\n",
    "        pre_list = []\n",
    "        least_pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "            pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        for s in all_sort[m][num_ndcg:]:\n",
    "            #print(s)\n",
    "            #print(target[m][s])\n",
    "            least_pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "        #print(ndcg)\n",
    "        total_ndcg += ndcg\n",
    "    avg_ndcg = total_ndcg/test_amount\n",
    "    print('NDCG:',avg_ndcg)\n",
    "    return pre_matrix,avg_ndcg\n",
    "\n",
    "# MAP\n",
    "\"\"\"\n",
    ">>> y_true = np.array([0, 0, 1, 1])\n",
    ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    ">>> average_precision_score(y_true, y_scores)\n",
    "\"\"\"\n",
    "def MAP(target,testRS):\n",
    "    print('target:',target)\n",
    "    print('testRS:',testRS)\n",
    "    total_prec = 0\n",
    "    for u in range(test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec+=average_precision_score(y_true, y_scores)\n",
    "    Map_value = total_prec/test_amount\n",
    "    print('MAP',Map_value)\n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(U, Y, A, E,Au, Ay, Aa, Av):\n",
    "    test_amount = 150\n",
    "    yt_test_amount = 18\n",
    "    result=np.zeros((test_amount,88))\n",
    "    RS=np.zeros((test_amount,88))\n",
    "    #test_idx --> Test 的 index\n",
    "    print(test_idx)\n",
    "    sum_alpha = 0\n",
    "    for s in range(test_amount):\n",
    "        #print(s,test_idx[s])\n",
    "\n",
    "        yes=[]\n",
    "        sample = [i for i in range(88)]\n",
    "        alpha=np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            alpha_a = np.dot(Au[test_idx[s]][sample[a]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[test_idx[s]][sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]][sample[a]],\n",
    "                    np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]][sample[a]],np.dot(E,np.expand_dims(all_3374[sample[a]],0).T))\n",
    "            \"\"\"\n",
    "            relu part ...\n",
    "            \"\"\"\n",
    "            alpha[a]=np.sum((relu(alpha_a)))\n",
    "            \"\"\"\n",
    "            tanh part ...\n",
    "            \"\"\"\n",
    "            #alpha[a]=np.sum((np.tanh(alpha_a)))*r\n",
    "            \n",
    "            \n",
    "        mul=np.zeros((1,64))\n",
    "        #print('alpha--------',alpha)\n",
    "        #print('add alpha------------',np.add(alpha,0.000000001))\n",
    "        added_alpha = np.add(alpha,0.0000000001)\n",
    "        norm_alpha = added_alpha/np.sum(added_alpha)\n",
    "        #print('alpha-----------',alpha)\n",
    "        #print('norm alpha--------------',norm_alpha)\n",
    "        sum_alpha += np.sum(alpha)\n",
    "        for i in range(len(sample)):\n",
    "            mul+=norm_alpha[i]*A[sample[i]] #attention alpha*Ai part\n",
    "        new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "        \n",
    "        \n",
    "        for k in range(88):\n",
    "            result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T)\n",
    "            \n",
    "    return RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_result(U, Y, A, E,Wu, Wy, Wa, Wv,save_name):\n",
    "    RS = testing(U, Y, A, E,Wu, Wy, Wa, Wv)\n",
    "    target,testRS,sumtarget = getScoreMatrix(RS)\n",
    "    prec_1,recall_1,f1_1 = getTOP1(target,testRS,sumtarget)\n",
    "    prec_3,recall_3,f1_3 = getTOP3(target,testRS,sumtarget)\n",
    "    prec_5,recall_5,f1_5 = getTOP5(target,testRS,sumtarget)\n",
    "    pre_matrix,avg_ndcg = NDCG(target,testRS)\n",
    "    Map_value = MAP(target,testRS)\n",
    "    msg = save_name+'\\nNDCG:'+str(round(avg_ndcg,3))+'\\n'+'MAP:'+str(round(Map_value,3))+'\\nTop1: prec'+str(round(prec_1,3))+' rec'+str(round(recall_1,3))+' f1'+str(round(f1_1,3))+'\\nTop3: prec'+str(round(prec_3,3))+' rec'+str(round(recall_3,3))+' f1'+str(round(f1_3,3))+'\\nTop5: prec'+str(round(prec_5,3))+' rec'+str(round(recall_5,3))+' f1'+str(round(f1_5,3))\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-19-783925155bc2>:150: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.66810779]]\n",
      "train_auc:------------------- 0.6537439779766001\n",
      "time: 3696.9656760692596  sec\n",
      "Current time: Fri Jun 26 18:38:58 2020\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.59611499]]\n",
      "train_auc:------------------- 0.807171369580179\n",
      "time: 7389.220627069473  sec\n",
      "Current time: Fri Jun 26 19:40:31 2020\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.52210325]]\n",
      "train_auc:------------------- 0.8472401927047488\n",
      "time: 11080.815837144852  sec\n",
      "Current time: Fri Jun 26 20:42:02 2020\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.45655852]]\n",
      "train_auc:------------------- 0.8552030282174811\n",
      "time: 14770.597996234894  sec\n",
      "Current time: Fri Jun 26 21:43:32 2020\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.41558161]]\n",
      "train_auc:------------------- 0.8582381280110117\n",
      "time: 18462.950731277466  sec\n",
      "Current time: Fri Jun 26 22:45:04 2020\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.39706964]]\n",
      "train_auc:------------------- 0.8557536132140399\n",
      "time: 22153.235005140305  sec\n",
      "Current time: Fri Jun 26 23:46:35 2020\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.38851468]]\n",
      "train_auc:------------------- 0.8560495526496903\n",
      "time: 25844.297674179077  sec\n",
      "Current time: Sat Jun 27 00:48:06 2020\n",
      "Total cost  25844.297674179077  sec\n",
      "Finish dims:, 200\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 44)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 6600\n",
      "top1\n",
      "prec  0.82 recall  0.15\n",
      "F1_score: 0.2536082474226804\n",
      "top3\n",
      "prec  0.7422222222222222 recall  0.4073170731707317\n",
      "F1_score: 0.525984251968504\n",
      "top5\n",
      "prec  0.6626666666666666 recall  0.6060975609756097\n",
      "F1_score: 0.6331210191082802\n",
      "NDCG: 0.8280999526465236\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 1.43780483  0.12801019  0.65864787 ... -0.8435314  -1.4189068\n",
      "  -1.4511899 ]\n",
      " [ 0.94042553  1.08744334  1.48395989 ... -0.4186882  -1.7245034\n",
      "  -1.73393794]\n",
      " [-0.86812738  0.99668504  1.08066389 ... -0.38456576 -1.65851674\n",
      "  -1.66114725]\n",
      " ...\n",
      " [ 0.40579819  1.04229725  0.42937516 ... -1.66815517 -0.37738198\n",
      "  -1.60834326]\n",
      " [ 1.99822337  0.52655281  1.6402493  ...  0.82142511 -1.84123117\n",
      "  -1.10032668]\n",
      " [ 0.80617217  0.34277729  1.38031141 ...  0.90478391 -2.08394626\n",
      "  -1.27499847]]\n",
      "MAP 0.7573204159446938\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "token = 'lk9WjNIjAuMTd94qdDC1oDszn38qjfCY9PiD3nvrot0'\n",
    "count_try = [str(i+1) for i in range(2)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in count_try:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    #print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    #print('Now Dims:',embedding_dims)\n",
    "                    print(try_i)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "                    lineNotifyMessage(token, 'ACF, All testing user positive feedback throwing into testing part, not leave half for training ')\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        with tf.variable_scope('feature_level'):\n",
    "                            embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    save_name = 'ACF_NR_machine5'+try_i\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr = training(save_name)\n",
    "                    np.savez('../Data/grid_search_weight/our/new/'+save_name+'.npz', U=Ur, Y=Yr, A=Ar,E=Er,Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr)\n",
    "                    print('Finish dims:,',embedding_dims)\n",
    "                    \n",
    "\n",
    "                    msg = test_result(Ur, Yr, Ar, Er,Aur, Ayr, Aar, Avr,save_name)\n",
    "                    lineNotifyMessage(token, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.72979676]]\n",
      "train_auc:------------------- 0.522869924294563\n",
      "time: 3653.1916241645813  sec\n",
      "Current time: Mon Jun 29 23:10:51 2020\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.68506904]]\n",
      "train_auc:------------------- 0.569002064693737\n",
      "time: 7286.273665189743  sec\n",
      "Current time: Tue Jun 30 00:11:24 2020\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.67795004]]\n",
      "train_auc:------------------- 0.6135994494150034\n",
      "time: 10920.95241689682  sec\n",
      "Current time: Tue Jun 30 01:11:59 2020\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.67126043]]\n",
      "train_auc:------------------- 0.6526428079834824\n",
      "time: 14556.3203125  sec\n",
      "Current time: Tue Jun 30 02:12:34 2020\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.66452765]]\n",
      "train_auc:------------------- 0.6825671025464556\n",
      "time: 18190.97221636772  sec\n",
      "Current time: Tue Jun 30 03:13:09 2020\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.65792859]]\n",
      "train_auc:------------------- 0.7062904335856848\n",
      "time: 21827.995994091034  sec\n",
      "Current time: Tue Jun 30 04:13:46 2020\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.65078996]]\n",
      "train_auc:------------------- 0.7303372333103922\n",
      "time: 25462.409260749817  sec\n",
      "Current time: Tue Jun 30 05:14:20 2020\n",
      "Total cost  25462.409260749817  sec\n",
      "Finish dims:, 200\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.58 recall  0.10609756097560975\n",
      "F1_score: 0.17938144329896907\n",
      "top3\n",
      "prec  0.5466666666666666 recall  0.3\n",
      "F1_score: 0.3874015748031496\n",
      "top5\n",
      "prec  0.48933333333333334 recall  0.4475609756097561\n",
      "F1_score: 0.467515923566879\n",
      "NDCG: 0.656879368031127\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.13303352  0.01474417 -0.07275531 ...  0.0066621   0.05966189\n",
      "   0.02027824]\n",
      " [ 0.17775734  0.31273988  0.23760394 ...  0.1258373  -0.05684089\n",
      "  -0.10259317]\n",
      " [-0.06672562  0.05417287  0.17187974 ... -0.16993897 -0.00276807\n",
      "  -0.10071258]\n",
      " ...\n",
      " [ 0.03299548  0.13277727 -0.02266737 ...  0.0318236  -0.05062729\n",
      "  -0.03032979]\n",
      " [ 0.38167137  0.11378208  0.21526182 ...  0.13371495 -0.06859975\n",
      "  -0.18694365]\n",
      " [-0.0864918  -0.03072231  0.17033854 ... -0.08904226 -0.0921753\n",
      "   0.03028312]]\n",
      "MAP 0.598245977090543\n",
      "2\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.72998543]]\n",
      "train_auc:------------------- 0.5217068134893325\n",
      "time: 3657.917559862137  sec\n",
      "Current time: Tue Jun 30 06:15:21 2020\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.68511592]]\n",
      "train_auc:------------------- 0.5677150722642808\n",
      "time: 7315.281385183334  sec\n",
      "Current time: Tue Jun 30 07:16:19 2020\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6780907]]\n",
      "train_auc:------------------- 0.6135375086028906\n",
      "time: 10972.706417322159  sec\n",
      "Current time: Tue Jun 30 08:17:16 2020\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.67110563]]\n",
      "train_auc:------------------- 0.6535375086028906\n",
      "time: 14631.998736143112  sec\n",
      "Current time: Tue Jun 30 09:18:15 2020\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.66432419]]\n",
      "train_auc:------------------- 0.6834686854783207\n",
      "time: 18292.994803905487  sec\n",
      "Current time: Tue Jun 30 10:19:16 2020\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.65777389]]\n",
      "train_auc:------------------- 0.7078389538885065\n",
      "time: 21961.253708600998  sec\n",
      "Current time: Tue Jun 30 11:20:25 2020\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.65074178]]\n",
      "train_auc:------------------- 0.7301927047487956\n",
      "time: 25622.191397190094  sec\n",
      "Current time: Tue Jun 30 12:21:26 2020\n",
      "Total cost  25622.191397190094  sec\n",
      "Finish dims:, 200\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "(150, 18)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.62 recall  0.11341463414634147\n",
      "F1_score: 0.19175257731958764\n",
      "top3\n",
      "prec  0.5644444444444444 recall  0.3097560975609756\n",
      "F1_score: 0.4\n",
      "top5\n",
      "prec  0.49733333333333335 recall  0.4548780487804878\n",
      "F1_score: 0.4751592356687898\n",
      "NDCG: 0.6684897105977202\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[ 0.22846265  0.01175844 -0.09900748 ... -0.04707607  0.04942723\n",
      "   0.01201991]\n",
      " [ 0.18775869  0.37730024  0.26966079 ...  0.16969754 -0.07509573\n",
      "  -0.13421223]\n",
      " [-0.06714542  0.01913032  0.18399292 ... -0.14513795 -0.009788\n",
      "  -0.07730195]\n",
      " ...\n",
      " [ 0.03066938  0.18379566 -0.02845328 ...  0.03879047 -0.05483143\n",
      "  -0.03620502]\n",
      " [ 0.43349353  0.0959912   0.2609191  ...  0.17277776 -0.07536826\n",
      "  -0.20059813]\n",
      " [-0.04881223 -0.03920444  0.23051582 ... -0.13286426 -0.10180372\n",
      "   0.05862278]]\n",
      "MAP 0.6094076683821756\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "token = 'lk9WjNIjAuMTd94qdDC1oDszn38qjfCY9PiD3nvrot0'\n",
    "count_try = [str(i+1) for i in range(2)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in count_try:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    #print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    #print('Now Dims:',embedding_dims)\n",
    "                    print(try_i)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "                    lineNotifyMessage(token, '目前論文裡的，ACF')\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        with tf.variable_scope('feature_level'):\n",
    "                            embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    save_name = 'ACF_NR_ori_lr5'+try_i\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr = training(save_name)\n",
    "                    np.savez('../Data/grid_search_weight/our/new/'+save_name+'.npz', U=Ur, Y=Yr, A=Ar,E=Er,Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr)\n",
    "                    print('Finish dims:,',embedding_dims)\n",
    "                    \n",
    "\n",
    "                    msg = test_result(Ur, Yr, Ar, Er,Aur, Ayr, Aar, Avr,save_name)\n",
    "                    lineNotifyMessage(token, msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import average_precision_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_sum(pars):\n",
    "    all_squ_sum = []\n",
    "    for par in pars:\n",
    "        all_squ_sum.append(np.sum(np.multiply(par,par)))\n",
    "    return all_squ_sum \n",
    "def shape_sum(pars):\n",
    "    all_shape = []\n",
    "    for par in pars:\n",
    "        all_shape.append(par.shape)\n",
    "    return all_shape\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "following_true = [0]*len(user_following)\n",
    "for i in range(len(user_following)):\n",
    "    each_user = []\n",
    "    for j in range(len(user_following[i])):\n",
    "        if user_following[i][j] == 1:\n",
    "            each_user.append(j)\n",
    "    following_true[i] = each_user\n",
    "#print(following_true)\n",
    "#最少跟最多的following \n",
    "minlen = 10000\n",
    "maxlen = 0\n",
    "num_of_follower = []\n",
    "for i in range(len(following_true)):\n",
    "    if len(following_true[i]) < minlen:\n",
    "        minlen = len(following_true[i])\n",
    "    if len(following_true[i]) > maxlen:\n",
    "        maxlen = len(following_true[i])\n",
    "    num_of_follower.append(len(following_true[i]))\n",
    "print('Min number of followings ',minlen)\n",
    "print('Max number of followings ',maxlen)\n",
    "test_amount = 150\n",
    "yt_test_amount = 18\n",
    "user_idx = [i for i in range(len(user_following))]\n",
    "#user_idx = user_idx_over10\n",
    "#test_idx is the number of user for testing\n",
    "random.seed(5)\n",
    "test_idx = sorted(random.sample(user_idx,test_amount))\n",
    "print(test_idx)\n",
    "# Training  and Testing --New\n",
    "train_t = [0]*(len(user_following))\n",
    "train_f = [0]*(len(user_following))\n",
    "# Testing \n",
    "test_t = [0]*test_amount\n",
    "test_f = [0]*test_amount\n",
    "test_pos = -1\n",
    "\n",
    "for i in range(len(user_following)):\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train\n",
    "        \n",
    "    else: #if in test id, choose 2 true and other \n",
    "        test_pos += 1\n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        #print(len(temp_t),math.ceil(0.5*len(temp_t)))\n",
    "        t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t[test_pos] = t_for_test\n",
    "        test_f[test_pos] = f_for_test\n",
    "        \n",
    "        #other for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        #print(len(t_for_train ))\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test/test_amount\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoreMatrix(RS):\n",
    "    #取出test的資料\n",
    "    testRS = np.zeros((test_amount,yt_test_amount)) #shape 150*18\n",
    "    target = np.zeros((test_amount,yt_test_amount)) #(150,18)\n",
    "    #test_t 是true的\n",
    "    #test_f 是false的\n",
    "\n",
    "    for z in range(test_amount):\n",
    "        user_id = test_idx[z]\n",
    "        #positive target YouTuber list\n",
    "        youtube_t = test_t[z] \n",
    "        #not target YouTuber list\n",
    "        youtube_f = test_f[z]\n",
    "\n",
    "        #前兩個放target的RS\n",
    "        for i in range(len(youtube_t)):\n",
    "            testRS[z][i] = RS[z][youtube_t[i]]\n",
    "            target[z][i] = user_following[user_id][youtube_t[i]]\n",
    "        for i in range(len(youtube_f)):\n",
    "            testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "            target[z][i+len(youtube_t)] = user_following[user_id][youtube_f[i]]\n",
    "    sumtarget = 0\n",
    "    for i in range(len(target)):\n",
    "        #print(np.sum(target[i]))\n",
    "        sumtarget += np.sum(target[i])\n",
    "    print('num of positive data in testing:',sumtarget)\n",
    "    print('total testing data:',test_amount*yt_test_amount)\n",
    "    return target, testRS,sumtarget\n",
    "\n",
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList\n",
    "\n",
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTOP1(target,testRS,sumtarget):\n",
    "    print('top1')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_0 = topN(list(testRS[i]),1) #取一個\n",
    "        print('top',top0)\n",
    "        print('testRS[i]',testRS[i])\n",
    "        #print(top_0)\n",
    "        if top_0[0] < int(np.sum(target[i])):\n",
    "            correct += 1\n",
    "    top1_prec = correct/len(testRS)\n",
    "    top1_recall = correct/(sumtarget)\n",
    "    print('prec ',top1_prec,'recall ',top1_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top1_prec,top1_recall))\n",
    "    return top1_prec,top1_recall,F1_score(top1_prec,top1_recall)\n",
    "def getTOP3(target,testRS,sumtarget):\n",
    "    print('top3')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_3 = topN(list(testRS[i]),3) #取一個\n",
    "        \n",
    "        #print(top_3)\n",
    "        for j in range(len(top_3)):\n",
    "            if top_3[j] < int(np.sum(target[i])):\n",
    "                correct += 1\n",
    "    top3_prec = correct/(len(testRS)*3)\n",
    "    top3_recall = correct/(sumtarget)\n",
    "    print('prec ',top3_prec,'recall ',top3_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top3_prec,top3_recall))\n",
    "    return top3_prec,top3_recall,F1_score(top3_prec,top3_recall)\n",
    "def getTOP5(target,testRS,sumtarget):\n",
    "    print('top5')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_5 = topN(list(testRS[i]),5) #取一個\n",
    "        print('top',top5)\n",
    "        print('testRS[i]',testRS[i])\n",
    "        #print(top_5)\n",
    "        for j in range(len(top_5)):\n",
    "            if top_5[j] < int(np.sum(target[i])):\n",
    "                correct += 1\n",
    "    top5_prec = correct/(len(testRS)*5)\n",
    "    top5_recall = correct/(sumtarget)\n",
    "    print('prec ',top5_prec,'recall ',top5_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top5_prec,top5_recall))\n",
    "    return top5_prec,top5_recall,F1_score(top5_prec,top5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NDCG\n",
    "https://daiwk.github.io/posts/nlp-ndcg.html\n",
    "\"\"\"\n",
    "# pre_list\n",
    "\"\"\"\n",
    "test_amount = 150\n",
    "yt_test_amount = 18\n",
    "\"\"\"\n",
    "def NDCG(target,testRS): #target是真正的喜好\n",
    "    all_sort = []\n",
    "    num_ndcg = 10\n",
    "    pre_matrix = np.zeros(shape=(test_amount,yt_test_amount)) #(150,18)\n",
    "    for i in range(test_amount): #user amount = 150\n",
    "        top_n = topN(list(testRS[i]),num_ndcg) #取10個\n",
    "        #print(top_n)\n",
    "        all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "        #print('all_sort',topN(list(testRS[i]),len(testRS[i])))\n",
    "        for j in range(len(top_n)):\n",
    "            pre_matrix[i][top_n[j]] = 1\n",
    "\n",
    "    #Ideal DCG，理想状况下的DCG。也就是说，相关性完全由高到低排序时算出的DCG：\n",
    "    def IDCG(ideal_list): #ideal_list example = [1,1,1,1,1,0,0,....]\n",
    "        idcg=0\n",
    "        print('ideal',ideal_list)\n",
    "        for i in range(len(ideal_list)):\n",
    "            #print((2**true_list[i]-1),math.log2(i+2))\n",
    "            idcg+= (2**ideal_list[i]-1)/ math.log2(i+2)\n",
    "        #print('idcg',idcg)\n",
    "        return idcg\n",
    "    def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "        dcg=0\n",
    "        print('prec',prec_list)\n",
    "        for i in range(len(prec_list)):\n",
    "            dcg+= (2**prec_list[i]-1)/ math.log2(i+2)\n",
    "        #print('dcg',dcg)\n",
    "        return dcg\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    for m in range(test_amount): # the number of testing users\n",
    "        idcg = IDCG(target[m][:num_ndcg])\n",
    "        pre_list = []\n",
    "        least_pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "            pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        for s in all_sort[m][num_ndcg:]:\n",
    "            #print(s)\n",
    "            #print(target[m][s])\n",
    "            least_pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "        #print(ndcg)\n",
    "        total_ndcg += ndcg\n",
    "    avg_ndcg = total_ndcg/test_amount\n",
    "    print('NDCG:',avg_ndcg)\n",
    "    return pre_matrix,avg_ndcg\n",
    "\n",
    "# MAP\n",
    "\"\"\"\n",
    ">>> y_true = np.array([0, 0, 1, 1])\n",
    ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    ">>> average_precision_score(y_true, y_scores)\n",
    "\"\"\"\n",
    "def MAP(target,testRS):\n",
    "    print('target:',target)\n",
    "    print('predict:',testRS)\n",
    "    total_prec = 0\n",
    "    for u in range(test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec+=average_precision_score(y_true, y_scores)\n",
    "    Map_value = total_prec/test_amount\n",
    "    print('MAP',Map_value)\n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(U, Y, A, E,Au, Ay, Aa, Av):\n",
    "\n",
    "    test_amount = 150\n",
    "    yt_test_amount = 18\n",
    "    result=np.zeros((test_amount,88))\n",
    "    RS=np.zeros((test_amount,88))\n",
    "    #test_idx --> Test 的 index\n",
    "    sum_alpha = 0\n",
    "    for s in range(test_amount):\n",
    "        #print(s,test_idx[s])\n",
    "\n",
    "        yes=[]\n",
    "        #sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "        sample = train_t[test_idx[s]]\n",
    "        #sample=result_yes_id[now]\n",
    "        alpha=np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            alpha_a = np.dot(Au[test_idx[s]][sample[a]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[test_idx[s]][sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]][sample[a]],\n",
    "                    np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]][sample[a]],np.dot(E,np.expand_dims(all_3374[sample[a]],0).T))\n",
    "            \"\"\"\n",
    "            relu part ...\n",
    "            \"\"\"\n",
    "            alpha[a]=np.sum((relu(alpha_a)))\n",
    "            \"\"\"\n",
    "            tanh part ...\n",
    "            \"\"\"\n",
    "            #alpha[a]=np.sum((np.tanh(alpha_a)))*r\n",
    "            \n",
    "            \n",
    "        mul=np.zeros((1,64))\n",
    "        #print('alpha--------',alpha)\n",
    "        #print('add alpha------------',np.add(alpha,0.000000001))\n",
    "        added_alpha = np.add(alpha,0.0000000001)\n",
    "        norm_alpha = added_alpha/np.sum(added_alpha)\n",
    "        #print('alpha-----------',alpha)\n",
    "        #print('norm alpha--------------',norm_alpha)\n",
    "        sum_alpha += np.sum(alpha)\n",
    "        for i in range(len(sample)):\n",
    "            mul+=norm_alpha[i]*A[sample[i]] #attention alpha*Ai part\n",
    "        new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "        \n",
    "        \n",
    "        for k in range(88):\n",
    "            result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T)\n",
    "            \n",
    "    print('sum_alpha',sum_alpha)\n",
    "    return RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Embedding\n",
    "import os \n",
    "#從grid_search_weight中找尋不同的file \n",
    "path = '../Data/grid_search_weight/acf/'\n",
    "all_files = os.listdir(path)\n",
    "new_files = []\n",
    "for file in all_files:\n",
    "    if 'Edims200.npz' in file:\n",
    "        new_files.append(file)\n",
    "all_files = new_files\n",
    "all_files = ['ACF_NRImg0_Edims200.npz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_files = list(set([file.split('_Img')[0] for file in all_files]))\n",
    "print(par_files)\n",
    "for par_file in par_files:\n",
    "    NDCG_List = []\n",
    "    MAP_List = []\n",
    "    DIM_List = []\n",
    "    TOP1_Prec = []\n",
    "    TOP3_Prec = []\n",
    "    TOP5_Prec = []\n",
    "    TOP1_F1 = []\n",
    "    TOP3_F1 = []\n",
    "    TOP5_F1 = []\n",
    "    TOP1_Recall = []\n",
    "    TOP3_Recall = []\n",
    "    TOP5_Recall = []\n",
    "    \n",
    "    csv_path = '../Data/grid_search_weight/acf/'\n",
    "    for file in all_files:\n",
    "        if par_file in file:\n",
    "            print(file)\n",
    "            par_data = np.load(path+file)\n",
    "            U = par_data['U']\n",
    "            Y = par_data['Y']\n",
    "            A = par_data['A']\n",
    "            E = par_data['E']\n",
    "            #W1 = par_data['W1']\n",
    "            Wu = par_data['Wu']\n",
    "            Wy = par_data['Wy']\n",
    "            Wa = par_data['Wa']\n",
    "            Wv = par_data['Wv']\n",
    "            RS = testing(U, Y, A, E,Wu, Wy, Wa, Wv)\n",
    "            target,testRS,sumtarget = getScoreMatrix(RS)\n",
    "            prec_1,recall_1,f1_1 = getTOP1(target,testRS,sumtarget)\n",
    "            prec_3,recall_3,f1_3 = getTOP3(target,testRS,sumtarget)\n",
    "            prec_5,recall_5,f1_5 = getTOP5(target,testRS,sumtarget)\n",
    "            pre_matrix,avg_ndcg = NDCG(target,testRS)\n",
    "            Map_value = MAP(target,testRS)\n",
    "            NDCG_List.append(avg_ndcg)\n",
    "            MAP_List.append(Map_value)\n",
    "            DIM_List.append(file.split('_Edims')[0])\n",
    "            TOP1_Prec.append(prec_1)\n",
    "            TOP1_Recall.append(recall_1)\n",
    "            TOP1_F1.append(f1_1)\n",
    "            TOP3_Prec.append(prec_3)\n",
    "            TOP3_Recall.append(recall_3)\n",
    "            TOP3_F1.append(f1_3)\n",
    "            TOP5_Prec.append(prec_5)\n",
    "            TOP5_Recall.append(recall_5)\n",
    "            TOP5_F1.append(f1_5)\n",
    "            print('--------------------------------------------------------------------------------------------')\n",
    "    #print(NDCG_List)\n",
    "    #print(DIM_List)\n",
    "    result_dict = {'Dims':DIM_List,'NDCG':NDCG_List,'MAP':MAP_List,\n",
    "                   'TOP1 Precision':TOP1_Prec,'TOP1 Recall':TOP1_Recall,'TOP1 F1':TOP1_F1,\n",
    "                  'TOP3 Precision':TOP3_Prec,'TOP3 Recall':TOP3_Recall,'TOP3 F1':TOP3_F1,\n",
    "                  'TOP5 Precision':TOP5_Prec,'TOP5 Recall':TOP5_Recall,'TOP5 F1':TOP5_F1}\n",
    "    df = pd.DataFrame(result_dict)\n",
    "    df.to_csv(csv_path+par_file+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
