{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import average_precision_score\n",
    "import pandas as pd\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#從grid_search_weight中找尋不同的file \n",
    "path = 'D:/ChilliHsu/Data/grid_search_weight/Our/'\n",
    "all_files = os.listdir(path)\n",
    "files = [file for file in all_files if 'ALL' in file and '.npz' in file]\n",
    "files\n",
    "files = ['ALL_2_0.npz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_sum(pars):\n",
    "    all_squ_sum = []\n",
    "    for par in pars:\n",
    "        all_squ_sum.append(np.sum(np.multiply(par,par)))\n",
    "    return all_squ_sum \n",
    "def shape_sum(pars):\n",
    "    all_shape = []\n",
    "    for par in pars:\n",
    "        all_shape.append(par.shape)\n",
    "    return all_shape\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_following = np.load('../Data/npy/user_following_1489.npy')\n",
    "all_3374 = np.load('../Data/npy/all_2939D_img0.5.npy')\n",
    "user_category = np.load('../Data/npy/user_category_1489.npy')\n",
    "YouTuber_category = np.load('../Data/npy/YouTuber_category_0.7.npy')\n",
    "active_users = np.load('../Data/npy/active_userID_1489.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_idx(num):\n",
    "    \n",
    "    is_used = []\n",
    "    for i in range(num+1):\n",
    "        random.seed(5)\n",
    "        all_test_idx = [i for i in range(len(user_following))]\n",
    "        left_idx = [i for i in all_test_idx if i not in is_used]\n",
    "        test_idx = sorted(random.sample(left_idx,150))\n",
    "        is_used = is_used+test_idx\n",
    "    return test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n",
      "user cateogory norm [[0.         1.         0.         ... 0.05714286 0.         0.        ]\n",
      " [0.24390244 0.         0.02439024 ... 0.         0.         0.09756098]\n",
      " [0.04210526 0.04210526 0.05263158 ... 0.02105263 0.         0.02105263]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01408451 0.01408451 0.04225352 ... 0.02816901 0.         0.02816901]\n",
      " [0.03703704 0.22222222 0.14814815 ... 0.         0.         0.        ]]\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "[3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "5.466666666666667\n"
     ]
    }
   ],
   "source": [
    "user_category_norm = np.zeros(user_category.shape)\n",
    "for i in range(len(user_category)):\n",
    "    user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "print('user_category after normalized by max...')\n",
    "print('user_category_norm shape ',user_category_norm.shape)\n",
    "print('user cateogory norm',user_category_norm)\n",
    "following_true = [0]*len(user_following)\n",
    "for i in range(len(user_following)):\n",
    "    each_user = []\n",
    "    for j in range(len(user_following[i])):\n",
    "        if user_following[i][j] == 1:\n",
    "            each_user.append(j)\n",
    "    following_true[i] = each_user\n",
    "#print(following_true)\n",
    "#最少跟最多的following \n",
    "minlen = 10000\n",
    "maxlen = 0\n",
    "num_of_follower = []\n",
    "for i in range(len(following_true)):\n",
    "    if len(following_true[i]) < minlen:\n",
    "        minlen = len(following_true[i])\n",
    "    if len(following_true[i]) > maxlen:\n",
    "        maxlen = len(following_true[i])\n",
    "    num_of_follower.append(len(following_true[i]))\n",
    "print('Min number of followings ',minlen)\n",
    "print('Max number of followings ',maxlen)\n",
    "test_amount = 150\n",
    "yt_test_amount = 18\n",
    "user_idx = [i for i in range(len(user_following))]\n",
    "#user_idx = user_idx_over10\n",
    "#test_idx is the number of user for testing\n",
    "random.seed(5)\n",
    "#test_idx = sorted(random.sample(user_idx,test_amount))\n",
    "test_idx = get_test_idx(0)\n",
    "\n",
    "\n",
    "\n",
    "print(test_idx)\n",
    "# Training  and Testing --New\n",
    "train_t = [0]*(len(user_following))\n",
    "train_f = [0]*(len(user_following))\n",
    "# Testing \n",
    "test_t = [0]*test_amount\n",
    "test_f = [0]*test_amount\n",
    "test_pos = -1\n",
    "\n",
    "for i in range(len(user_following)):\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train\n",
    "        \n",
    "    else: #if in test id, choose 2 true and other \n",
    "        test_pos += 1\n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        #print(len(temp_t),math.ceil(0.5*len(temp_t)))\n",
    "        t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t[test_pos] = t_for_test\n",
    "        test_f[test_pos] = f_for_test\n",
    "        \n",
    "        #other for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        #print(len(t_for_train ))\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test/test_amount\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "index = 0 \n",
    "for train_idx in range(len(train_t)):\n",
    "    if train_idx in test_idx:\n",
    "        #print(len(train_t[train_idx])+len(train_f[train_idx])+len(test_t[index])+len(test_f[index]))\n",
    "        #print(train_f[train_idx])\n",
    "        #print(test_f[index])\n",
    "        all_idx = train_t[train_idx]+train_f[train_idx]+test_t[index]+test_f[index]\n",
    "        print(len(sorted(list(set(all_idx)))))\n",
    "        index+=1\n",
    "    else:\n",
    "        all_idx = train_t[train_idx]+train_f[train_idx]\n",
    "        print(len(sorted(list(set(all_idx)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoreMatrix(RS):\n",
    "    #取出test的資料\n",
    "    print('RS shape',RS.shape)\n",
    "    testRS = np.zeros((test_amount,yt_test_amount)) #shape 150*18\n",
    "    target = np.zeros((test_amount,yt_test_amount))\n",
    "    #test_t 是true的\n",
    "    #test_f 是false的\n",
    "\n",
    "    for z in range(test_amount):\n",
    "        user_id = test_idx[z]\n",
    "        #positive target YouTuber list\n",
    "        youtube_t = test_t[z] \n",
    "        #not target YouTuber list\n",
    "        youtube_f = test_f[z]\n",
    "\n",
    "        #前兩個放target的RS\n",
    "        for i in range(len(youtube_t)):\n",
    "            testRS[z][i] = RS[z][youtube_t[i]]\n",
    "            target[z][i] = user_following[user_id][youtube_t[i]]\n",
    "        for i in range(len(youtube_f)):\n",
    "            testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "            target[z][i+len(youtube_t)] = user_following[user_id][youtube_f[i]]\n",
    "    sumtarget = 0\n",
    "    for i in range(len(target)):\n",
    "        #print(np.sum(target[i]))\n",
    "        sumtarget += np.sum(target[i])\n",
    "    print('num of positive data in testing:',sumtarget)\n",
    "    print('total testing data:',test_amount*yt_test_amount)\n",
    "    return target, testRS,sumtarget\n",
    "\n",
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList\n",
    "\n",
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTOP1(target,testRS,sumtarget):\n",
    "    print('top1')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_0 = topN(list(testRS[i]),1) #取一個\n",
    "        \n",
    "        #print(top_0)\n",
    "        if top_0[0] < int(np.sum(target[i])):\n",
    "            correct += 1\n",
    "    top1_prec = correct/len(testRS)\n",
    "    top1_recall = correct/(sumtarget)\n",
    "    print('prec ',top1_prec,'recall ',top1_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top1_prec,top1_recall))\n",
    "    return top1_prec,top1_recall,F1_score(top1_prec,top1_recall)\n",
    "def getTOP3(target,testRS,sumtarget):\n",
    "    print('top3')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_3 = topN(list(testRS[i]),3) #取一個\n",
    "        \n",
    "        #print(top_3)\n",
    "        for j in range(len(top_3)):\n",
    "            if top_3[j] < int(np.sum(target[i])):\n",
    "                correct += 1\n",
    "    top3_prec = correct/(len(testRS)*3)\n",
    "    top3_recall = correct/(sumtarget)\n",
    "    print('prec ',top3_prec,'recall ',top3_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top3_prec,top3_recall))\n",
    "    return top3_prec,top3_recall,F1_score(top3_prec,top3_recall)\n",
    "def getTOP5(target,testRS,sumtarget):\n",
    "    print('top5')\n",
    "    correct = 0\n",
    "    for i in range(len(testRS)):\n",
    "        top_5 = topN(list(testRS[i]),5) #取一個\n",
    "       \n",
    "        #print(top_5)\n",
    "        for j in range(len(top_5)):\n",
    "            if top_5[j] < int(np.sum(target[i])):\n",
    "                correct += 1\n",
    "    top5_prec = correct/(len(testRS)*5)\n",
    "    top5_recall = correct/(sumtarget)\n",
    "    print('prec ',top5_prec,'recall ',top5_recall)\n",
    "    #f1 score\n",
    "    print('F1_score:',F1_score(top5_prec,top5_recall))\n",
    "    return top5_prec,top5_recall,F1_score(top5_prec,top5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NDCG\n",
    "https://daiwk.github.io/posts/nlp-ndcg.html\n",
    "\"\"\"\n",
    "# pre_list\n",
    "\"\"\"\n",
    "test_amount = 150\n",
    "yt_test_amount = 18\n",
    "\"\"\"\n",
    "def NDCG(target,testRS): #target是真正的喜好\n",
    "    all_sort = []\n",
    "    num_ndcg = 10\n",
    "    pre_matrix = np.zeros(shape=(test_amount,yt_test_amount)) #(150,18)\n",
    "    for i in range(test_amount): #user amount = 150\n",
    "        top_n = topN(list(testRS[i]),num_ndcg) #取10個\n",
    "        #print(top_n)\n",
    "        all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "        #print('all_sort',topN(list(testRS[i]),len(testRS[i])))\n",
    "        for j in range(len(top_n)):\n",
    "            pre_matrix[i][top_n[j]] = 1\n",
    "\n",
    "    #Ideal DCG，理想状况下的DCG。也就是说，相关性完全由高到低排序时算出的DCG：\n",
    "    def IDCG(ideal_list): #ideal_list example = [1,1,1,1,1,0,0,....]\n",
    "        idcg=0\n",
    "        #print('ideal',ideal_list)\n",
    "        for i in range(len(ideal_list)):\n",
    "            #print((2**true_list[i]-1),math.log2(i+2))\n",
    "            idcg+= (2**ideal_list[i]-1)/ math.log2(i+2)\n",
    "        #print('idcg',idcg)\n",
    "        return idcg\n",
    "    def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "        dcg=0\n",
    "        #print('prec',prec_list)\n",
    "        for i in range(len(prec_list)):\n",
    "            dcg+= (2**prec_list[i]-1)/ math.log2(i+2)\n",
    "        #print('dcg',dcg)\n",
    "        return dcg\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    for m in range(test_amount): # the number of testing users\n",
    "        idcg = IDCG(target[m][:num_ndcg])\n",
    "        pre_list = []\n",
    "        least_pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "            pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        for s in all_sort[m][num_ndcg:]:\n",
    "            #print(s)\n",
    "            #print(target[m][s])\n",
    "            least_pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "        #print(ndcg)\n",
    "        total_ndcg += ndcg\n",
    "    avg_ndcg = total_ndcg/test_amount\n",
    "    print('NDCG:',avg_ndcg)\n",
    "    return pre_matrix,avg_ndcg\n",
    "\n",
    "# MAP\n",
    "\"\"\"\n",
    ">>> y_true = np.array([0, 0, 1, 1])\n",
    ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    ">>> average_precision_score(y_true, y_scores)\n",
    "\"\"\"\n",
    "def MAP(target,testRS):\n",
    "    print('target:',target)\n",
    "    print('testRS:',testRS)\n",
    "    total_prec = 0\n",
    "    for u in range(test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec+=average_precision_score(y_true, y_scores)\n",
    "    Map_value = total_prec/test_amount\n",
    "    print('MAP',Map_value)\n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(U, Y, A, E,Au, Ay, Aa, Av,B):\n",
    "    \n",
    "    test_amount = 150\n",
    "    yt_test_amount = 18\n",
    "    result=np.zeros((test_amount,88))\n",
    "    RS=np.zeros((test_amount,88))\n",
    "    #test_idx --> Test 的 index\n",
    "    sum_alpha = 0\n",
    "    for s in range(test_amount):\n",
    "        #print(s,test_idx[s])\n",
    "\n",
    "        yes=[]\n",
    "        #sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "        sample = [i for i in range(88)]\n",
    "        #sample = train_t[test_idx[s]]\n",
    "        \n",
    "        #sample=result_yes_id[now]\n",
    "        alpha=np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "            alpha_a = np.dot(Au[test_idx[s]][sample[a]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[test_idx[s]][sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]][sample[a]],\n",
    "                    np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]][sample[a]],np.dot(E,np.expand_dims(all_3374[sample[a]],0).T))\n",
    "            \"\"\"\n",
    "            relu part ...\n",
    "            \"\"\"\n",
    "            alpha[a]=np.sum((relu(alpha_a)))*r\n",
    "            \"\"\"\n",
    "            tanh part ...\n",
    "            \"\"\"\n",
    "            #alpha[a]=np.sum((np.tanh(alpha_a)))*r\n",
    "            \n",
    "            \n",
    "        mul=np.zeros((1,64))\n",
    "        #print('alpha--------',alpha)\n",
    "        #print('add alpha------------',np.add(alpha,0.000000001))\n",
    "        added_alpha = np.add(np.maximum(alpha, 0),0.0000000001)\n",
    "        norm_alpha = added_alpha/np.sum(added_alpha)\n",
    "        \n",
    "        #print(norm_alpha)\n",
    "        #print('alpha-----------',alpha)\n",
    "        #print('norm alpha--------------',norm_alpha)\n",
    "        sum_alpha += np.sum(alpha)\n",
    "        for i in range(len(sample)):\n",
    "            mul+=norm_alpha[i]*A[sample[i]] #attention alpha*Ai part\n",
    "        new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "        \n",
    "        \n",
    "        for k in range(88):\n",
    "            result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]],np.dot(E, all_3374[k].T))\n",
    "            \n",
    "    print('sum_alpha',sum_alpha)\n",
    "    return RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALL5.npz']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#With Embedding\n",
    "import os \n",
    "#從grid_search_weight中找尋不同的file \n",
    "path = '../Data/grid_search_weight/Our/'\n",
    "all_files = os.listdir(path)\n",
    "new_all_files = []\n",
    "for file in all_files:\n",
    "    if 'ALL' in file:\n",
    "        new_all_files.append(file)\n",
    "all_files = new_all_files\n",
    "print(all_files)\n",
    "all_files = ['Img0.5_ALL_E150.npz','Img0.5_ALL_E160.npz','Img0.5_ALL_E170.npz']\"\"\"\n",
    "files = ['ALL5.npz']\n",
    "all_files = files\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/grid_search_weight/Our/'\n",
    "#all_files = ['Img0.5_ALL_E150.npz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALL']\n",
      "ALL5.npz\n",
      "sum_alpha 2.9489840346822778\n",
      "RS shape (150, 88)\n",
      "num of positive data in testing: 820.0\n",
      "total testing data: 2700\n",
      "top1\n",
      "prec  0.8533333333333334 recall  0.15609756097560976\n",
      "F1_score: 0.2639175257731959\n",
      "top3\n",
      "prec  0.7533333333333333 recall  0.41341463414634144\n",
      "F1_score: 0.5338582677165353\n",
      "top5\n",
      "prec  0.6853333333333333 recall  0.6268292682926829\n",
      "F1_score: 0.6547770700636942\n",
      "NDCG: 0.8652579468799466\n",
      "target: [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "testRS: [[  3.02030123  -1.01979985   0.95287444 ...   1.17643283 -11.95212281\n",
      "   -9.90817714]\n",
      " [  2.47776733  -1.25722218   2.8429347  ...  -0.09373315  -2.23774073\n",
      "   -3.72977524]\n",
      " [ -6.25081016  -0.42638052  -0.70702485 ...  -9.8286897   -6.79765448\n",
      "   -9.87507106]\n",
      " ...\n",
      " [ -3.42275241  -2.11855332  -4.43099991 ...  -3.94354252 -12.62768986\n",
      "   -9.15580671]\n",
      " [ -0.04858852  -2.52945011  -0.15355501 ...  -4.8963626   -9.80703672\n",
      "   -9.5876712 ]\n",
      " [ -0.94412115  -0.60076609  -0.29968517 ...  -7.33913957  -7.84908457\n",
      "   -1.30394827]]\n",
      "MAP 0.7942395430467396\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#par_files = list(set([file.split('_E')[0] for file in all_files]))\n",
    "par_files = ['ALL']\n",
    "print(par_files)\n",
    "for par_file in par_files:\n",
    "    NDCG_List = []\n",
    "    MAP_List = []\n",
    "    DIM_List = []\n",
    "    TOP1_Prec = []\n",
    "    TOP3_Prec = []\n",
    "    TOP5_Prec = []\n",
    "    TOP1_F1 = []\n",
    "    TOP3_F1 = []\n",
    "    TOP5_F1 = []\n",
    "    TOP1_Recall = []\n",
    "    TOP3_Recall = []\n",
    "    TOP5_Recall = []\n",
    "    \n",
    "    csv_path = '../Data/grid_search_weight/Our/'\n",
    "    for file in all_files:\n",
    "        if par_file in file:\n",
    "            print(file)\n",
    "            par_data = np.load(path+file)\n",
    "            U = par_data['U']\n",
    "            Y = par_data['Y']\n",
    "            A = par_data['A']\n",
    "            E = par_data['E']\n",
    "            #W1 = par_data['W1']\n",
    "            Wu = par_data['Wu']\n",
    "            Wy = par_data['Wy']\n",
    "            Wa = par_data['Wa']\n",
    "            Wv = par_data['Wv']\n",
    "            B = par_data['B']\n",
    "            \"\"\"print(U.shape)\n",
    "            print(Y.shape)\n",
    "            print(A.shape)\n",
    "            print(E.shape)\n",
    "            print(B.shape)\"\"\"\n",
    "            RS = testing(U, Y, A, E,Wu, Wy, Wa, Wv,B)\n",
    "            target,testRS,sumtarget = getScoreMatrix(RS)\n",
    "            prec_1,recall_1,f1_1 = getTOP1(target,testRS,sumtarget)\n",
    "            prec_3,recall_3,f1_3 = getTOP3(target,testRS,sumtarget)\n",
    "            prec_5,recall_5,f1_5 = getTOP5(target,testRS,sumtarget)\n",
    "            pre_matrix,avg_ndcg = NDCG(target,testRS)\n",
    "            Map_value = MAP(target,testRS)\n",
    "            NDCG_List.append(avg_ndcg)\n",
    "            MAP_List.append(Map_value)\n",
    "            DIM_List.append(file)\n",
    "            TOP1_Prec.append(prec_1)\n",
    "            TOP1_Recall.append(recall_1)\n",
    "            TOP1_F1.append(f1_1)\n",
    "            TOP3_Prec.append(prec_3)\n",
    "            TOP3_Recall.append(recall_3)\n",
    "            TOP3_F1.append(f1_3)\n",
    "            TOP5_Prec.append(prec_5)\n",
    "            TOP5_Recall.append(recall_5)\n",
    "            TOP5_F1.append(f1_5)\n",
    "            print('--------------------------------------------------------------------------------------------')\n",
    "    #print(NDCG_List)\n",
    "    #print(DIM_List)\n",
    "    result_dict = {'Dims':DIM_List,'NDCG':NDCG_List,'MAP':MAP_List,\n",
    "                   'TOP1 Precision':TOP1_Prec,'TOP1 Recall':TOP1_Recall,'TOP1 F1':TOP1_F1,\n",
    "                  'TOP3 Precision':TOP3_Prec,'TOP3 Recall':TOP3_Recall,'TOP3 F1':TOP3_F1,\n",
    "                  'TOP5 Precision':TOP5_Prec,'TOP5 Recall':TOP5_Recall,'TOP5 F1':TOP5_F1}\n",
    "    df = pd.DataFrame(result_dict)\n",
    "    df.to_csv(csv_path+par_file+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Feature Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Img0.5_no_300following_E200.npz', 'Img0.5_no_300image_E200.npz', 'Img0.5_no_300social_E200.npz', 'Img0.5_no_300text_E200.npz', 'Img0.5_no_300video_E200.npz', 'Img0.5_only_Image_E200.npz', 'Img0.5_only_text_E200.npz']\n"
     ]
    }
   ],
   "source": [
    "#With Embedding\n",
    "import os \n",
    "#從grid_search_weight中找尋不同的file \n",
    "path = '../Data/npy/mask_feature/result300/'\n",
    "all_files = os.listdir(path)\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_files = list(set([file.split('_')[1]+'_'+file.split('_')[2] for file in all_files]))\n",
    "print(par_files)\n",
    "for par_file in par_files:\n",
    "    NDCG_List = []\n",
    "    MAP_List = []\n",
    "    DIM_List = []\n",
    "    TOP1_Prec = []\n",
    "    TOP3_Prec = []\n",
    "    TOP5_Prec = []\n",
    "    TOP1_F1 = []\n",
    "    TOP3_F1 = []\n",
    "    TOP5_F1 = []\n",
    "    TOP1_Recall = []\n",
    "    TOP3_Recall = []\n",
    "    TOP5_Recall = []\n",
    "    \n",
    "    #csv_path = '../Data/grid_search_weight/CSV_Tan/'\n",
    "    csv_path = '../Data/npy/mask_feature/csv/'\n",
    "    for file in all_files:\n",
    "        if par_file in file:\n",
    "            print(file)\n",
    "            par_data = np.load(path+file)\n",
    "            U = par_data['U']\n",
    "            Y = par_data['Y']\n",
    "            A = par_data['A']\n",
    "            E = par_data['E']\n",
    "            #W1 = par_data['W1']\n",
    "            Wu = par_data['Wu']\n",
    "            Wy = par_data['Wy']\n",
    "            Wa = par_data['Wa']\n",
    "            Wv = par_data['Wv']\n",
    "            B = par_data['B']\n",
    "            RS = testing(U, Y, A, E,Wu, Wy, Wa, Wv,B)\n",
    "            target,testRS,sumtarget = getScoreMatrix(RS)\n",
    "            print('target',target)\n",
    "            prec_1,recall_1,f1_1 = getTOP1(target,testRS,sumtarget)\n",
    "            prec_3,recall_3,f1_3 = getTOP3(target,testRS,sumtarget)\n",
    "            prec_5,recall_5,f1_5 = getTOP5(target,testRS,sumtarget)\n",
    "            pre_matrix,avg_ndcg = NDCG(target,testRS)\n",
    "            Map_value = MAP(target,pre_matrix)\n",
    "            NDCG_List.append(avg_ndcg)\n",
    "            MAP_List.append(Map_value)\n",
    "            DIM_List.append(file.split('_')[1])\n",
    "            TOP1_Prec.append(prec_1)\n",
    "            TOP1_Recall.append(recall_1)\n",
    "            TOP1_F1.append(f1_1)\n",
    "            TOP3_Prec.append(prec_3)\n",
    "            TOP3_Recall.append(recall_3)\n",
    "            TOP3_F1.append(f1_3)\n",
    "            TOP5_Prec.append(prec_5)\n",
    "            TOP5_Recall.append(recall_5)\n",
    "            TOP5_F1.append(f1_5)\n",
    "            print('--------------------------------------------------------------------------------------------')\n",
    "    #print(NDCG_List)\n",
    "    #print(DIM_List)\n",
    "    result_dict = {'Dims':DIM_List,'NDCG':NDCG_List,'MAP':MAP_List,\n",
    "                   'TOP1 Precision':TOP1_Prec,'TOP1 Recall':TOP1_Recall,'TOP1 F1':TOP1_F1,\n",
    "                  'TOP3 Precision':TOP3_Prec,'TOP3 Recall':TOP3_Recall,'TOP3 F1':TOP3_F1,\n",
    "                  'TOP5 Precision':TOP5_Prec,'TOP5 Recall':TOP5_Recall,'TOP5 F1':TOP5_F1}\n",
    "    df = pd.DataFrame(result_dict)\n",
    "    df.to_csv(csv_path+par_file+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['onlyImg.npz', 'ImgVideo.npz']\n",
      "vbpr_onlyImg.npz\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'A is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-abbbd07c1713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpar_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'U'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpar_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpar_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpar_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'E'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m#W1 = par_data['W1']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ntu\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    264\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not a file in the archive\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'A is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "#For VBPR\n",
    "par_files = list(set([file.split('_')[1] for file in all_files]))\n",
    "print(par_files)\n",
    "for par_file in par_files:\n",
    "    NDCG_List = []\n",
    "    MAP_List = []\n",
    "    DIM_List = []\n",
    "    TOP1_Prec = []\n",
    "    TOP3_Prec = []\n",
    "    TOP5_Prec = []\n",
    "    TOP1_F1 = []\n",
    "    TOP3_F1 = []\n",
    "    TOP5_F1 = []\n",
    "    TOP1_Recall = []\n",
    "    TOP3_Recall = []\n",
    "    TOP5_Recall = []\n",
    "    \n",
    "    csv_path = '../Data/grid_search_weight/CSV_Tan/'\n",
    "    #csv_path = '../Data/npy/mask_feature/csv/'\n",
    "    for file in all_files:\n",
    "        if par_file in file:\n",
    "            print(file)\n",
    "            par_data = np.load(path+file)\n",
    "            U = par_data['U']\n",
    "            Y = par_data['Y']\n",
    "            A = par_data['A']\n",
    "            E = par_data['E']\n",
    "            #W1 = par_data['W1']\n",
    "            Wu = parRS,sumtarget = getScoreMatrix(RS)\n",
    "            print('target',target)\n",
    "            prec_1,recall_1,f1_1 = getTOP1(target,testRS,sumtarget)\n",
    "            prec_3,recall_3,f1_3 = getTOP3(target,testRS,sumtarget)\n",
    "            prec_5,recall_5,f1_5 = getTOP5(target,testRS,sumtarget)\n",
    "            pre_matrix,avg_data['Wu']\n",
    "            Wy = par_data['Wy']\n",
    "            Wa = par_data['Wa']\n",
    "            Wv = par_data['Wv']\n",
    "            B = par_data['B']\n",
    "            RS = testing(U, Y, A, E,Wu, Wy, Wa, Wv,B)\n",
    "            target,test_ndcg = NDCG(target,testRS)\n",
    "            Map_value = MAP(target,pre_matrix)\n",
    "            NDCG_List.append(avg_ndcg)\n",
    "            MAP_List.append(Map_value)\n",
    "            DIM_List.append(file.split('_')[1])\n",
    "            TOP1_Prec.append(prec_1)\n",
    "            TOP1_Recall.append(recall_1)\n",
    "            TOP1_F1.append(f1_1)\n",
    "            TOP3_Prec.append(prec_3)\n",
    "            TOP3_Recall.append(recall_3)\n",
    "            TOP3_F1.append(f1_3)\n",
    "            TOP5_Prec.append(prec_5)\n",
    "            TOP5_Recall.append(recall_5)\n",
    "            TOP5_F1.append(f1_5)\n",
    "            print('--------------------------------------------------------------------------------------------')\n",
    "    #print(NDCG_List)\n",
    "    #print(DIM_List)\n",
    "    result_dict = {'Dims':DIM_List,'NDCG':NDCG_List,'MAP':MAP_List,\n",
    "                   'TOP1 Precision':TOP1_Prec,'TOP1 Recall':TOP1_Recall,'TOP1 F1':TOP1_F1,\n",
    "                  'TOP3 Precision':TOP3_Prec,'TOP3 Recall':TOP3_Recall,'TOP3 F1':TOP3_F1,\n",
    "                  'TOP5 Precision':TOP5_Prec,'TOP5 Recall':TOP5_Recall,'TOP5 F1':TOP5_F1}\n",
    "    df = pd.DataFrame(result_dict)\n",
    "    df.to_csv(csv_path+par_file+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_files = list(set([file.split('_Edims')[0] for file in all_files]))\n",
    "print(par_files)\n",
    "for par_file in par_files:\n",
    "    NDCG_List = []\n",
    "    MAP_List = []\n",
    "    DIM_List = []\n",
    "    TOP1_Prec = []\n",
    "    TOP3_Prec = []\n",
    "    TOP5_Prec = []\n",
    "    TOP1_F1 = []\n",
    "    TOP3_F1 = []\n",
    "    TOP5_F1 = []\n",
    "    TOP1_Recall = []\n",
    "    TOP3_Recall = []\n",
    "    TOP5_Recall = []\n",
    "    \n",
    "    csv_path = '../Data/grid_search_weight/CSV_Tan/'\n",
    "    for file in all_files:\n",
    "        if par_file in file:\n",
    "            print(file)\n",
    "            par_data = np.load(path+file)\n",
    "            U = par_data['U']\n",
    "            Y = par_data['Y']\n",
    "            A = par_data['A']\n",
    "            E = par_data['E']\n",
    "            #W1 = par_data['W1']\n",
    "            Wu = par_data['Wu']\n",
    "            Wy = par_data['Wy']\n",
    "            Wa = par_data['Wa']\n",
    "            Wv = par_data['Wv']\n",
    "            B = par_data['B']\n",
    "            RS = testing(U, Y, A, E,Wu, Wy, Wa, Wv,B)\n",
    "            target,testRS,sumtarget = getScoreMatrix(RS)\n",
    "            prec_1,recall_1,f1_1 = getTOP1(target,testRS,sumtarget)\n",
    "            prec_3,recall_3,f1_3 = getTOP3(target,testRS,sumtarget)\n",
    "            prec_5,recall_5,f1_5 = getTOP5(target,testRS,sumtarget)\n",
    "            pre_matrix,avg_ndcg = NDCG(target,testRS)\n",
    "            Map_value = MAP(target,pre_matrix)\n",
    "            NDCG_List.append(avg_ndcg)\n",
    "            MAP_List.append(Map_value)\n",
    "            DIM_List.append(file.split('Edims')[1].split('.npz')[0])\n",
    "            TOP1_Prec.append(prec_1)\n",
    "            TOP1_Recall.append(recall_1)\n",
    "            TOP1_F1.append(f1_1)\n",
    "            TOP3_Prec.append(prec_3)\n",
    "            TOP3_Recall.append(recall_3)\n",
    "            TOP3_F1.append(f1_3)\n",
    "            TOP5_Prec.append(prec_5)\n",
    "            TOP5_Recall.append(recall_5)\n",
    "            TOP5_F1.append(f1_5)\n",
    "            print('--------------------------------------------------------------------------------------------')\n",
    "    #print(NDCG_List)\n",
    "    #print(DIM_List)\n",
    "    result_dict = {'Dims':DIM_List,'NDCG':NDCG_List,'MAP':MAP_List,\n",
    "                   'TOP1 Precision':TOP1_Prec,'TOP1 Recall':TOP1_Recall,'TOP1 F1':TOP1_F1,\n",
    "                  'TOP3 Precision':TOP3_Prec,'TOP3 Recall':TOP3_Recall,'TOP3 F1':TOP3_F1,\n",
    "                  'TOP5 Precision':TOP5_Prec,'TOP5 Recall':TOP5_Recall,'TOP5 F1':TOP5_F1}\n",
    "    df = pd.DataFrame(result_dict)\n",
    "    df.to_csv(csv_path+par_file+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG For Different Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## code here !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1]\n",
      "[3, 6, 2]\n",
      "[1, 2, 13, 4, 5, 3, 17]\n",
      "[1, 10, 8, 2, 5, 4]\n",
      "[4, 1, 12]\n",
      "[1, 15, 11]\n",
      "[2, 1, 10, 0]\n",
      "[3, 2, 5, 7, 4, 18]\n",
      "[3, 18, 8, 5, 13]\n",
      "[12, 16, 7, 0, 9]\n",
      "avg_accuarcy for count_0: 0.24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),int(np.sum(target[i])))\n",
    "    count_0_all.append(top_0)\n",
    "    print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = 0\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        #print(int(np.sum(target[i])))\n",
    "        total+=int(np.sum(target[i]))\n",
    "        if count_0_all[i][j] < int(np.sum(target[i])): #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "avg_acc = acc_0/100\n",
    "print('avg_accuarcy for count_0:',avg_acc)\n",
    "print(acc_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOP1,TOP3,TOP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14409628, 0.14409674, 0.14135303, 0.14211949, 0.14409625,\n",
       "       0.14211893, 0.14211929])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list = [0.02162337, 0.02162659, 0.00240219, 0.00780985, 0.02162318, 0.00780592, 0.00780849]\n",
    "softmax(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user = []\n",
    "for i in range(3):\n",
    "    temp_t = []\n",
    "    temp_t.append(1)\n",
    "    temp_t.append(2)\n",
    "    temp_t.append(3)\n",
    "    user.append(temp_t)\n",
    "np.array(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([-1, 1, 0,2])\n",
    "average_precision_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
