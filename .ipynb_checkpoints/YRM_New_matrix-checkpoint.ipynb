{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data positive feedback dynamic (20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_following = np.load('../Data/npy/user_following_1489.npy')[:100]\n",
    "all_3374 = np.load('../Data/npy/all_3374D.npy')\n",
    "user_category = np.load('../Data/npy/user_category_1489.npy')[:100]\n",
    "YouTuber_category = np.load('../Data/npy/YouTuber_category_0.7.npy')\n",
    "active_users = np.load('../Data/npy/active_userID_1489.npy')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of followers over 5: 42\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each YouTuber\n",
    "YouTuber_followers = np.sum(user_following, axis=0)\n",
    "#print(YouTuber_followers)\n",
    "over5 = 0\n",
    "for num in YouTuber_followers:\n",
    "    if num >= 5:\n",
    "        over5+=1\n",
    "print('The num of followers over 5:',over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_following shape  (100, 88)\n",
      "all_3374 shape  (88, 3374)\n",
      "user_category shape  (100, 17)\n",
      "YouTuber_category shape  (88, 17)\n"
     ]
    }
   ],
   "source": [
    "print('user_following shape ',user_following.shape)\n",
    "print('all_3374 shape ',all_3374.shape)\n",
    "print('user_category shape ',user_category.shape)\n",
    "print('YouTuber_category shape ',YouTuber_category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_category after normalized by max...\n",
      "user_category_norm shape  (100, 17)\n"
     ]
    }
   ],
   "source": [
    "user_category_norm = np.zeros(user_category.shape)\n",
    "for i in range(len(user_category)):\n",
    "    user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "print('user_category after normalized by max...')\n",
    "print('user_category_norm shape ',user_category_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "following_true = [0]*len(user_following)\n",
    "for i in range(len(user_following)):\n",
    "    each_user = []\n",
    "    for j in range(len(user_following[i])):\n",
    "        if user_following[i][j] == 1:\n",
    "            each_user.append(j)\n",
    "    following_true[i] = each_user\n",
    "#print(following_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings  5\n",
      "Max number of followings  21\n"
     ]
    }
   ],
   "source": [
    "#最少跟最多的following \n",
    "minlen = 10000\n",
    "maxlen = 0\n",
    "num_of_follower = []\n",
    "for i in range(len(following_true)):\n",
    "    if len(following_true[i]) < minlen:\n",
    "        minlen = len(following_true[i])\n",
    "    if len(following_true[i]) > maxlen:\n",
    "        maxlen = len(following_true[i])\n",
    "    num_of_follower.append(len(following_true[i]))\n",
    "print('Min number of followings ',minlen)\n",
    "print('Max number of followings ',maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 8: 68\n",
      "over 10: 56\n",
      "over 12: 28\n",
      "avg 10: 12.517857142857142\n"
     ]
    }
   ],
   "source": [
    "over_10 = 0\n",
    "over_8 = 0\n",
    "over_12 = 0\n",
    "user_idx_over10 = []\n",
    "avg_over10 = 0\n",
    "for i in range(len(num_of_follower)):\n",
    "    num = num_of_follower[i]\n",
    "    if num >= 10:\n",
    "        over_10 += 1\n",
    "        user_idx_over10.append(i)\n",
    "        avg_over10+=num\n",
    "    if num >= 8:\n",
    "        over_8 += 1\n",
    "    if num >= 12:\n",
    "        over_12 += 1\n",
    "print('over 8:',over_8)\n",
    "print('over 10:',over_10)\n",
    "print('over 12:',over_12)\n",
    "print('avg 10:',avg_over10/len(user_idx_over10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amount = 10\n",
    "yt_test_amount = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81, 14, 3, 94, 35, 31, 28, 17, 13, 86]\n"
     ]
    }
   ],
   "source": [
    "user_idx = [i for i in range(len(user_following))]\n",
    "#user_idx = user_idx_over10\n",
    "#test_idx is the number of user for testing\n",
    "random.seed(42)\n",
    "test_idx = random.sample(user_idx,test_amount)\n",
    "print(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training  and Testing --New\n",
    "train_t = [0]*(len(user_following))\n",
    "train_f = [0]*(len(user_following))\n",
    "# Testing \n",
    "test_t = [0]*test_amount\n",
    "test_f = [0]*test_amount\n",
    "test_pos = -1\n",
    "\n",
    "for i in range(len(user_following)):\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train\n",
    "        \n",
    "    else: #if in test id, choose 2 true and other \n",
    "        test_pos += 1\n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        #print(len(temp_t),int(len(temp_t)/2))\n",
    "        t_for_test = random.sample(temp_t,math.ceil(len(temp_t)/2))\n",
    "        #t_for_test = random.sample(temp_t,int(len(temp_t)/2))\n",
    "        f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t[test_pos] = t_for_test\n",
    "        test_f[test_pos] = f_for_test\n",
    "        \n",
    "        #other for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        #print(len(t_for_train ))\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test/test_amount\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 100\n",
      "The length of train_f: 100\n",
      "The length of test_t: 10\n",
      "The length of test_f: 10\n"
     ]
    }
   ],
   "source": [
    "# train_t[i] 代表的是user i positive feedback\n",
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_t))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation  Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Embedding for feature vector\n",
    "\"\"\"\n",
    "n: the number of users\n",
    "m: the number of YouTubers\n",
    "k: latent dims\n",
    "l: feature dims\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "n = len(user_following)\n",
    "m = 88  \n",
    "k = 100\n",
    "l = 3374\n",
    "embedding_dims = 150\n",
    "\n",
    "user = tf.placeholder(tf.int32,shape=(1,))\n",
    "i = tf.placeholder(tf.int32, shape=(1,))\n",
    "j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                      initializer=tf.random_normal_initializer(0,1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                      initializer=tf.random_normal_initializer(0,1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                      initializer=tf.random_normal_initializer(0,1,seed=3))\n",
    "    W1 = tf.get_variable(\"W1\", [n, k],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [n,k,k],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wy = tf.get_variable(\"Wy\", [m,k,k],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wa = tf.get_variable(\"Wa\", [n,k,k],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wv = tf.get_variable(\"Wv\", [n,k,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "    ########## Error part, how to get auxisize dynamically\n",
    "    ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "    \n",
    "with tf.variable_scope('feature_level'):\n",
    "    Embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                             # initializer=tf.contrib.layers.xavier_initializer())\n",
    "                                     initializer=tf.random_normal_initializer(0.00001,0.000001,seed=10))\n",
    "\n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "w1 = tf.nn.embedding_lookup(W1, user) #(1*k)\n",
    "#wu = Wu\n",
    "wu = tf.expand_dims(tf.squeeze(tf.nn.embedding_lookup(Wu, user)),0) #(k*k)\n",
    "wy = tf.expand_dims(tf.squeeze(tf.nn.embedding_lookup(Wy, i)),0) #(k*k)\n",
    "#wa = Wa\n",
    "wa = tf.expand_dims(tf.squeeze(tf.nn.embedding_lookup(Wa, user)),0) #(k*k)\n",
    "#wv = Wv\n",
    "wv = tf.expand_dims(tf.squeeze(tf.nn.embedding_lookup(Wv, user)),0) #(k,l)\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "embedding = Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-e1be71bd3678>:77: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "a_list=tf.Variable([])\n",
    "q = tf.constant(0)\n",
    "def att_cond(q,a_list):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "def att_body(q,a_list):\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "\n",
    "    a_list = tf.concat([a_list,(tf.matmul( w1, tf.nn.relu( tf.matmul(wu, u, transpose_b=True) +\n",
    "            tf.matmul(wy, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "            tf.matmul(wa, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "            tf.matmul(wv, xfi, transpose_b=True)))[0][0])*r[q]],0)\n",
    "    q += 1\n",
    "    return q,  a_list\n",
    "\n",
    "_, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "a_list_soft=tf.nn.softmax(a_list)\n",
    "\n",
    "\n",
    "aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "q = tf.constant(0)\n",
    "def sum_att_cond(q,aux_np):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def sum_att_body(q,aux_np):\n",
    "    #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "    aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "    q += 1\n",
    "    return q, aux_np\n",
    "\n",
    "_,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "\"\"\"\n",
    "for q in range(3): #取q個auxliary item\n",
    "    aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "\"\"\"\n",
    "\n",
    "aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "#tf.print('aux attention:',aux_np)\n",
    "aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "# ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "norm_par = [tf.reduce_sum(tf.multiply(u, u)),tf.reduce_sum(tf.multiply(vi, vi)),tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "           tf.reduce_sum(tf.multiply(w1, w1)),tf.reduce_sum(tf.multiply(wu, wu)),tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "           tf.reduce_sum(tf.multiply(wa, wa)),tf.reduce_sum(tf.multiply(wv,wv)),tf.reduce_sum(tf.multiply(beta,beta))]\n",
    "l2_norm = tf.add_n([\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.0001 * tf.reduce_sum(tf.multiply(w1, w1)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.01 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            \n",
    "          ])\n",
    "\n",
    "loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraction: 0\n",
      "0\n",
      "par square: [113.02298, 114.21673, 109.56071, 0.97611254, 0.058644943, 0.5756031, 0.05903706, 1.3886439e-06, 0.0029592686]\n",
      "before softmax: [0.03501258 0.09206246 0.02508074 0.12962599 0.06892628 0.02553602\n",
      " 0.04210936]\n",
      "after softmax: [0.13927102 0.14744742 0.13789465 0.1530914  0.1440752  0.13795742\n",
      " 0.14026289]\n",
      "1\n",
      "par square: [97.70038, nan, nan, 0.6548741, 0.0018531202, 0.43781805, 0.0017720365, 5.6292856e-07, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n",
      " 0.14285715]\n",
      "2\n",
      "par square: [96.21189, nan, nan, 0.48288867, 3.0699985e-06, 0.33150533, 3.0358185e-06, 5.9641683e-09, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "3\n",
      "par square: [104.55119, nan, nan, 0.6985035, 0.005256822, 0.06353392, 0.0050555975, 0.0014186422, nan]\n",
      "before softmax: [0. 0. 0.]\n",
      "after softmax: [0.33333334 0.33333334 0.33333334]\n",
      "4\n",
      "par square: [64.019905, nan, nan, 0.41520578, 4.2188393e-07, 0.013289727, 4.1585724e-07, 3.954345e-07, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n",
      " 0.14285715]\n",
      "5\n",
      "par square: [84.0253, nan, nan, 0.30477557, 1.0591335e-11, 0.007969047, 1.07935275e-11, 9.978235e-12, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.08333334 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334\n",
      " 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334]\n",
      "6\n",
      "par square: [75.46627, nan, nan, 0.389006, 5.669092e-10, 0.19020149, 5.715716e-10, 6.9552475e-10, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "7\n",
      "par square: [85.37573, nan, nan, 0.21448658, 8.7460144e-13, 0.013054229, 8.884384e-13, 1.1883558e-12, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308]\n",
      "8\n",
      "par square: [90.26393, nan, nan, 0.46758494, 1.6721548e-05, 0.0045004757, 1.7293516e-05, 2.9269428e-05, nan]\n",
      "before softmax: [0. 0. 0. 0. 0.]\n",
      "after softmax: [0.2 0.2 0.2 0.2 0.2]\n",
      "9\n",
      "par square: [86.95219, nan, nan, 0.17749408, 1.184963e-14, 7.826776e-06, 1.2150404e-14, 1.9452554e-14, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "10\n",
      "par square: [78.01016, nan, nan, 0.23808831, 6.5656087e-12, 6.887501e-06, 6.6948244e-12, 1.002805e-11, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.08333334 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334\n",
      " 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334]\n",
      "11\n",
      "par square: [101.81076, nan, nan, 0.24677251, 7.1752743e-12, 7.277177e-08, 7.064795e-12, 9.46709e-12, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.08333334 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334\n",
      " 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334]\n",
      "12\n",
      "par square: [71.59448, nan, nan, 0.44389063, 2.3386547e-06, 0.003134954, 2.3515236e-06, 3.0426775e-06, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "13\n",
      "par square: [111.603096, nan, nan, 0.24669835, 4.6290408e-10, 2.422903e-06, 4.6012597e-10, 6.2025074e-10, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "14\n",
      "par square: [81.95633, nan, nan, 0.48911694, 0.00013883403, 1.7737953e-07, 0.0001376418, 0.00020578098, nan]\n",
      "before softmax: [0. 0. 0. 0.]\n",
      "after softmax: [0.25 0.25 0.25 0.25]\n",
      "15\n",
      "par square: [118.75239, nan, nan, 0.35519412, 2.4197362e-07, 8.191774e-07, 2.310042e-07, 3.3941294e-07, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n",
      " 0.14285715]\n",
      "16\n",
      "par square: [83.70283, nan, nan, 0.35443395, 2.3766233e-06, 2.2541995e-08, 2.393118e-06, 2.79053e-06, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "17\n",
      "par square: [101.97809, nan, nan, 0.35008022, 2.3830174e-07, 3.927882e-09, 2.3759763e-07, 3.3234645e-07, nan]\n",
      "before softmax: [0. 0. 0. 0. 0. 0. 0.]\n",
      "after softmax: [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n",
      " 0.14285715]\n",
      "18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d0bf1599c108>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 _norm_par,_embedding,_a_list,r3,_auc, _loss,_=sess.run([norm_par,embedding,a_list,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n\u001b[0;32m    103\u001b[0m                                         \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_yes\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0ml_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnew_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_id_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnew_r_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                                         image_i:image_1,image_j:image_2})\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;31m#print(XUIJ)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "loss_acc_list = []\n",
    "t0=time.time()\n",
    "\n",
    "#use_true=init_list_of_objects(136)\n",
    "#use_test=init_list_of_objects(136)\n",
    "\n",
    "#train_pair_t=[] #positive feedback\n",
    "#train_pair_f=[] #negative feedback\n",
    "train_yes_id=[] \n",
    "for q in range(5):\n",
    "    print('Iteraction:',q)\n",
    "    train_auc=0\n",
    "    total_loss=0\n",
    "    xuij_auc=0\n",
    "    length = 0\n",
    "    for z in range(n):\n",
    "        print(z)\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes=[]\n",
    "        yesr=[]\n",
    "        \n",
    "        \n",
    "        sample=random.sample(train_t[z],len(train_t[z])) #隨機選3個sample true's YouTuber\n",
    "        train_yes_id.append(sample) #sample全部丟進去\n",
    "        \n",
    "        #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "        #change\n",
    "        r_3=np.zeros(len(sample)) \n",
    "        alpha_history = []\n",
    "        a_list_history = []\n",
    "        U_history = []\n",
    "        Y_history = []\n",
    "        \n",
    "        #print(len(sample))\n",
    "        #check if all YouTuber are in train_t or train_f\n",
    "        #if len(train_t[z])+len(train_f[z]) != 88:\n",
    "            #print(z,len(train_t[z])+len(train_f[z]))\n",
    "         \n",
    "        for b in range(len(sample)):\n",
    "            yes.append(all_3374[sample[b]])\n",
    "            yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "            #print('YouTuber_category ', YouTuber_category[sample[k]])\n",
    "            #print('User_category ',user_category_norm[z])\n",
    "        #print(yesr)\n",
    "        \n",
    "        for b in range(len(yesr)):\n",
    "            r_3[b]=max(yesr[b])\n",
    "        #print('r_3:',r_3)\n",
    "        \n",
    "        yes=np.array(yes)\n",
    "        #print('user shape should be ',np.array([z]).shape)\n",
    "        #print('xf shape should be ',yes.shape)\n",
    "        #print('r shape should be ',np.array(r_3).shape)\n",
    "        #print('l_id shape should be ',np.array(sample).shape)\n",
    "        \n",
    "        #not_used_list = list(set(train_t[z]).difference(set(sample)))\n",
    "        \n",
    "        #取positive \n",
    "        train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "        train_f_sample = random.sample(train_f[z],20)\n",
    "        for ta in train_t_sample:\n",
    "            #print(ta,'--> positive feedback')\n",
    "            \n",
    "            pos = sample.index(ta)\n",
    "            #new_sample = np.delete(sample,[pos])\n",
    "            #new_yes = np.delete(yes,[pos],axis=0)\n",
    "            #new_r_3 = np.delete(r_3,[pos])\n",
    "            new_sample = sample\n",
    "            new_yes = yes\n",
    "            new_r_3 = r_3\n",
    "            #print(len(yes),len(new_yes))\n",
    "            #print(yes)\n",
    "            #print(new_yes)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #ta=random.choice(train_t[z]) #ta is true positve photo\n",
    "            #train_pair_t.append(ta)\n",
    "            image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "            #print('Image_1 shape ',image_1.shape)\n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            #print('True:',train_t_sample,'Now:',ta)\n",
    "            #print('False:',train_f_sample)\n",
    "            for b in train_f_sample:\n",
    "                #print('likes:',ta,';Not likes:',b)\n",
    "                #b=random.choice(train_f[z])  #b is no feedback photo\n",
    "                #train_pair_f.append(b)\n",
    "                image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                #print('Image_2 shape',image_2.shape)\n",
    "            \n",
    "                #use_test[z].append(b)\n",
    "                _norm_par,_embedding,_a_list,r3,_auc, _loss,_=sess.run([norm_par,embedding,a_list,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: new_yes , l_id:new_sample, l_id_len:[len(new_sample)],r:new_r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                \n",
    "                #print(XUIJ)\n",
    "                #print('loss=',_loss)\n",
    "                #print('auc=',_auc)\n",
    "                \n",
    "                #print('after softmax:',r3)\n",
    "                #print('before softmax:',_a_list)\n",
    "                #print('par square:',_norm_par)\n",
    "                #print('embedding:',_embedding)\n",
    "                #print('---------------------------------------------------')\n",
    "                a_list_history.append(_a_list)\n",
    "                alpha_history.append(r3)\n",
    "                train_auc+=_auc\n",
    "                total_loss+=_loss\n",
    "                length += 1\n",
    "            #now1+=1\n",
    "        #final par for this user \n",
    "        print('par square:',_norm_par)\n",
    "        print('before softmax:',_a_list)\n",
    "        print('after softmax:',r3)\n",
    "        \n",
    "        #np.save('../Data/latent_factor/YRM_up10_ALL/Embedding/'+str(q)+'_'+str(z),_embedding)\n",
    "    \n",
    "    #print('mine:',xuij_auc/136)    \n",
    "    #print('a_list_soft:',r3)\n",
    "    print(\"total_loss:-----------------\", total_loss/length)\n",
    "    print(\"train_auc:-------------------\", train_auc/length)\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "    loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "    print('time:',time.time()-t0,' sec')\n",
    "print('Total cost ',time.time()-t0,' sec')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "loss= [[0.1786452]]\n",
      "acc= 0.9414904862579281\n",
      "time= 177.7749536037445\n",
      "Iteration: 1\n",
      "loss= [[0.07883559]]\n",
      "acc= 0.9730443974630021\n",
      "time= 355.5616991519928\n",
      "Iteration: 2\n",
      "loss= [[0.04341662]]\n",
      "acc= 0.9867864693446089\n",
      "time= 532.7977256774902\n",
      "Iteration: 3\n",
      "loss= [[0.03551585]]\n",
      "acc= 0.9904862579281184\n",
      "time= 709.9629302024841\n",
      "Iteration: 4\n",
      "loss= [[0.02745813]]\n",
      "acc= 0.9932875264270613\n",
      "time= 887.3929221630096\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(loss_acc_list)):\n",
    "    print('Iteration:',i)\n",
    "    print('loss=',loss_acc_list[i][0])\n",
    "    print('acc=',loss_acc_list[i][1])\n",
    "    print('time=',loss_acc_list[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Y, A,Au, Ay, Aa, Av,E,B =sess.run([user_latent, item_latent, aux_item, Wu, Wy, Wa, Wv,embedding,Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.04104333e-12 -1.59074717e-12  7.57114708e-14 ... -1.20459675e-11\n",
      "  -1.77368114e-12  1.34361391e-12]\n",
      " [ 2.86623346e-14 -1.48995859e-15  3.98614328e-13 ... -2.75339408e-12\n",
      "   2.87653365e-12 -1.13229345e-13]\n",
      " [ 3.80117893e-20  2.53917678e-20 -7.02632868e-20 ...  8.73379031e-26\n",
      "   6.67104248e-24  5.15725269e-20]\n",
      " ...\n",
      " [-1.33012264e-30 -1.37221756e-30 -1.60515466e-27 ...  5.94920121e-29\n",
      "  -9.55780329e-27 -6.63738444e-26]\n",
      " [-4.83271807e-20  8.01136149e-23  4.46649668e-23 ...  2.85591806e-22\n",
      "  -7.36369785e-23  2.13547140e-23]\n",
      " [ 2.88172382e-15 -8.73344438e-17  2.50763935e-15 ...  1.40498149e-16\n",
      "   4.21605356e-17 -2.34492516e-15]]\n"
     ]
    }
   ],
   "source": [
    "print(Av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent shape:  (100, 100)\n",
      "photo latent shape:  (88, 100)\n",
      "Auxilary latent shape:  (88, 100)\n",
      "Wu weight shape: (100, 100)\n",
      "Wy weight shape: (88, 100)\n",
      "Wa weight shape: (100, 100)\n",
      "Wv weight shape: (100, 3374)\n",
      "Beta shape: (100, 150)\n"
     ]
    }
   ],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "#print('W1 weight shape: ',A1.shape)\n",
    "print('Wu weight shape:',Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:',Aa.shape)\n",
    "print('Wv weight shape:',Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 81\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-4396657204d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         alpha[a]=(relu(np.dot(Au,np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa,\n\u001b[1;32m---> 21\u001b[1;33m                             np.expand_dims(A[sample[a]],0).T)+ np.dot(Av,np.expand_dims(all_3374[sample[a]],0).T)))*r\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mmul\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'alpha------------'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# without embedding\n",
    "result=np.zeros((test_amount,88))\n",
    "RS=np.zeros((test_amount,88))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id=[]\n",
    "for s in range(test_amount):\n",
    "    print(s,test_idx[s])\n",
    "\n",
    "    yes=[]\n",
    "    sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha=np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        #print(test_idx[s])\n",
    "        #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "        alpha[a]=(relu(np.dot(Au,np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa,\n",
    "                            np.expand_dims(A[sample[a]],0).T)+ np.dot(Av,np.expand_dims(all_3374[sample[a]],0).T)))*r\n",
    "    mul=np.zeros((1,100))\n",
    "    print('alpha------------',alpha)\n",
    "    print('softmax alpha--------------',softmax(alpha))\n",
    "    for i in range(len(sample)):\n",
    "        mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    for k in range(88):\n",
    "        result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]], all_3374[k].T)\n",
    "print(RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 81\n",
      "Relu--- [0.00016628]\n",
      "Relu--- [-0.00011262]\n",
      "Relu--- [0.00025554]\n",
      "Relu--- [-0.00030319]\n",
      "Relu--- [-0.00047679]\n",
      "Relu--- [0.0002052]\n",
      "alpha------------ [9.82560122e-05 0.00000000e+00 2.32311267e-05 0.00000000e+00\n",
      " 0.00000000e+00 1.21252158e-04]\n",
      "softmax alpha-------------- [0.1666763  0.16665992 0.1666638  0.16665992 0.16665992 0.16668013]\n",
      "1 14\n",
      "Relu--- [-0.00031382]\n",
      "Relu--- [-2.28048077e-05]\n",
      "Relu--- [0.00086642]\n",
      "Relu--- [0.00029657]\n",
      "alpha------------ [0.00000000e+00 0.00000000e+00 6.51441302e-06 2.96568532e-04]\n",
      "softmax alpha-------------- [0.24998106 0.24998106 0.24998268 0.2500552 ]\n",
      "2 3\n",
      "Relu--- [-0.00765053]\n",
      "Relu--- [0.00041229]\n",
      "Relu--- [-0.00089143]\n",
      "alpha------------ [0.         0.00041229 0.        ]\n",
      "softmax alpha-------------- [0.33328752 0.33342496 0.33328752]\n",
      "3 94\n",
      "Relu--- [0.00013523]\n",
      "Relu--- [-0.00011597]\n",
      "Relu--- [-0.0001066]\n",
      "Relu--- [-0.00028271]\n",
      "Relu--- [0.00016099]\n",
      "alpha------------ [0.00013523 0.         0.         0.         0.00016099]\n",
      "softmax alpha-------------- [0.2000152  0.19998815 0.19998815 0.19998815 0.20002035]\n",
      "4 35\n",
      "Relu--- [0.00028992]\n",
      "Relu--- [0.00119222]\n",
      "Relu--- [-0.00015454]\n",
      "alpha------------ [3.70340265e-05 1.19222361e-03 0.00000000e+00]\n",
      "softmax alpha-------------- [0.33320907 0.33359421 0.33319673]\n",
      "5 31\n",
      "Relu--- [-0.03486211]\n",
      "Relu--- [-0.01646018]\n",
      "alpha------------ [0. 0.]\n",
      "softmax alpha-------------- [0.5 0.5]\n",
      "6 28\n",
      "Relu--- [0.00021726]\n",
      "Relu--- [-6.89993265e-05]\n",
      "Relu--- [-1.93463134e-05]\n",
      "Relu--- [0.00016039]\n",
      "Relu--- [0.00020521]\n",
      "Relu--- [-0.00026754]\n",
      "alpha------------ [0.00021726 0.         0.         0.00016039 0.00020521 0.        ]\n",
      "softmax alpha-------------- [0.16668669 0.16665048 0.16665048 0.16667721 0.16668468 0.16665048]\n",
      "7 17\n",
      "Relu--- [-0.00028097]\n",
      "Relu--- [-0.00011259]\n",
      "Relu--- [-0.00010724]\n",
      "Relu--- [-0.00115872]\n",
      "Relu--- [-0.00013566]\n",
      "Relu--- [-4.86755247e-06]\n",
      "Relu--- [-0.00031457]\n",
      "alpha------------ [0. 0. 0. 0. 0. 0. 0.]\n",
      "softmax alpha-------------- [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "8 13\n",
      "Relu--- [0.00025543]\n",
      "Relu--- [0.00016604]\n",
      "Relu--- [-0.00019897]\n",
      "Relu--- [0.00020153]\n",
      "Relu--- [-0.00031458]\n",
      "Relu--- [0.00085856]\n",
      "Relu--- [9.52168557e-05]\n",
      "Relu--- [-1.62368924e-05]\n",
      "Relu--- [-0.0001176]\n",
      "Relu--- [-0.00013566]\n",
      "alpha------------ [2.55430903e-04 9.96241230e-05 0.00000000e+00 4.03060287e-05\n",
      " 0.00000000e+00 2.57566525e-04 1.90433711e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "softmax alpha-------------- [0.10001882 0.10000324 0.09999328 0.09999731 0.09999328 0.10001904\n",
      " 0.09999518 0.09999328 0.09999328 0.09999328]\n",
      "9 86\n",
      "Relu--- [-0.00033174]\n",
      "Relu--- [0.0003753]\n",
      "Relu--- [-0.00050621]\n",
      "alpha------------ [0.        0.0003753 0.       ]\n",
      "softmax alpha-------------- [0.33329163 0.33341674 0.33329163]\n"
     ]
    }
   ],
   "source": [
    "#with Embedding\n",
    "result=np.zeros((test_amount,88))\n",
    "RS=np.zeros((test_amount,88))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id=[]\n",
    "for s in range(test_amount):\n",
    "    print(s,test_idx[s])\n",
    "\n",
    "    yes=[]\n",
    "    sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha=np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        #print(test_idx[s])\n",
    "        #print(np.expand_dims(U[test_idx[s]],0).shape)\n",
    "        #print(Au.shape)\n",
    "        alpha[a]=(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                            np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T)))*r\n",
    "        print('Relu---',np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                            np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))\n",
    "    mul=np.zeros((1,100))\n",
    "    \n",
    "    print('alpha------------',alpha)\n",
    "    print('softmax alpha--------------',softmax(alpha))\n",
    "    for i in range(len(sample)):\n",
    "        mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    for k in range(88):\n",
    "        result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]], np.dot(E,all_3374[k].T))\n",
    "#print(RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取出test的資料\n",
    "testRS = np.zeros((test_amount,yt_test_amount)) #shape 150*20\n",
    "target = np.zeros((test_amount,yt_test_amount))\n",
    "#test_t 是true的\n",
    "#test_f 是false的\n",
    "        \n",
    "for z in range(test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    #positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    #not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "    #前兩個放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81236903, -0.58483609,  6.80391838,  5.16642127, -2.07160275,\n",
       "        -1.07073596,  1.47381239, -6.71778664, -4.38768845,  0.62992083,\n",
       "        -1.17663274, -2.8907774 , -5.39840717, -2.93548915,  2.69882834,\n",
       "        -2.17588813,  0.40968546, -1.19366449,  2.86035374, -0.89758822],\n",
       "       [ 4.71708011, -1.58780145, -1.03309382, -0.67662295,  3.10909413,\n",
       "         5.28811126,  2.64583548,  0.48845324, -0.85946778, -2.68177119,\n",
       "         3.79132009, 16.2498122 ,  4.95179154,  1.55345964,  3.20726294,\n",
       "         5.59514535,  3.13114932,  1.72466598,  4.90734012,  1.51089048],\n",
       "       [ 8.30910635,  7.73596105, 10.79921633,  5.14078609,  8.42743181,\n",
       "        -1.1353207 , -2.82199001, -4.85774304,  0.03773887, -1.99882284,\n",
       "         0.48202722, -7.56569824,  0.65614982,  0.15277922,  2.30333698,\n",
       "         1.18460849, -1.90262803,  2.51180269, -3.06295594,  0.02852794],\n",
       "       [ 4.01153543,  8.72515679, -1.05443674,  1.71226907,  4.38822361,\n",
       "        -3.01381406, -1.41162891,  6.00718244, -0.21815271, -4.85486096,\n",
       "        -7.99508038, -0.35379022, -2.82632389, -0.53056999, -7.17799221,\n",
       "        -4.47289056,  1.14995297,  0.91493523, -0.88025809, -6.36184031],\n",
       "       [-5.10977159,  0.27648   ,  2.03131683, -6.35900397, -8.135052  ,\n",
       "        -4.22702492,  5.56968557, -7.40558493,  0.16244514,  2.92751272,\n",
       "        -7.96777407,  0.85508987, -3.36670558, -4.24067466,  5.69572389,\n",
       "         3.31577016, -4.09929774, -3.93535844,  1.46600474, -0.16934804],\n",
       "       [ 8.77538858,  8.59547616,  7.03003853,  0.42328595, -3.43729275,\n",
       "        -1.39524645, -4.52030786,  3.00179581,  6.83516461,  3.17143502,\n",
       "        -0.49510118,  2.67852302, -0.7255044 ,  5.68543439, -1.18536637,\n",
       "        -1.46007579,  2.64479433,  5.12366225,  0.73113542,  4.94949294],\n",
       "       [ 4.63342576,  1.35695015, 10.31988596, -6.26078778,  1.93537459,\n",
       "        -1.46766042,  0.40693888, 10.45356221, -1.04408235, -1.17538328,\n",
       "         2.07977522, -2.35566236, -1.45896968, -1.71505922, -2.46108018,\n",
       "        -0.46832543,  2.03195159,  0.07305653, 10.01181153, -1.96525495],\n",
       "       [ 2.06136778,  3.24433192, 10.76607244,  2.55284609,  5.94773032,\n",
       "         7.10372148, -3.51326772,  3.31036854, -2.93518841,  9.71970935,\n",
       "        -3.4180543 ,  2.54777862,  4.42242822, -4.14260021, -0.17679022,\n",
       "        -4.50335166, -3.95632114, -3.42744614, -1.14227129,  0.21639856],\n",
       "       [10.12504705,  9.14274145,  3.81423751,  5.61062082, 11.87236902,\n",
       "         7.48218368,  8.68993234,  1.65649649,  3.06148816, 10.52294481,\n",
       "         3.95008602,  9.01919861,  4.95436399,  2.20254502, 10.91237032,\n",
       "         8.44180635, -2.48162304, 13.81676168, 11.42387647,  5.44285384],\n",
       "       [ 5.11403143,  4.42832741,  4.42567061,  4.20461979, 12.82928766,\n",
       "         5.24007008, -3.27406158, -2.1839148 , -7.64930079, -2.49685362,\n",
       "        -5.07434716, -4.39147384,  7.53475226, -8.58692149,  0.8384589 ,\n",
       "         5.70371416,  6.03576033, -4.24261441, -5.62322689, 12.18609744]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of positive data in testing: 55.0\n",
      "total testing data: 200\n"
     ]
    }
   ],
   "source": [
    "sumtarget = 0\n",
    "for i in range(len(target)):\n",
    "    #print(np.sum(target[i]))\n",
    "    sumtarget += np.sum(target[i])\n",
    "print('num of positive data in testing:',sumtarget)\n",
    "print('total testing data:',test_amount*yt_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),int(np.sum(target[i])))\n",
    "    count_0_all.append(top_0)\n",
    "    #print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = 0\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        #print(int(np.sum(target[i])))\n",
    "        total+=int(np.sum(target[i]))\n",
    "        if count_0_all[i][j] < int(np.sum(target[i])): #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "avg_acc = acc_0/100\n",
    "#print('avg_accuarcy for count_0:',avg_acc)\n",
    "#print(acc_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[11]\n",
      "[2]\n",
      "[1]\n",
      "[14]\n",
      "[0]\n",
      "[7]\n",
      "[2]\n",
      "[17]\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),1) #取一個\n",
    "    print(top_0)\n",
    "    count_0_all.append(top_0)\n",
    "    #print(np.sum(target[i]))\n",
    "    #print(top_0)\n",
    "    if top_0[0] < int(np.sum(target[i])):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.6 recall  0.10909090909090909\n"
     ]
    }
   ],
   "source": [
    "top1_prec = correct/len(testRS)\n",
    "top1_recall = correct/(sumtarget)\n",
    "print('prec ',top1_prec,'recall ',top1_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score: 0.1846153846153846\n"
     ]
    }
   ],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top1_prec,top1_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_3 = topN(list(testRS[i]),3) #取一個\n",
    "    count_0_all.append(top_3)\n",
    "    #print(top_3)\n",
    "    for j in range(len(top_3)):\n",
    "        if top_3[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.5333333333333333 recall  0.2909090909090909\n"
     ]
    }
   ],
   "source": [
    "top3_prec = correct/(len(testRS)*3)\n",
    "top3_recall = correct/(sumtarget)\n",
    "print('prec ',top3_prec,'recall ',top3_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score: 0.3764705882352941\n"
     ]
    }
   ],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top3_prec,top3_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 18, 14, 6]\n",
      "[11, 15, 5, 12, 18]\n",
      "[2, 4, 0, 1, 3]\n",
      "[1, 7, 4, 0, 3]\n",
      "[14, 6, 15, 9, 2]\n",
      "[0, 1, 2, 8, 13]\n",
      "[7, 2, 18, 0, 10]\n",
      "[2, 9, 5, 4, 12]\n",
      "[17, 4, 18, 14, 9]\n",
      "[4, 19, 12, 16, 15]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    print(top_5)\n",
    "    count_0_all.append(top_5)\n",
    "    #print(top_5)\n",
    "    for j in range(len(top_5)):\n",
    "        if top_5[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec  0.46 recall  0.41818181818181815\n"
     ]
    }
   ],
   "source": [
    "top5_prec = correct/(len(testRS)*5)\n",
    "top5_recall = correct/(sumtarget)\n",
    "print('prec ',top5_prec,'recall ',top5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score: 0.43809523809523804\n"
     ]
    }
   ],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top5_prec,top5_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_list\n",
    "\"\"\"\n",
    "test_amount = 150\n",
    "yt_test_amount = 20\n",
    "\"\"\"\n",
    "all_sort = []\n",
    "pre_matrix = np.zeros(shape=(test_amount,yt_test_amount))\n",
    "for i in range(test_amount):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    #print(top_5)\n",
    "    all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "    for j in range(len(top_5)):\n",
    "        pre_matrix[i][top_5[j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NDCG\n",
    "https://daiwk.github.io/posts/nlp-ndcg.html\n",
    "\"\"\"\n",
    "#Ideal DCG，理想状况下的DCG。也就是说，相关性完全由高到低排序时算出的DCG：\n",
    "def IDCG(ideal_list): #ideal_list example = [1,1,1,1,1,0,0,....]\n",
    "    idcg=0\n",
    "    for i in range(len(ideal_list)):\n",
    "        #print((2**true_list[i]-1),math.log2(i+2))\n",
    "        idcg+= (2**ideal_list[i]-1)/ math.log2(i+2)\n",
    "    #print('idcg',idcg)\n",
    "    return idcg\n",
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg=0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg+= (2**prec_list[i]-1)/ math.log2(i+2)\n",
    "    #print('dcg',dcg)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.49306817004062264\n"
     ]
    }
   ],
   "source": [
    "total_ndcg = 0\n",
    "num_ndcg = 5\n",
    "for m in range(test_amount):\n",
    "    idcg = IDCG([1]*num_ndcg)\n",
    "    pre_list = []\n",
    "    for s in all_sort[m][:num_ndcg]:\n",
    "        #print(s)\n",
    "        #print(target[m][s])\n",
    "        pre_list.append(target[m][s])\n",
    "    dcg = DCG(pre_list)\n",
    "    ndcg = dcg/idcg\n",
    "    #print(ndcg)\n",
    "    total_ndcg += ndcg\n",
    "avg_ndcg = total_ndcg/test_amount\n",
    "print('NDCG:',avg_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ">>> y_true = np.array([0, 0, 1, 1])\n",
    ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    ">>> average_precision_score(y_true, y_scores)\n",
    "\"\"\"\n",
    "total_prec = 0\n",
    "for u in range(test_amount):\n",
    "    y_true = target[u]\n",
    "    y_scores = pre_matrix[u]\n",
    "    total_prec+=average_precision_score(y_true, y_scores)\n",
    "MAP = total_prec/test_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP 0.46765151515151515\n"
     ]
    }
   ],
   "source": [
    "print('MAP',MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
