{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_following = np.load('../Data/npy/user_following_1489.npy')\n",
    "all_3374 = np.load('../Data/npy/all_2939D_img0.5.npy')\n",
    "user_category = np.load('../Data/npy/user_category_1489.npy')\n",
    "YouTuber_category = np.load('../Data/npy/YouTuber_category_0.7.npy')\n",
    "active_users = np.load('../Data/npy/active_userID_1489.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test_amount = 150\n",
    "yt_test_amount = 18\n",
    "all_auxilary = [i for i in range(88)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test(is_used,user_following,feature,user_category,item_category,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    print('user_category shape ',user_category.shape)\n",
    "    print('YouTuber_category shape ',item_category.shape)\n",
    "    user_category_norm = np.zeros(user_category.shape)\n",
    "    for i in range(len(user_category)):\n",
    "        user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "    print('user_category after normalized by max...')\n",
    "    print('user_category_norm shape ',user_category_norm.shape)\n",
    "    print('user cateogory norm',user_category_norm)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = item_category[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    all_user_idx = [i for i in range(len(user_following))]\n",
    "    user_idx = [i for i in all_user_idx if i not in is_used]\n",
    "    random.seed(5)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    is_used = is_used+test_idx\n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    test_pos = -1\n",
    "    for i in range(num_of_users):\n",
    "        t_for_train = []\n",
    "        f_for_train = []\n",
    "        if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    t_for_train.append(j)\n",
    "                else:\n",
    "                    f_for_train.append(j)\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "\n",
    "        else: #if in test id, choose 2 true and other \n",
    "            test_pos += 1\n",
    "            temp_t = []\n",
    "            temp_f = []\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    temp_t.append(j)\n",
    "                else:\n",
    "                    temp_f.append(j)\n",
    "            t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "            f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,user_category_norm,is_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(user_following)\n",
    "m = 88  \n",
    "k = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.trainable_variables()])\n",
    "    print('trainable_vars:',sess.run(all_trainable_vars))\n",
    "    \"\"\"for v in tf.trainable_variables():\n",
    "        print(v)\"\"\"\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(10):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #user degree of category favor \n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                train_f_sample = random.sample(train_f[z],10)\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    \n",
    "                    _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "                    \n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])\n",
    "    np.savez('../Data/pair_t_test/Our/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    #np.savez('../Data/npy/mask_feature/result300/'+save_name+'.npz', \n",
    "    #                    U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    \n",
    "    return U, Y, A, E, Au, Ay, Aa, Av,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate_train_test----\n",
      "user_following shape  (1489, 88)\n",
      "feature shape  (88, 2939)\n",
      "user_category shape  (1489, 17)\n",
      "YouTuber_category shape  (88, 17)\n",
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n",
      "user cateogory norm [[0.         1.         0.         ... 0.05714286 0.         0.        ]\n",
      " [0.24390244 0.         0.02439024 ... 0.         0.         0.09756098]\n",
      " [0.04210526 0.04210526 0.05263158 ... 0.02105263 0.         0.02105263]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01408451 0.01408451 0.04225352 ... 0.02816901 0.         0.02816901]\n",
      " [0.03703704 0.22222222 0.14814815 ... 0.         0.         0.        ]]\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "test_idx [3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n",
      "150\n",
      "Finished []\n",
      "2939\n",
      "Now Dims: 200\n",
      "trainable_vars: 52356768\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.55322178]]\n",
      "train_auc:------------------- 0.8049277357192016\n",
      "time: 4152.019584655762  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.41607652]]\n",
      "train_auc:------------------- 0.8410461114934618\n",
      "time: 8314.757387161255  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.38375051]]\n",
      "train_auc:------------------- 0.8630626290433586\n",
      "time: 12477.970615386963  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.36299155]]\n",
      "train_auc:------------------- 0.8724638678596008\n",
      "time: 16659.394601345062  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.34535541]]\n",
      "train_auc:------------------- 0.8806538196834136\n",
      "time: 20841.71004152298  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.33418055]]\n",
      "train_auc:------------------- 0.8837026841018583\n",
      "time: 25023.167372226715  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.32441256]]\n",
      "train_auc:------------------- 0.8882863041982106\n",
      "time: 29176.676420927048  sec\n",
      "Iteraction: 7\n",
      "total_loss:----------------- [[0.313709]]\n",
      "train_auc:------------------- 0.8934549208534067\n",
      "time: 33358.40491294861  sec\n",
      "Iteraction: 8\n",
      "total_loss:----------------- [[0.30471328]]\n",
      "train_auc:------------------- 0.8986097728836889\n",
      "time: 37541.194135427475  sec\n",
      "Iteraction: 9\n",
      "total_loss:----------------- [[0.2973694]]\n",
      "train_auc:------------------- 0.902401927047488\n",
      "time: 41709.28166651726  sec\n",
      "Total cost  41709.28166651726  sec\n",
      "Finish dims:, 200\n",
      "Generate_train_test----\n",
      "user_following shape  (1489, 88)\n",
      "feature shape  (88, 2939)\n",
      "user_category shape  (1489, 17)\n",
      "YouTuber_category shape  (88, 17)\n",
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n",
      "user cateogory norm [[0.         1.         0.         ... 0.05714286 0.         0.        ]\n",
      " [0.24390244 0.         0.02439024 ... 0.         0.         0.09756098]\n",
      " [0.04210526 0.04210526 0.05263158 ... 0.02105263 0.         0.02105263]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01408451 0.01408451 0.04225352 ... 0.02816901 0.         0.02816901]\n",
      " [0.03703704 0.22222222 0.14814815 ... 0.         0.         0.        ]]\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "test_idx [4, 8, 13, 22, 31, 43, 52, 54, 70, 77, 79, 87, 110, 115, 123, 131, 134, 153, 155, 169, 182, 189, 211, 213, 235, 246, 249, 260, 273, 290, 299, 302, 305, 318, 335, 342, 343, 361, 367, 373, 383, 384, 389, 406, 410, 417, 420, 427, 454, 455, 459, 471, 476, 478, 480, 494, 496, 547, 565, 566, 573, 575, 576, 586, 587, 604, 635, 645, 667, 681, 687, 694, 707, 708, 714, 715, 720, 723, 729, 731, 769, 775, 785, 789, 790, 798, 799, 820, 821, 826, 829, 836, 839, 846, 850, 860, 870, 879, 884, 898, 930, 935, 939, 954, 956, 965, 972, 998, 1023, 1039, 1045, 1060, 1061, 1067, 1075, 1082, 1101, 1102, 1108, 1131, 1170, 1171, 1197, 1210, 1218, 1232, 1242, 1245, 1254, 1291, 1312, 1321, 1340, 1347, 1356, 1360, 1370, 1375, 1391, 1406, 1407, 1419, 1421, 1425, 1428, 1436, 1455, 1479, 1483, 1485]\n",
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n",
      "300\n",
      "Finished ['0']\n",
      "2939\n",
      "Now Dims: 200\n",
      "trainable_vars: 52356768\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.54974991]]\n",
      "train_auc:------------------- 0.8064243506939673\n",
      "time: 4148.076457262039  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.41498136]]\n",
      "train_auc:------------------- 0.8426068434794558\n",
      "time: 8337.000770807266  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.38143962]]\n",
      "train_auc:------------------- 0.8644565068022537\n",
      "time: 12513.83088684082  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.36095623]]\n",
      "train_auc:------------------- 0.8737460491960973\n",
      "time: 16692.55219745636  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.34284083]]\n",
      "train_auc:------------------- 0.8812972378727497\n",
      "time: 20856.42867207527  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.33338029]]\n",
      "train_auc:------------------- 0.8846846227841143\n",
      "time: 25045.551762104034  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.32321291]]\n",
      "train_auc:------------------- 0.8883468462278411\n",
      "time: 29234.934940099716  sec\n",
      "Iteraction: 7\n",
      "total_loss:----------------- [[0.31274231]]\n",
      "train_auc:------------------- 0.8932595849938161\n",
      "time: 33404.813784360886  sec\n",
      "Iteraction: 8\n",
      "total_loss:----------------- [[0.30457212]]\n",
      "train_auc:------------------- 0.8980417754569191\n",
      "time: 37572.64285326004  sec\n",
      "Iteraction: 9\n",
      "total_loss:----------------- [[0.29636706]]\n",
      "train_auc:------------------- 0.9020681599560259\n",
      "time: 41746.33907175064  sec\n",
      "Total cost  41746.33907175064  sec\n",
      "Finish dims:, 200\n",
      "Generate_train_test----\n",
      "user_following shape  (1489, 88)\n",
      "feature shape  (88, 2939)\n",
      "user_category shape  (1489, 17)\n",
      "YouTuber_category shape  (88, 17)\n",
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n",
      "user cateogory norm [[0.         1.         0.         ... 0.05714286 0.         0.        ]\n",
      " [0.24390244 0.         0.02439024 ... 0.         0.         0.09756098]\n",
      " [0.04210526 0.04210526 0.05263158 ... 0.02105263 0.         0.02105263]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01408451 0.01408451 0.04225352 ... 0.02816901 0.         0.02816901]\n",
      " [0.03703704 0.22222222 0.14814815 ... 0.         0.         0.        ]]\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "test_idx [5, 11, 16, 27, 36, 51, 61, 63, 82, 89, 91, 101, 127, 132, 142, 151, 154, 159, 175, 177, 193, 206, 214, 238, 240, 265, 277, 280, 292, 310, 329, 341, 346, 347, 362, 369, 382, 386, 390, 391, 392, 413, 421, 425, 437, 438, 445, 446, 463, 466, 474, 477, 485, 517, 518, 522, 536, 542, 544, 546, 560, 562, 619, 643, 644, 653, 655, 656, 666, 668, 686, 718, 722, 737, 762, 766, 776, 778, 787, 795, 807, 808, 814, 815, 822, 825, 833, 837, 840, 876, 882, 892, 896, 897, 905, 906, 929, 931, 938, 941, 948, 951, 962, 966, 977, 979, 988, 999, 1004, 1017, 1051, 1052, 1056, 1062, 1077, 1079, 1089, 1094, 1096, 1126, 1129, 1152, 1168, 1169, 1177, 1191, 1192, 1199, 1208, 1211, 1217, 1236, 1239, 1240, 1247, 1258, 1271, 1317, 1318, 1342, 1348, 1361, 1368, 1382, 1393, 1396, 1405, 1448, 1470, 1482]\n",
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n",
      "450\n",
      "Finished ['0', '1']\n",
      "2939\n",
      "Now Dims: 200\n",
      "trainable_vars: 52356768\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.547971]]\n",
      "train_auc:------------------- 0.8072987674722854\n",
      "time: 4136.8450763225555  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.41575039]]\n",
      "train_auc:------------------- 0.8406527576946912\n",
      "time: 8308.889150619507  sec\n",
      "Iteraction: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:----------------- [[0.38385164]]\n",
      "train_auc:------------------- 0.8622598636645321\n",
      "time: 12503.337681770325  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.36324066]]\n",
      "train_auc:------------------- 0.8735385250981202\n",
      "time: 16689.352742433548  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.34776642]]\n",
      "train_auc:------------------- 0.8794463953728568\n",
      "time: 20884.65567088127  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.33683037]]\n",
      "train_auc:------------------- 0.883281691110652\n",
      "time: 25043.763369321823  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.32576412]]\n",
      "train_auc:------------------- 0.8883219720443435\n",
      "time: 29238.362821102142  sec\n",
      "Iteraction: 7\n",
      "total_loss:----------------- [[0.31644783]]\n",
      "train_auc:------------------- 0.8931212559388556\n",
      "time: 33432.98570418358  sec\n",
      "Iteraction: 8\n",
      "total_loss:----------------- [[0.30722418]]\n",
      "train_auc:------------------- 0.8966053845624182\n",
      "time: 37626.33842635155  sec\n",
      "Iteraction: 9\n",
      "total_loss:----------------- [[0.29897165]]\n",
      "train_auc:------------------- 0.9008469324519728\n",
      "time: 41794.73580479622  sec\n",
      "Total cost  41794.73580479622  sec\n",
      "Finish dims:, 200\n",
      "Generate_train_test----\n",
      "user_following shape  (1489, 88)\n",
      "feature shape  (88, 2939)\n",
      "user_category shape  (1489, 17)\n",
      "YouTuber_category shape  (88, 17)\n",
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n",
      "user cateogory norm [[0.         1.         0.         ... 0.05714286 0.         0.        ]\n",
      " [0.24390244 0.         0.02439024 ... 0.         0.         0.09756098]\n",
      " [0.04210526 0.04210526 0.05263158 ... 0.02105263 0.         0.02105263]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01408451 0.01408451 0.04225352 ... 0.02816901 0.         0.02816901]\n",
      " [0.03703704 0.22222222 0.14814815 ... 0.         0.         0.        ]]\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "test_idx [1, 7, 12, 19, 32, 38, 39, 42, 60, 76, 93, 94, 104, 105, 113, 144, 145, 164, 170, 184, 196, 202, 212, 222, 224, 239, 248, 250, 262, 268, 269, 274, 289, 293, 300, 301, 309, 311, 317, 333, 365, 375, 385, 398, 416, 424, 440, 450, 453, 458, 465, 470, 473, 479, 482, 502, 514, 516, 525, 537, 541, 548, 550, 553, 555, 569, 580, 588, 606, 610, 620, 624, 651, 669, 677, 691, 709, 724, 745, 753, 771, 772, 773, 794, 817, 830, 841, 880, 891, 895, 907, 915, 923, 936, 945, 950, 1025, 1033, 1038, 1046, 1057, 1059, 1065, 1080, 1119, 1125, 1127, 1135, 1141, 1142, 1146, 1155, 1157, 1162, 1173, 1186, 1219, 1221, 1222, 1255, 1260, 1280, 1290, 1295, 1296, 1300, 1303, 1322, 1323, 1330, 1353, 1373, 1376, 1379, 1381, 1384, 1385, 1400, 1413, 1415, 1416, 1422, 1432, 1434, 1452, 1456, 1463, 1466, 1478, 1481]\n",
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n",
      "600\n",
      "Finished ['0', '1', '2']\n",
      "2939\n",
      "Now Dims: 200\n",
      "trainable_vars: 52356768\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.55110947]]\n",
      "train_auc:------------------- 0.8061391447820707\n",
      "time: 4180.860145330429  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.41775735]]\n",
      "train_auc:------------------- 0.8411109583390622\n",
      "time: 8356.893084049225  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.38473222]]\n",
      "train_auc:------------------- 0.8622164168843668\n",
      "time: 12529.629980802536  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.36332022]]\n",
      "train_auc:------------------- 0.8729135157431597\n",
      "time: 16676.085304260254  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.34649834]]\n",
      "train_auc:------------------- 0.8793963976350887\n",
      "time: 20865.013000011444  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.33493395]]\n",
      "train_auc:------------------- 0.8844012099546267\n",
      "time: 25047.237864255905  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.32419233]]\n",
      "train_auc:------------------- 0.8885604289839131\n",
      "time: 29225.161279916763  sec\n",
      "Iteraction: 7\n",
      "total_loss:----------------- [[0.315124]]\n",
      "train_auc:------------------- 0.8921146706998487\n",
      "time: 33384.299038648605  sec\n",
      "Iteraction: 8\n",
      "total_loss:----------------- [[0.30654943]]\n",
      "train_auc:------------------- 0.8960813969476145\n",
      "time: 37537.5472035408  sec\n",
      "Iteraction: 9\n",
      "total_loss:----------------- [[0.29822637]]\n",
      "train_auc:------------------- 0.901065585040561\n",
      "time: 41743.16114807129  sec\n",
      "Total cost  41743.16114807129  sec\n",
      "Finish dims:, 200\n",
      "Generate_train_test----\n",
      "user_following shape  (1489, 88)\n",
      "feature shape  (88, 2939)\n",
      "user_category shape  (1489, 17)\n",
      "YouTuber_category shape  (88, 17)\n",
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n",
      "user cateogory norm [[0.         1.         0.         ... 0.05714286 0.         0.        ]\n",
      " [0.24390244 0.         0.02439024 ... 0.         0.         0.09756098]\n",
      " [0.04210526 0.04210526 0.05263158 ... 0.02105263 0.         0.02105263]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01408451 0.01408451 0.04225352 ... 0.02816901 0.         0.02816901]\n",
      " [0.03703704 0.22222222 0.14814815 ... 0.         0.         0.        ]]\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "test_idx [2, 14, 17, 24, 30, 41, 49, 50, 62, 74, 92, 103, 112, 125, 126, 138, 149, 172, 173, 190, 198, 204, 216, 227, 234, 247, 257, 261, 263, 278, 282, 291, 294, 315, 316, 323, 324, 328, 351, 353, 359, 371, 374, 387, 388, 444, 447, 451, 452, 462, 464, 484, 500, 507, 528, 534, 539, 549, 557, 561, 563, 568, 571, 577, 578, 598, 608, 612, 616, 636, 640, 641, 649, 652, 658, 660, 664, 673, 678, 683, 693, 719, 730, 743, 748, 774, 800, 810, 832, 842, 847, 852, 864, 913, 914, 932, 943, 964, 971, 974, 980, 1012, 1018, 1028, 1031, 1041, 1054, 1070, 1083, 1090, 1091, 1095, 1097, 1133, 1137, 1178, 1188, 1193, 1195, 1204, 1214, 1223, 1226, 1249, 1266, 1278, 1282, 1293, 1302, 1304, 1311, 1316, 1331, 1334, 1349, 1350, 1351, 1357, 1363, 1365, 1386, 1394, 1402, 1424, 1439, 1444, 1445, 1464, 1477, 1486]\n",
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n",
      "750\n",
      "Finished ['0', '1', '2', '3']\n",
      "2939\n",
      "Now Dims: 200\n",
      "trainable_vars: 52356768\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.54984236]]\n",
      "train_auc:------------------- 0.8053004262340162\n",
      "time: 4147.095548152924  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.41524893]]\n",
      "train_auc:------------------- 0.840815344424584\n",
      "time: 8308.384399175644  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.3823625]]\n",
      "train_auc:------------------- 0.8642032173793482\n",
      "time: 12476.083240747452  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.36139784]]\n",
      "train_auc:------------------- 0.873216004399835\n",
      "time: 16647.80694270134  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.34531196]]\n",
      "train_auc:------------------- 0.8807094733947477\n",
      "time: 20821.425988912582  sec\n",
      "Iteraction: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-909fbe316b1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    190\u001b[0m                     \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxuij\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                     \u001b[0mUr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAyr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAvr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ALL_2_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Finish dims:,'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-b930cf365446>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(save_name)\u001b[0m\n\u001b[0;32m     68\u001b[0m                     _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n\u001b[0;32m     69\u001b[0m                                         \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myes\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0ml_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_id_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpositive_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpositive_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                                         image_i:image_1,image_j:image_2})\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[1;31m#print(_a_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[1;31m#print(r3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "l = all_3374.shape[1]\n",
    "try_count = [str(try_i) for try_i in range(5)]\n",
    "is_used = []\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "\n",
    "for tri in try_count:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #第一次已經跑完了\n",
    "                    #is_used = [3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
    "                    \n",
    "                    train_t,train_f,test_t,test_f,user_category_norm,is_used = generate_train_test(is_used,user_following,all_3374,user_category,YouTuber_category,active_users,user_test_amount,yt_test_amount)\n",
    "                    print(len(list(set(is_used))))\n",
    "                    #clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(tri)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    \n",
    "                    \n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=-1+int(tri)))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=-1+int(tri))) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=-1+int(tri)))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training('ALL_2_'+tri)\n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "l = all_3374.shape[1]\n",
    "try_count = [str(try_i) for try_i in range(5)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "\n",
    "for tri in try_count:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #第一次已經跑完了\n",
    "                    #is_used = [3, 6, 10, 18, 26, 37, 44, 46, 59, 65, 67, 75, 95, 99, 106, 114, 116, 133, 135, 147, 160, 165, 186, 188, 208, 221, 231, 243, 259, 270, 284, 298, 303, 304, 321, 326, 330, 339, 340, 360, 363, 370, 372, 378, 402, 403, 407, 419, 426, 428, 441, 443, 486, 503, 504, 510, 512, 513, 523, 524, 540, 564, 572, 592, 605, 611, 617, 626, 627, 633, 634, 639, 642, 646, 648, 679, 692, 696, 697, 704, 705, 726, 727, 732, 734, 739, 742, 749, 752, 761, 770, 779, 784, 797, 827, 831, 835, 849, 857, 863, 886, 911, 927, 933, 946, 947, 953, 960, 967, 984, 985, 990, 1049, 1050, 1074, 1085, 1092, 1104, 1113, 1116, 1124, 1175, 1184, 1200, 1207, 1216, 1220, 1230, 1235, 1250, 1264, 1265, 1275, 1277, 1281, 1283, 1307, 1329, 1333, 1335, 1388, 1404, 1411, 1414, 1426, 1438, 1443, 1449, 1474, 1476]\n",
    "                    \n",
    "                    train_t,train_f,test_t,test_f,user_category_norm,is_used = generate_train_test(is_used,user_following,all_3374,user_category,YouTuber_category,active_users,user_test_amount,yt_test_amount)\n",
    "                    print(len(list(set(is_used))))\n",
    "                    \"\"\" #clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(tri)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    \"\"\"n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\"\"\"\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    \n",
    "                    \n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=-1+int(tri)))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=-1+int(tri))) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=-1+int(tri)))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    \"\"\"for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\"\"\"\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training('ALL'+tri)\n",
    "                    print('Finish dims:,',embedding_dims)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
