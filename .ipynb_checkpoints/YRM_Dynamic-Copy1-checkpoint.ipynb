{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_following = np.load('../Data/npy/user_following.npy')\n",
    "image_2048 = np.load('../Data/npy/Image_2048D.npy')\n",
    "user_category = np.load('../Data/npy/user_category_1216.npy')\n",
    "YouTuber_category = np.load('../Data/npy/YouTuber_category_0.7.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_following shape  (1489, 88)\n",
      "image_2048 shape  (88, 2048)\n",
      "user_category shape  (1489, 17)\n",
      "YouTuber_category shape  (88, 17)\n"
     ]
    }
   ],
   "source": [
    "print('user_following shape ',user_following.shape)\n",
    "print('image_2048 shape ',image_2048.shape)\n",
    "print('user_category shape ',user_category.shape)\n",
    "print('YouTuber_category shape ',YouTuber_category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n"
     ]
    }
   ],
   "source": [
    "user_category_norm = np.zeros(user_category.shape)\n",
    "for i in range(len(user_category)):\n",
    "    user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "print('user_category after normalized by max...')\n",
    "print('user_category_norm shape ',user_category_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "following_true = [0]*len(user_following)\n",
    "for i in range(len(user_following)):\n",
    "    each_user = []\n",
    "    for j in range(len(user_following[i])):\n",
    "        if user_following[i][j] == 1:\n",
    "            each_user.append(j)\n",
    "    following_true[i] = each_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings  5\n",
      "Max number of followings  34\n"
     ]
    }
   ],
   "source": [
    "#最少跟最多的following \n",
    "minlen = 10000\n",
    "maxlen = 0\n",
    "for i in range(len(following_true)):\n",
    "    if len(following_true[i]) < minlen:\n",
    "        minlen = len(following_true[i])\n",
    "    if len(following_true[i]) > maxlen:\n",
    "        maxlen = len(following_true[i])\n",
    "print('Min number of followings ',minlen)\n",
    "print('Max number of followings ',maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = [i for i in range(len(user_following))]\n",
    "#test_idx is the number of user for testing\n",
    "test_idx = random.sample(user_idx,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training  and Testing\n",
    "train_t = [0]*(len(user_following))\n",
    "train_f = [0]*(len(user_following))\n",
    "# Testing \n",
    "test_t = [0]*200\n",
    "test_f = [0]*200\n",
    "test_pos = -1\n",
    "\n",
    "for i in range(len(user_following)):\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train\n",
    "        \n",
    "    else: #if in test id, choose 2 true and other \n",
    "        test_pos += 1\n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        # random choose 2 true and 8 false for test \n",
    "        t_for_test = random.sample(temp_t,2)\n",
    "        f_for_test  = random.sample(temp_f,8)\n",
    "        test_t[test_pos] = t_for_test\n",
    "        test_f[test_pos] = f_for_test\n",
    "        \n",
    "        #other for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 200\n",
      "The length of test_f: 200\n"
     ]
    }
   ],
   "source": [
    "# train_t[i] 代表的是user i positive feedback\n",
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_t))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1489"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_aux_size = [len(train_t[i]) for i in range(len(train_t))]\n",
    "len(user_aux_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation  Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "n: the number of users\n",
    "m: the number of YouTubers\n",
    "k: latent dims\n",
    "l: feature dims\n",
    "\"\"\"\n",
    "n = 1489 \n",
    "m = 88  \n",
    "k = 32\n",
    "l = 2048 \n",
    "\n",
    "user = tf.placeholder(tf.int32,shape=(1,))\n",
    "i = tf.placeholder(tf.int32, shape=(1,))\n",
    "j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "\"\"\"\n",
    "r3,_auc, _loss,_=sess.run([a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample,l_id_len:len(sample),r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                      initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                      initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                      initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    W1 = tf.get_variable(\"W1\", [n, k],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [n,k,k],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wy = tf.get_variable(\"Wy\", [n,k,k],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wa = tf.get_variable(\"Wa\", [n,k,k],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wv = tf.get_variable(\"Wv\", [n,k,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    \n",
    "\n",
    "    aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "    ########## Error part, how to get auxisize dynamically\n",
    "    ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "    \n",
    "with tf.variable_scope('feature_level'):\n",
    "    Beta = tf.get_variable(\"beta\", [n,l],\n",
    "                             # initializer=tf.contrib.layers.xavier_initializer())\n",
    "                                     initializer=tf.random_normal_initializer(0.00001,0.000001,seed=10))\n",
    "\n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "w1 = tf.nn.embedding_lookup(W1, user) #(1*k)\n",
    "wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(k*k)\n",
    "wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(k*k)\n",
    "wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(k*k)\n",
    "wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(k,l)\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-b8f5848ee2cf>:74: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "a_list=tf.Variable([])\n",
    "q = tf.constant(0)\n",
    "def att_cond(q,a_list):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "def att_body(q,a_list):\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "    \n",
    "    a_list = tf.concat([a_list,[(tf.matmul( w1, tf.nn.relu( tf.matmul(wu, u, transpose_b=True) +\n",
    "        tf.matmul(wy, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "        tf.matmul(wa, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "        tf.matmul(wv, xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "    q += 1\n",
    "    return q,  a_list\n",
    "\n",
    "_, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for q in range(3): #取l_id個 YouTuber 的 類別\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "    a_list.append((tf.matmul( w1, tf.nn.relu( tf.matmul(wu, u, transpose_b=True) +\n",
    "        tf.matmul(wy, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "        tf.matmul(wa, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "        tf.matmul(wv, xfi, transpose_b=True)))[0][0])*r[q])\n",
    "\"\"\"\n",
    "a_list_soft=tf.nn.softmax(a_list)\n",
    "\n",
    "\n",
    "aux_np = tf.expand_dims(tf.zeros(32),0) #dimension (1,32)\n",
    "q = tf.constant(0)\n",
    "def sum_att_cond(q,aux_np):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def sum_att_body(q,aux_np):\n",
    "    #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "    aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)])  # [[7, 16], [10, 25]]\n",
    "    q += 1\n",
    "    return q, aux_np\n",
    "\n",
    "_,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "\"\"\"\n",
    "for q in range(3): #取q個auxliary item\n",
    "    aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,image_i, transpose_b=True)\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,image_j, transpose_b=True)\n",
    "\n",
    "xuij = xui- xuj\n",
    "\n",
    "l2_norm = tf.add_n([\n",
    "            0.001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.001 * tf.reduce_sum(tf.multiply(w1, w1)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.1 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            \n",
    "          ])\n",
    "\n",
    "loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.43658698]]\n",
      "train_auc:------------------- 0.898556077904634\n",
      "time: 595.3776261806488  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.29628184]]\n",
      "train_auc:------------------- 0.8846541302887844\n",
      "time: 1192.8945577144623  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.26666075]]\n",
      "train_auc:------------------- 0.9013096037609134\n",
      "time: 1790.4275691509247  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.23308463]]\n",
      "train_auc:------------------- 0.9183344526527871\n",
      "time: 2388.032740831375  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.22402173]]\n",
      "train_auc:------------------- 0.9225319006044325\n",
      "time: 2985.543319940567  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.22249722]]\n",
      "train_auc:------------------- 0.9270651443922096\n",
      "time: 3583.111857175827  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.20726994]]\n",
      "train_auc:------------------- 0.9351578240429819\n",
      "time: 4180.742301225662  sec\n",
      "Iteraction: 7\n",
      "total_loss:----------------- [[0.20086594]]\n",
      "train_auc:------------------- 0.9391202149093352\n",
      "time: 4778.303627252579  sec\n",
      "Iteraction: 8\n",
      "total_loss:----------------- [[0.19866593]]\n",
      "train_auc:------------------- 0.9402283411685695\n",
      "time: 5375.787429571152  sec\n",
      "Iteraction: 9\n",
      "total_loss:----------------- [[0.19667388]]\n",
      "train_auc:------------------- 0.9411349899261249\n",
      "time: 5973.369047880173  sec\n",
      "Total cost  5973.369047880173  sec\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "t0=time.time()\n",
    "\n",
    "#use_true=init_list_of_objects(136)\n",
    "#use_test=init_list_of_objects(136)\n",
    "\n",
    "train_pair_t=[] #positive feedback\n",
    "train_pair_f=[] #negative feedback\n",
    "train_yes_id=[] \n",
    "for q in range(10):\n",
    "    print('Iteraction:',q)\n",
    "    train_auc=0\n",
    "    total_loss=0\n",
    "    xuij_auc=0\n",
    "    length = 0\n",
    "    for z in range(1489):\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes=[]\n",
    "        yesr=[]\n",
    "        r_3=np.zeros(len(train_t[z])) \n",
    "        \n",
    "        #這裡不知道怎麼讓3變成變動型的長度\n",
    "        sample=random.sample(train_t[z],len(train_t[z])) #隨機選3個sample true's YouTuber\n",
    "        train_yes_id.append(sample) #sample全部丟進去\n",
    "        \n",
    "        for k in range(len(sample)):\n",
    "            yes.append(image_2048[sample[k]])\n",
    "            yesr.append(YouTuber_category[sample[k]]*user_category_norm[z])\n",
    "            #print('YouTuber_category ', YouTuber_category[sample[k]])\n",
    "            #print('User_category ',user_category_norm[z])\n",
    "        for k in range(len(sample)):\n",
    "            r_3[k]=max(yesr[k])\n",
    "        \n",
    "        yes=np.array(yes)\n",
    "        #print('user shape should be ',np.array([z]).shape)\n",
    "        #print('xf shape should be ',yes.shape)\n",
    "        #print('r shape should be ',np.array(r_3).shape)\n",
    "        #print('l_id shape should be ',np.array(sample).shape)\n",
    "        \n",
    "        #not_used_list = list(set(train_t[z]).difference(set(sample)))\n",
    "        \n",
    "        train_t_sample = random.sample(train_t[z],2)\n",
    "        #print('number of positive feedback', len(train_t[z]))\n",
    "        for ta in train_t_sample:\n",
    "            #ta=random.choice(train_t[z]) #ta is true positve photo\n",
    "            train_pair_t.append(ta)\n",
    "            image_1=np.expand_dims(image_2048[ta],0) #(1,2048)\n",
    "            #print('Image_1 shape ',image_1.shape)\n",
    "            train_f_sample = random.sample(train_f[z],10)\n",
    "            for b in train_f_sample:\n",
    "                #print('likes:',ta,';Not likes:',b)\n",
    "                #b=random.choice(train_f[z])  #b is no feedback photo\n",
    "                train_pair_f.append(b)\n",
    "                image_2=np.expand_dims(image_2048[b],0) #(1,2048)\n",
    "                #print('Image_2 shape',image_2.shape)\n",
    "            \n",
    "                #use_test[z].append(b)\n",
    "                r3,_auc, _loss,_=sess.run([a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                #print(XUIJ)\n",
    "                #print('loss=',_loss)\n",
    "                #print('auc=',_auc)\n",
    "                #print('sub r3:',r3)\n",
    "                train_auc+=_auc\n",
    "                total_loss+=_loss\n",
    "                length += 1\n",
    "            #now1+=1\n",
    "    \n",
    "    #print('mine:',xuij_auc/136)    \n",
    "    #print('a_list_soft:',r3)\n",
    "    print(\"total_loss:-----------------\", total_loss/length)\n",
    "    print(\"train_auc:-------------------\", train_auc/length)\n",
    "    print('time:',time.time()-t0,' sec')\n",
    "print('Total cost ',time.time()-t0,' sec')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Y, A, A1, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, W1, Wu, Wy, Wa, Wv,Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent shape:  (1489, 32)\n",
      "photo latent shape:  (88, 32)\n",
      "Auxilary latent shape:  (88, 32)\n",
      "W1 weight shape:  (1489, 32)\n",
      "Wu weight shape: (1489, 32, 32)\n",
      "Wy weight shape: (1489, 32, 32)\n",
      "Wa weight shape: (1489, 32, 32)\n",
      "Wv weight shape: (1489, 32, 2048)\n",
      "Beta shape: (1489, 2048)\n"
     ]
    }
   ],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('W1 weight shape: ',A1.shape)\n",
    "print('Wu weight shape:',Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:',Aa.shape)\n",
    "print('Wv weight shape:',Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 619\n",
      "softmax alpha-------------- [0.11111174 0.11110595 0.11111175 0.11111766 0.11110977 0.11111171\n",
      " 0.11110908 0.11111006 0.11111229]\n",
      "1 175\n",
      "softmax alpha-------------- [0.25000715 0.24999735 0.24999637 0.24999914]\n",
      "2 1076\n",
      "softmax alpha-------------- [0.09999564 0.10000095 0.09999904 0.10000294 0.10000369 0.09999825\n",
      " 0.10000273 0.09999887 0.09999921 0.09999867]\n",
      "3 877\n",
      "softmax alpha-------------- [0.12500227 0.12500127 0.12500473 0.12499921 0.12498982 0.12499717\n",
      " 0.12500285 0.12500268]\n",
      "4 694\n",
      "softmax alpha-------------- [0.11110773 0.11110786 0.11110967 0.11111604 0.11111938 0.11110753\n",
      " 0.11111666 0.11110771 0.11110741]\n",
      "5 239\n",
      "softmax alpha-------------- [0.12500096 0.12499863 0.12500346 0.12498898 0.12499013 0.12500187\n",
      " 0.12500588 0.12501008]\n",
      "6 432\n",
      "softmax alpha-------------- [0.12499704 0.12499984 0.1250024  0.12499848 0.12499954 0.12499947\n",
      " 0.12499949 0.12500374]\n",
      "7 1072\n",
      "softmax alpha-------------- [0.07692188 0.07692309 0.07692399 0.07692354 0.0769236  0.07692132\n",
      " 0.07692231 0.07692379 0.07692235 0.07692352 0.07692362 0.0769234\n",
      " 0.07692358]\n",
      "8 269\n",
      "softmax alpha-------------- [0.07692054 0.07692244 0.07692357 0.07692249 0.07692312 0.07692315\n",
      " 0.07692421 0.07692359 0.07692288 0.07692268 0.07692249 0.07692256\n",
      " 0.07692629]\n",
      "9 470\n",
      "softmax alpha-------------- [0.16666916 0.16666944 0.1666651  0.16666334 0.16666972 0.16666323]\n",
      "10 1412\n",
      "softmax alpha-------------- [0.24999839 0.24999736 0.25000737 0.24999688]\n",
      "11 205\n",
      "softmax alpha-------------- [0.0909084  0.09090989 0.09090788 0.09091    0.09091009 0.09091094\n",
      " 0.09090834 0.09090806 0.09090986 0.09090839 0.09090815]\n",
      "12 1300\n",
      "softmax alpha-------------- [0.10000079 0.09999561 0.09999796 0.10000213 0.10000313 0.10000314\n",
      " 0.10000309 0.09999715 0.09999657 0.10000043]\n",
      "13 1086\n",
      "softmax alpha-------------- [0.07142904 0.07142872 0.07142855 0.07142979 0.07142875 0.07142994\n",
      " 0.07142911 0.07142906 0.07142087 0.07142989 0.07142954 0.07142788\n",
      " 0.07142918 0.07142968]\n",
      "14 158\n",
      "softmax alpha-------------- [0.11111002 0.11110762 0.11110887 0.11111424 0.11110668 0.11111472\n",
      " 0.11111232 0.11111527 0.11111025]\n",
      "15 364\n",
      "softmax alpha-------------- [0.33332564 0.33334057 0.33333379]\n",
      "16 1160\n",
      "softmax alpha-------------- [0.09090781 0.09090793 0.09091398 0.09090868 0.09090873 0.09090814\n",
      " 0.09090794 0.09090864 0.0909121  0.09090777 0.09090829]\n",
      "17 554\n",
      "softmax alpha-------------- [0.1111089  0.11111281 0.11110912 0.11110939 0.11110951 0.11111327\n",
      " 0.11111847 0.11110911 0.11110943]\n",
      "18 42\n",
      "softmax alpha-------------- [0.12499848 0.12500455 0.12499905 0.12499873 0.12499831 0.12500405\n",
      " 0.12499711 0.12499971]\n",
      "19 525\n",
      "softmax alpha-------------- [0.14287322 0.1428531  0.14285556 0.1428555  0.14285297 0.14285435\n",
      " 0.14285529]\n",
      "20 1189\n",
      "softmax alpha-------------- [0.03846186 0.03846121 0.0384626  0.03846231 0.0384621  0.0384609\n",
      " 0.03846079 0.03846221 0.03846136 0.03845913 0.03846318 0.03846004\n",
      " 0.03846234 0.03846037 0.0384626  0.03846131 0.0384623  0.03846165\n",
      " 0.03846229 0.03846042 0.03846259 0.03846209 0.03846148 0.03846066\n",
      " 0.0384615  0.0384607 ]\n",
      "21 923\n",
      "softmax alpha-------------- [0.09090968 0.09090872 0.09090513 0.09090963 0.09090961 0.09090959\n",
      " 0.09091079 0.09090934 0.09090935 0.09090987 0.0909083 ]\n",
      "22 1106\n",
      "softmax alpha-------------- [0.33333909 0.33332715 0.33333376]\n",
      "23 48\n",
      "softmax alpha-------------- [0.33334058 0.33332971 0.33332971]\n",
      "24 1082\n",
      "softmax alpha-------------- [0.09091137 0.09090702 0.09090726 0.09089417 0.09090973 0.09091082\n",
      " 0.09090983 0.09091243 0.09091246 0.09091173 0.09091319]\n",
      "25 37\n",
      "softmax alpha-------------- [0.11111072 0.11111229 0.11111035 0.11111062 0.11111248 0.11111078\n",
      " 0.11111188 0.11111022 0.11111066]\n",
      "26 639\n",
      "softmax alpha-------------- [0.16666734 0.1666676  0.16666506 0.16666747 0.16666515 0.16666738]\n",
      "27 977\n",
      "softmax alpha-------------- [0.33333965 0.33332305 0.3333373 ]\n",
      "28 920\n",
      "softmax alpha-------------- [0.1250014  0.12500141 0.1250018  0.12499653 0.12500072 0.12499736\n",
      " 0.1250004  0.12500038]\n",
      "29 813\n",
      "softmax alpha-------------- [0.12500115 0.12500149 0.12500101 0.12500154 0.12500115 0.12499215\n",
      " 0.12500133 0.12500017]\n",
      "30 149\n",
      "softmax alpha-------------- [0.19999347 0.20000092 0.20000261 0.20000105 0.20000195]\n",
      "31 895\n",
      "softmax alpha-------------- [0.20000323 0.19999471 0.19999637 0.2000016  0.2000041 ]\n",
      "32 1254\n",
      "softmax alpha-------------- [0.20001396 0.19998494 0.20001387 0.19999756 0.19998967]\n",
      "33 885\n",
      "softmax alpha-------------- [0.25000436 0.2499995  0.24999201 0.25000412]\n",
      "34 335\n",
      "softmax alpha-------------- [0.10000477 0.09999935 0.09999779 0.09999744 0.09999754 0.10000027\n",
      " 0.10000124 0.1000012  0.10000084 0.09999956]\n",
      "35 1330\n",
      "softmax alpha-------------- [0.05555502 0.05555888 0.05555796 0.05555351 0.05555351 0.05555628\n",
      " 0.05555834 0.05555809 0.05555355 0.05555511 0.055556   0.0555583\n",
      " 0.05555639 0.05555308 0.05555493 0.05555368 0.05555495 0.05555243]\n",
      "36 935\n",
      "softmax alpha-------------- [0.33333092 0.33333532 0.33333376]\n",
      "37 957\n",
      "softmax alpha-------------- [0.16666609 0.16667207 0.16667116 0.16666295 0.1666641  0.16666363]\n",
      "38 476\n",
      "softmax alpha-------------- [0.09090878 0.09090861 0.09090861 0.09090863 0.09091155 0.09090871\n",
      " 0.09090861 0.09090861 0.09090861 0.09090851 0.09091076]\n",
      "39 659\n",
      "softmax alpha-------------- [0.09999637 0.10000213 0.09999549 0.09999683 0.10000103 0.09999903\n",
      " 0.10000394 0.10000184 0.10000254 0.1000008 ]\n",
      "40 727\n",
      "softmax alpha-------------- [0.12499136 0.12499047 0.12501079 0.12501178 0.12500608 0.12500882\n",
      " 0.12499026 0.12499044]\n",
      "41 850\n",
      "softmax alpha-------------- [0.09090743 0.09090735 0.09090865 0.09091232 0.09090731 0.09090742\n",
      " 0.0909101  0.09090734 0.09090799 0.09091034 0.09091374]\n",
      "42 1314\n",
      "softmax alpha-------------- [0.14285696 0.14285367 0.14285927 0.14285775 0.14285647 0.14285921\n",
      " 0.14285666]\n",
      "43 290\n",
      "softmax alpha-------------- [0.14285477 0.14286025 0.14286501 0.14285373 0.14284553 0.14285823\n",
      " 0.14286248]\n",
      "44 1355\n",
      "softmax alpha-------------- [0.33332948 0.3333314  0.33333912]\n",
      "45 1364\n",
      "softmax alpha-------------- [0.0666682  0.06666827 0.06666612 0.06666654 0.06666496 0.06666564\n",
      " 0.0666657  0.06666774 0.06666819 0.06666819 0.06666596 0.06666833\n",
      " 0.06666566 0.06666461 0.06666592]\n",
      "46 857\n",
      "softmax alpha-------------- [0.20000039 0.19999604 0.19999911 0.19999273 0.20001173]\n",
      "47 8\n",
      "softmax alpha-------------- [0.33333075 0.33333849 0.33333076]\n",
      "48 762\n",
      "softmax alpha-------------- [0.14285421 0.14285895 0.14285797 0.14285514 0.14285883 0.14285605\n",
      " 0.14285887]\n",
      "49 259\n",
      "softmax alpha-------------- [0.1428561  0.14285721 0.1428586  0.14285867 0.14285773 0.14285698\n",
      " 0.14285471]\n",
      "50 828\n",
      "softmax alpha-------------- [0.05882512 0.05882176 0.05882565 0.05882489 0.0588255  0.05882462\n",
      " 0.05882166 0.05882173 0.05882658 0.05882361 0.05882138 0.05882153\n",
      " 0.05882547 0.05882307 0.05882168 0.05882417 0.05882159]\n",
      "51 1128\n",
      "softmax alpha-------------- [0.10000135 0.09999919 0.09999913 0.09999919 0.09999886 0.10000166\n",
      " 0.10000073 0.09999931 0.10000095 0.09999963]\n",
      "52 642\n",
      "softmax alpha-------------- [0.2500001  0.25000444 0.24998998 0.25000548]\n",
      "53 1179\n",
      "softmax alpha-------------- [0.33333342 0.33333316 0.33333342]\n",
      "54 1253\n",
      "softmax alpha-------------- [0.14285691 0.1428619  0.14285783 0.14285864 0.1428455  0.14286158\n",
      " 0.14285763]\n",
      "55 976\n",
      "softmax alpha-------------- [0.16667346 0.16666408 0.16667057 0.16666452 0.16666369 0.16666368]\n",
      "56 1215\n",
      "softmax alpha-------------- [0.33333288 0.3333254  0.33334172]\n",
      "57 1133\n",
      "softmax alpha-------------- [0.11111017 0.11111247 0.11111233 0.11111203 0.11110996 0.11111125\n",
      " 0.11111025 0.11111156 0.11110998]\n",
      "58 188\n",
      "softmax alpha-------------- [0.06250137 0.06249936 0.06250263 0.06249962 0.06249929 0.06250206\n",
      " 0.06249894 0.06249809 0.06250147 0.06249776 0.06249888 0.06250146\n",
      " 0.06249785 0.06249889 0.06250273 0.0624996 ]\n",
      "59 822\n",
      "softmax alpha-------------- [0.19999862 0.20000179 0.19999916 0.20000082 0.19999962]\n",
      "60 545\n",
      "softmax alpha-------------- [0.09090906 0.09091084 0.09091085 0.0909104  0.09090907 0.09091133\n",
      " 0.09091123 0.09090413 0.09091005 0.09090345 0.09090959]\n",
      "61 243\n",
      "softmax alpha-------------- [0.1666644  0.16666816 0.16667045 0.16666514 0.16665727 0.16667458]\n",
      "62 505\n",
      "softmax alpha-------------- [0.16666985 0.16666837 0.1666603  0.16666734 0.16666828 0.16666586]\n",
      "63 408\n",
      "softmax alpha-------------- [0.16666638 0.16666849 0.16666166 0.16666631 0.16666769 0.16666948]\n",
      "64 1334\n",
      "softmax alpha-------------- [0.11111033 0.11111164 0.11111108 0.11110953 0.11111163 0.11111125\n",
      " 0.11111167 0.11111179 0.11111106]\n",
      "65 455\n",
      "softmax alpha-------------- [0.25000166 0.24998874 0.25001711 0.24999249]\n",
      "66 1273\n",
      "softmax alpha-------------- [0.25000082 0.25000153 0.24999206 0.25000559]\n",
      "67 821\n",
      "softmax alpha-------------- [0.11111358 0.11110713 0.11111241 0.11111342 0.11110943 0.11111344\n",
      " 0.11111018 0.11111236 0.11110807]\n",
      "68 563\n",
      "softmax alpha-------------- [0.06666946 0.06666642 0.06666922 0.06666621 0.06666399 0.0666649\n",
      " 0.06667013 0.06666718 0.06666673 0.06666467 0.0666658  0.0666661\n",
      " 0.06666623 0.06666623 0.06666672]\n",
      "69 587\n",
      "softmax alpha-------------- [0.09090937 0.09090939 0.09090927 0.09091014 0.09090958 0.09090856\n",
      " 0.09091044 0.09090609 0.09090744 0.09091075 0.09090896]\n",
      "70 633\n",
      "softmax alpha-------------- [0.08332908 0.0833365  0.08333769 0.0833311  0.08333229 0.08333317\n",
      " 0.08332957 0.08333691 0.08333017 0.08332997 0.08333679 0.08333677]\n",
      "71 1237\n",
      "softmax alpha-------------- [0.08333547 0.08333236 0.08333326 0.08333256 0.08333503 0.083335\n",
      " 0.08333157 0.08333168 0.08333359 0.08333292 0.08333428 0.08333229]\n",
      "72 434\n",
      "softmax alpha-------------- [0.11111245 0.11111101 0.11110947 0.11111361 0.1111096  0.11111123\n",
      " 0.11110998 0.11111132 0.11111133]\n",
      "73 646\n",
      "softmax alpha-------------- [0.16666675 0.16666579 0.16666756 0.16666967 0.16666751 0.16666272]\n",
      "74 1166\n",
      "softmax alpha-------------- [0.14286051 0.1428574  0.14285655 0.1428551  0.14285682 0.1428558\n",
      " 0.14285782]\n",
      "75 148\n",
      "softmax alpha-------------- [0.09090767 0.0909115  0.09090675 0.09090687 0.09091134 0.09090687\n",
      " 0.09091188 0.09091185 0.09090728 0.09091164 0.09090636]\n",
      "76 880\n",
      "softmax alpha-------------- [0.11111055 0.11110876 0.11111336 0.11111117 0.11111063 0.11110863\n",
      " 0.11111198 0.11111237 0.11111257]\n",
      "77 1124\n",
      "softmax alpha-------------- [0.12499999 0.12500426 0.12500188 0.12500162 0.12500003 0.1249989\n",
      " 0.1249968  0.12499651]\n",
      "78 768\n",
      "softmax alpha-------------- [0.06666969 0.06666542 0.06666542 0.06667007 0.06666553 0.06666562\n",
      " 0.06666724 0.0666667  0.06666641 0.06666672 0.06666573 0.0666668\n",
      " 0.0666656  0.06666645 0.06666659]\n",
      "79 217\n",
      "softmax alpha-------------- [0.20000559 0.19999326 0.19999733 0.19999865 0.20000517]\n",
      "80 529\n",
      "softmax alpha-------------- [0.14285952 0.14285651 0.14285847 0.14285704 0.14285444 0.14285695\n",
      " 0.14285707]\n",
      "81 237\n",
      "softmax alpha-------------- [0.04999925 0.05000027 0.05000105 0.05000091 0.05000097 0.04999716\n",
      " 0.0499992  0.05000116 0.05000092 0.05000117 0.04999873 0.05000117\n",
      " 0.04999842 0.04999816 0.05000102 0.04999995 0.05000092 0.05000021\n",
      " 0.04999901 0.05000036]\n",
      "82 861\n",
      "softmax alpha-------------- [0.24999558 0.25000665 0.24999967 0.2499981 ]\n",
      "83 332\n",
      "softmax alpha-------------- [0.08333408 0.08333308 0.08333229 0.0833335  0.08333484 0.08333369\n",
      " 0.08333051 0.08333364 0.08333482 0.08333257 0.08333364 0.08333333]\n",
      "84 1212\n",
      "softmax alpha-------------- [0.05262945 0.05263228 0.05263194 0.05263209 0.0526314  0.05263237\n",
      " 0.052632   0.05263081 0.05263243 0.05263188 0.05263238 0.05263044\n",
      " 0.05263211 0.05263101 0.05262975 0.05263195 0.05263231 0.05263154\n",
      " 0.05263185]\n",
      "85 1323\n",
      "softmax alpha-------------- [0.09090493 0.09091331 0.09090511 0.09091184 0.09090588 0.09090516\n",
      " 0.09091329 0.09091216 0.09091124 0.09090577 0.09091131]\n",
      "86 1016\n",
      "softmax alpha-------------- [0.24999897 0.24999875 0.24999643 0.25000586]\n",
      "87 1229\n",
      "softmax alpha-------------- [0.33333204 0.33332776 0.3333402 ]\n",
      "88 1009\n",
      "softmax alpha-------------- [0.11111058 0.1111109  0.1111107  0.11111181 0.1111054  0.11110947\n",
      " 0.11111351 0.11111051 0.11111712]\n",
      "89 842\n",
      "softmax alpha-------------- [0.33332747 0.33333863 0.3333339 ]\n",
      "90 1224\n",
      "softmax alpha-------------- [0.16666663 0.16666712 0.16666593 0.16666679 0.16666858 0.16666495]\n",
      "91 1230\n",
      "softmax alpha-------------- [0.04166659 0.04166683 0.04166695 0.04166669 0.04166704 0.04166666\n",
      " 0.04166689 0.04166726 0.04166653 0.04166661 0.04166652 0.04166688\n",
      " 0.04166627 0.04166684 0.04166724 0.04166658 0.04166658 0.04166654\n",
      " 0.04166653 0.04166666 0.04166526 0.0416673  0.04166604 0.04166672]\n",
      "92 354\n",
      "softmax alpha-------------- [0.20000058 0.19999994 0.20000004 0.19999947 0.19999998]\n",
      "93 116\n",
      "softmax alpha-------------- [0.09090862 0.0909129  0.0909061  0.09090832 0.09091022 0.09091191\n",
      " 0.09090622 0.09090983 0.09090709 0.09090651 0.09091226]\n",
      "94 990\n",
      "softmax alpha-------------- [0.08333363 0.08333552 0.08333533 0.08333354 0.08333017 0.08333414\n",
      " 0.08332878 0.0833344  0.08333118 0.08333274 0.0833359  0.08333467]\n",
      "95 736\n",
      "softmax alpha-------------- [0.24999251 0.25000251 0.25000265 0.25000232]\n",
      "96 951\n",
      "softmax alpha-------------- [0.33331923 0.3333424  0.33333837]\n",
      "97 1097\n",
      "softmax alpha-------------- [0.24998835 0.24999209 0.25001054 0.25000902]\n",
      "98 1196\n",
      "softmax alpha-------------- [0.09090905 0.09090926 0.09091097 0.09090933 0.09090914 0.09090877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.09090894 0.09090928 0.09090787 0.09090983 0.09090757]\n",
      "99 1468\n",
      "softmax alpha-------------- [0.16666319 0.16666594 0.16666926 0.16666297 0.16667045 0.16666819]\n",
      "100 1110\n",
      "softmax alpha-------------- [0.19999581 0.19999916 0.20000756 0.20000246 0.19999502]\n",
      "101 689\n",
      "softmax alpha-------------- [0.09091136 0.09090699 0.09090686 0.09091259 0.09091094 0.09091005\n",
      " 0.09091004 0.09091062 0.09090683 0.09090684 0.09090688]\n",
      "102 769\n",
      "softmax alpha-------------- [0.16666355 0.1666799  0.16666349 0.16666404 0.16666444 0.16666458]\n",
      "103 601\n",
      "softmax alpha-------------- [0.11111148 0.1111127  0.11111152 0.11111112 0.11111214 0.11111042\n",
      " 0.11110821 0.1111114  0.11111099]\n",
      "104 799\n",
      "softmax alpha-------------- [0.14285073 0.14285998 0.14285942 0.14286164 0.1428538  0.14285069\n",
      " 0.14286375]\n",
      "105 440\n",
      "softmax alpha-------------- [0.08333501 0.08333137 0.08333256 0.08333217 0.08333383 0.08333357\n",
      " 0.08333313 0.08333272 0.08333428 0.08333313 0.08333535 0.08333288]\n",
      "106 628\n",
      "softmax alpha-------------- [0.09999899 0.09999724 0.09999896 0.09999883 0.09999954 0.10000443\n",
      " 0.10000103 0.1000003  0.09999801 0.10000267]\n",
      "107 680\n",
      "softmax alpha-------------- [0.19999778 0.19999847 0.19999884 0.20000675 0.19999816]\n",
      "108 1019\n",
      "softmax alpha-------------- [0.14285454 0.1428589  0.14286207 0.14285714 0.14285566 0.14285444\n",
      " 0.14285725]\n",
      "109 11\n",
      "softmax alpha-------------- [0.09999913 0.10000135 0.09999932 0.09999836 0.1000028  0.09999838\n",
      " 0.10000083 0.10000055 0.1000003  0.09999897]\n",
      "110 161\n",
      "softmax alpha-------------- [0.20000324 0.19999893 0.20000081 0.19999789 0.19999913]\n",
      "111 629\n",
      "softmax alpha-------------- [0.24999871 0.25000288 0.24999797 0.25000044]\n",
      "112 1068\n",
      "softmax alpha-------------- [0.10000038 0.09999863 0.1000001  0.1000004  0.09999929 0.10000344\n",
      " 0.10000017 0.09999805 0.09999839 0.10000113]\n",
      "113 283\n",
      "softmax alpha-------------- [0.11110593 0.1111151  0.11111424 0.11110663 0.11110188 0.11110564\n",
      " 0.11111882 0.1111193  0.11111244]\n",
      "114 1094\n",
      "softmax alpha-------------- [0.11110921 0.11111702 0.11110674 0.11111091 0.11110898 0.11110919\n",
      " 0.11111434 0.11110929 0.11111432]\n",
      "115 391\n",
      "softmax alpha-------------- [0.11111096 0.11111131 0.11111134 0.11111198 0.11111244 0.11111107\n",
      " 0.11111093 0.11110924 0.11111073]\n",
      "116 840\n",
      "softmax alpha-------------- [0.11111248 0.11111167 0.11110941 0.11111141 0.11111083 0.11111084\n",
      " 0.11111    0.11111056 0.1111128 ]\n",
      "117 1377\n",
      "softmax alpha-------------- [0.11111117 0.11111023 0.11111149 0.11111025 0.11111117 0.11111207\n",
      " 0.11111111 0.11111229 0.11111022]\n",
      "118 672\n",
      "softmax alpha-------------- [0.33335337 0.33334078 0.33330585]\n",
      "119 535\n",
      "softmax alpha-------------- [0.06250013 0.06250175 0.06249986 0.06249748 0.06250108 0.06250145\n",
      " 0.06249748 0.06250053 0.06250036 0.0625     0.06250163 0.06250009\n",
      " 0.0624992  0.06250039 0.06249956 0.06249902]\n",
      "120 1322\n",
      "softmax alpha-------------- [0.10000015 0.09999963 0.09999888 0.10000017 0.09999942 0.09999961\n",
      " 0.09999957 0.10000285 0.09999894 0.1000008 ]\n",
      "121 221\n",
      "softmax alpha-------------- [0.24999298 0.24999158 0.25000474 0.2500107 ]\n",
      "122 873\n",
      "softmax alpha-------------- [0.12500332 0.12499979 0.12499986 0.12499825 0.12500072 0.12499804\n",
      " 0.1250028  0.12499722]\n",
      "123 995\n",
      "softmax alpha-------------- [0.14285624 0.14285792 0.14285776 0.14285662 0.14285788 0.14285569\n",
      " 0.14285789]\n",
      "124 593\n",
      "softmax alpha-------------- [0.09999874 0.1000052  0.09999935 0.10000382 0.09999841 0.09999751\n",
      " 0.09999814 0.10000041 0.09999862 0.09999981]\n",
      "125 1240\n",
      "softmax alpha-------------- [0.14286018 0.14285291 0.14285676 0.1428549  0.14285984 0.1428598\n",
      " 0.14285561]\n",
      "126 847\n",
      "softmax alpha-------------- [0.11111123 0.11111144 0.11111025 0.11111211 0.1111129  0.11110867\n",
      " 0.11111189 0.11111179 0.11110974]\n",
      "127 164\n",
      "softmax alpha-------------- [0.20000044 0.20000051 0.2000005  0.20000069 0.19999785]\n",
      "128 522\n",
      "softmax alpha-------------- [0.14285817 0.1428581  0.14285629 0.14286004 0.1428573  0.14285375\n",
      " 0.14285635]\n",
      "129 1420\n",
      "softmax alpha-------------- [0.1428578  0.14285554 0.14286386 0.14285604 0.14285467 0.14285555\n",
      " 0.14285654]\n",
      "130 991\n",
      "softmax alpha-------------- [0.06250148 0.06249918 0.06249988 0.06249836 0.0624998  0.06249851\n",
      " 0.06249945 0.06250487 0.06249941 0.06249888 0.0625009  0.06250012\n",
      " 0.0625009  0.06249972 0.06249913 0.06249941]\n",
      "131 2\n",
      "softmax alpha-------------- [0.14285551 0.14285662 0.14285459 0.14286077 0.14285648 0.14285928\n",
      " 0.14285675]\n",
      "132 1036\n",
      "softmax alpha-------------- [0.12500334 0.12500114 0.12499996 0.12500034 0.12499814 0.12500273\n",
      " 0.12499762 0.12499675]\n",
      "133 636\n",
      "softmax alpha-------------- [0.14285154 0.14285927 0.14285783 0.14285243 0.14285273 0.14286543\n",
      " 0.14286077]\n",
      "134 465\n",
      "softmax alpha-------------- [0.16666463 0.16666584 0.16666655 0.16666763 0.16666368 0.16667167]\n",
      "135 484\n",
      "softmax alpha-------------- [0.05555531 0.05555553 0.05555482 0.05555509 0.0555549  0.05555649\n",
      " 0.05555575 0.05555575 0.05555541 0.05555627 0.05555507 0.05555743\n",
      " 0.05555521 0.05555526 0.05555528 0.05555541 0.05555579 0.05555521]\n",
      "136 165\n",
      "softmax alpha-------------- [0.33333286 0.33333094 0.3333362 ]\n",
      "137 1302\n",
      "softmax alpha-------------- [0.16666597 0.16666953 0.16666976 0.16666331 0.16666229 0.16666914]\n",
      "138 1168\n",
      "softmax alpha-------------- [0.1428575  0.14285213 0.14285623 0.14286245 0.1428569  0.14285776\n",
      " 0.14285702]\n",
      "139 887\n",
      "softmax alpha-------------- [0.12500611 0.12499694 0.12499589 0.12499703 0.12500103 0.12500709\n",
      " 0.12499673 0.12499919]\n",
      "140 225\n",
      "softmax alpha-------------- [0.12500025 0.12499957 0.12500022 0.12499962 0.12499982 0.12500001\n",
      " 0.12500029 0.12500022]\n",
      "141 853\n",
      "softmax alpha-------------- [0.20000088 0.20000158 0.20000115 0.19999798 0.19999841]\n",
      "142 431\n",
      "softmax alpha-------------- [0.12500097 0.12500093 0.12500027 0.12500209 0.12500236 0.12500069\n",
      " 0.12499612 0.12499658]\n",
      "143 1487\n",
      "softmax alpha-------------- [0.24999188 0.25000071 0.25000075 0.25000666]\n",
      "144 1162\n",
      "softmax alpha-------------- [0.08333275 0.08333268 0.08333654 0.08333257 0.08333284 0.08333278\n",
      " 0.08333078 0.08333353 0.08333584 0.08333285 0.08333259 0.08333426]\n",
      "145 1232\n",
      "softmax alpha-------------- [0.24999829 0.25000135 0.25000431 0.24999604]\n",
      "146 178\n",
      "softmax alpha-------------- [0.05882133 0.05881921 0.0588261  0.05882497 0.05882544 0.05882334\n",
      " 0.05882601 0.0588254  0.05882304 0.05882616 0.05882487 0.05882483\n",
      " 0.05882379 0.05881927 0.05881999 0.05882307 0.0588232 ]\n",
      "147 355\n",
      "softmax alpha-------------- [0.24999607 0.24999037 0.25000622 0.25000734]\n",
      "148 632\n",
      "softmax alpha-------------- [0.20000399 0.199992   0.2000011  0.20000575 0.19999716]\n",
      "149 1345\n",
      "softmax alpha-------------- [0.08333334 0.08333333 0.08333334 0.08333108 0.08333231 0.08333424\n",
      " 0.08333343 0.08333341 0.0833332  0.08333489 0.08333362 0.08333381]\n",
      "150 817\n",
      "softmax alpha-------------- [0.11110931 0.11111243 0.11111263 0.11111044 0.11111064 0.11111029\n",
      " 0.11111065 0.11111231 0.11111131]\n",
      "151 1296\n",
      "softmax alpha-------------- [0.0714304  0.07142882 0.07143061 0.07142628 0.07142617 0.07142658\n",
      " 0.07142753 0.07143043 0.0714303  0.07143056 0.07142968 0.07143063\n",
      " 0.07142716 0.07142485]\n",
      "152 428\n",
      "softmax alpha-------------- [0.16667657 0.16667124 0.16666615 0.16666332 0.16665744 0.16666528]\n",
      "153 238\n",
      "softmax alpha-------------- [0.11111345 0.1111134  0.11111088 0.11110777 0.11111077 0.11110983\n",
      " 0.11111558 0.11111055 0.11110777]\n",
      "154 1202\n",
      "softmax alpha-------------- [0.1428581  0.1428542  0.14285687 0.14285996 0.14285569 0.14285469\n",
      " 0.14286048]\n",
      "155 758\n",
      "softmax alpha-------------- [0.12499977 0.12500036 0.12500235 0.12500098 0.12499864 0.12500278\n",
      " 0.12499752 0.12499761]\n",
      "156 980\n",
      "softmax alpha-------------- [0.14285662 0.14285826 0.14285563 0.14285795 0.14285804 0.14285804\n",
      " 0.14285545]\n",
      "157 1239\n",
      "softmax alpha-------------- [0.05882184 0.05882538 0.05882279 0.05882359 0.05882297 0.05882492\n",
      " 0.05882352 0.05882357 0.05882131 0.0588254  0.05882302 0.05882363\n",
      " 0.05882352 0.05882444 0.05882182 0.05882258 0.05882569]\n",
      "158 270\n",
      "softmax alpha-------------- [0.25000275 0.25000357 0.24999316 0.25000052]\n",
      "159 937\n",
      "softmax alpha-------------- [0.04545379 0.0454541  0.04545379 0.04545488 0.04545379 0.04545563\n",
      " 0.04545449 0.04545508 0.04545534 0.04545441 0.04545401 0.04545433\n",
      " 0.04545498 0.04545415 0.04545561 0.04545393 0.04545507 0.04545379\n",
      " 0.04545418 0.04545677 0.04545379 0.04545408]\n",
      "160 214\n",
      "softmax alpha-------------- [0.25000733 0.25000716 0.24998939 0.24999612]\n",
      "161 1205\n",
      "softmax alpha-------------- [0.1999929  0.20000876 0.19999384 0.20001248 0.19999202]\n",
      "162 1464\n",
      "softmax alpha-------------- [0.14285757 0.14285282 0.14285747 0.14285754 0.1428637  0.14285555\n",
      " 0.14285535]\n",
      "163 7\n",
      "softmax alpha-------------- [0.09091016 0.09090929 0.09090779 0.09090923 0.0909106  0.09090938\n",
      " 0.09090973 0.09090924 0.09090769 0.09090774 0.09090915]\n",
      "164 32\n",
      "softmax alpha-------------- [0.12499682 0.1250043  0.12500161 0.12500014 0.1249966  0.12500172\n",
      " 0.12499675 0.12500207]\n",
      "165 121\n",
      "softmax alpha-------------- [0.11111184 0.11111211 0.11110845 0.11111198 0.11111202 0.11111067\n",
      " 0.11110989 0.11111118 0.11111186]\n",
      "166 744\n",
      "softmax alpha-------------- [0.07692231 0.07692509 0.07692136 0.07692788 0.07692427 0.0769247\n",
      " 0.07692122 0.07692654 0.07692151 0.07692112 0.07692116 0.07692113\n",
      " 0.07692171]\n",
      "167 994\n",
      "softmax alpha-------------- [0.19999802 0.19999852 0.20000076 0.20000164 0.20000105]\n",
      "168 1066\n",
      "softmax alpha-------------- [0.16667093 0.16666198 0.16666465 0.16666281 0.16666972 0.16666991]\n",
      "169 361\n",
      "softmax alpha-------------- [0.19999095 0.20000304 0.20000494 0.19999442 0.20000664]\n",
      "170 1069\n",
      "softmax alpha-------------- [0.33333853 0.33332362 0.33333785]\n",
      "171 1459\n",
      "softmax alpha-------------- [0.11111122 0.11111019 0.11110562 0.11111272 0.11111654 0.11111207\n",
      " 0.11111245 0.1111094  0.11110979]\n",
      "172 308\n",
      "softmax alpha-------------- [0.09090792 0.09091407 0.09090879 0.0909091  0.09090928 0.09090489\n",
      " 0.09091086 0.09090767 0.09091012 0.09090867 0.09090863]\n",
      "173 520\n",
      "softmax alpha-------------- [0.09999987 0.10000151 0.0999994  0.09999934 0.10000115 0.100001\n",
      " 0.1000001  0.09999928 0.09999887 0.09999948]\n",
      "174 876\n",
      "softmax alpha-------------- [0.09090498 0.09091663 0.09091341 0.09091242 0.09090974 0.09090593\n",
      " 0.09090474 0.09090473 0.0909108  0.09090488 0.09091175]\n",
      "175 932\n",
      "softmax alpha-------------- [0.12500039 0.12500054 0.12499888 0.12500187 0.12500097 0.12500015\n",
      " 0.12499817 0.12499903]\n",
      "176 1156\n",
      "softmax alpha-------------- [0.08333756 0.08333748 0.08333296 0.08333308 0.08333445 0.08334096\n",
      " 0.08332944 0.08332807 0.08333249 0.08332981 0.08333225 0.08333145]\n",
      "177 662\n",
      "softmax alpha-------------- [0.33333081 0.33333885 0.33333034]\n",
      "178 984\n",
      "softmax alpha-------------- [0.20000673 0.2000007  0.20000443 0.19999338 0.19999476]\n",
      "179 1216\n",
      "softmax alpha-------------- [0.11110854 0.11112355 0.11110855 0.11111157 0.11110964 0.11111139\n",
      " 0.11110324 0.11111467 0.11110885]\n",
      "180 1436\n",
      "softmax alpha-------------- [0.16667119 0.16666838 0.16666777 0.16666245 0.16666608 0.16666413]\n",
      "181 43\n",
      "softmax alpha-------------- [0.12500215 0.12499606 0.12499785 0.12500242 0.12500105 0.12500187\n",
      " 0.12499989 0.1249987 ]\n",
      "182 210\n",
      "softmax alpha-------------- [0.12501308 0.12499853 0.12499762 0.1249976  0.12499874 0.12499905\n",
      " 0.12499768 0.12499769]\n",
      "183 378\n",
      "softmax alpha-------------- [0.08333377 0.08333432 0.08333428 0.08333456 0.08333184 0.08333077\n",
      " 0.0833342  0.08333355 0.08333184 0.08333399 0.08333297 0.08333391]\n",
      "184 655\n",
      "softmax alpha-------------- [0.12500075 0.12500021 0.12499854 0.12500053 0.12499388 0.12500352\n",
      " 0.12500137 0.1250012 ]\n",
      "185 326\n",
      "softmax alpha-------------- [0.2499922  0.24998868 0.24998882 0.2500303 ]\n",
      "186 1357\n",
      "softmax alpha-------------- [0.14285394 0.14287171 0.14285041 0.1428539  0.14285335 0.14285401\n",
      " 0.14286267]\n",
      "187 704\n",
      "softmax alpha-------------- [0.06666738 0.06666704 0.06666506 0.06666665 0.0666677  0.0666676\n",
      " 0.06666697 0.06666643 0.0666668  0.06666641 0.06666578 0.06666624\n",
      " 0.06666562 0.06666699 0.06666733]\n",
      "188 574\n",
      "softmax alpha-------------- [0.04999999 0.05000043 0.05000096 0.04999938 0.05000004 0.05000047\n",
      " 0.04999888 0.04999862 0.05000034 0.05000046 0.05000007 0.05000018\n",
      " 0.04999862 0.05000128 0.05000074 0.04999925 0.05000028 0.05000021\n",
      " 0.05000042 0.04999937]\n",
      "189 1462\n",
      "softmax alpha-------------- [0.33333338 0.33333198 0.33333464]\n",
      "190 157\n",
      "softmax alpha-------------- [0.20000144 0.19999956 0.19999969 0.19999889 0.20000042]\n",
      "191 461\n",
      "softmax alpha-------------- [0.14285782 0.1428565  0.14285871 0.14285787 0.1428568  0.14285404\n",
      " 0.14285825]\n",
      "192 962\n",
      "softmax alpha-------------- [0.14285541 0.14285983 0.14285929 0.14285728 0.14285599 0.14285605\n",
      " 0.14285615]\n",
      "193 287\n",
      "softmax alpha-------------- [0.1666661  0.16666468 0.16666267 0.16667477 0.16666455 0.16666722]\n",
      "194 227\n",
      "softmax alpha-------------- [0.07692241 0.07692167 0.07692261 0.07692367 0.07692224 0.07692357\n",
      " 0.07692332 0.07692403 0.07692217 0.07692496 0.07692408 0.07692302\n",
      " 0.07692225]\n",
      "195 1150\n",
      "softmax alpha-------------- [0.083335   0.08333844 0.08333042 0.08333014 0.08333684 0.0833303\n",
      " 0.08332932 0.08332733 0.083335   0.08333452 0.08333578 0.08333692]\n",
      "196 1137\n",
      "softmax alpha-------------- [0.0500004  0.05000142 0.05000014 0.04999884 0.05000138 0.04999724\n",
      " 0.05000065 0.05000309 0.04999641 0.05000128 0.05000133 0.05000097\n",
      " 0.05000005 0.05000036 0.04999661 0.04999969 0.05000165 0.04999764\n",
      " 0.05000118 0.04999968]\n",
      "197 1406\n",
      "softmax alpha-------------- [0.16666241 0.16666199 0.16666344 0.16667048 0.16666271 0.16667897]\n",
      "198 30\n",
      "softmax alpha-------------- [0.08333522 0.08333194 0.08333466 0.08333501 0.08333413 0.08333191\n",
      " 0.0833315  0.08333386 0.08333253 0.08333301 0.08333234 0.08333389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 494\n",
      "softmax alpha-------------- [0.12500535 0.1250055  0.12500527 0.12499555 0.12499539 0.12499552\n",
      " 0.12500504 0.12499238]\n",
      "[2.87535014 1.75057218 2.69432993 4.85072411 3.12099056 3.13097162\n",
      " 1.63191052 4.01333912 5.56458614 0.765456   3.67157739 3.97191017\n",
      " 3.24378358 2.4253219  6.23240707 4.58837329 1.3892063  4.31815865\n",
      " 3.30904709 4.94784735 1.6276781  3.72509614 2.14981532 0.70450129\n",
      " 6.58090356 4.70395902 4.21040021 4.29922449 5.50120153 5.95975051\n",
      " 5.23532166 1.70656795 3.21524818 2.21622504 0.08978345 5.81256104\n",
      " 3.80021296 2.53248812 2.58190951 2.44506777 4.60414938 6.62539436\n",
      " 3.03918864 7.5274398  1.15056083 4.87520306 2.92885649 2.59561959\n",
      " 2.88269401 5.081179   3.81385504 3.75424073 2.44872233 2.706339\n",
      " 4.5526099  2.64593648 4.09560541 6.6507663  2.63925389 2.09603226\n",
      " 3.83591531 3.70843931 3.17565661 4.7821282  1.55462828 0.89119548\n",
      " 4.31827927 8.13430222 4.20360525 2.7739807  6.03290588 6.44507164\n",
      " 1.58130232 2.43416964 4.64143474 4.62135675 4.43603132 1.61329926\n",
      " 6.06245113 7.26954709 1.64828219 3.8003809  4.99546709 2.74421828\n",
      " 2.70737188 4.0019007  2.50760557 4.08652323]\n"
     ]
    }
   ],
   "source": [
    "result=np.zeros((200,88))\n",
    "RS=np.zeros((200,88))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id=[]\n",
    "for s in range(200):\n",
    "    print(s,test_idx[s])\n",
    "\n",
    "    yes=[]\n",
    "    sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha=np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "    \n",
    "        alpha[a]=np.dot(A1[test_idx[s]],(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[test_idx[s]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                            np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(image_2048[sample[a]],0).T))))*r\n",
    "    mul=np.zeros((1,32))\n",
    "    #print('alpha------------',alpha)\n",
    "    print('softmax alpha--------------',softmax(alpha))\n",
    "    for i in range(len(sample)):\n",
    "        mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    for k in range(88):\n",
    "        result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]], image_2048[k].T)\n",
    "print(RS[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取出test的資料\n",
    "testRS = np.zeros((200,10)) #shape 200*10\n",
    "\n",
    "#test_t 是true的\n",
    "#test_f 是false的\n",
    "        \n",
    "for z in range(200):\n",
    "    user_id = test_idx[z]\n",
    "    #positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    #not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "    #前兩個放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_accuarcy for count_0: 0.32\n"
     ]
    }
   ],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),2)\n",
    "    count_0_all.append(top_0)\n",
    "    #print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = len(count_0_all)*len(count_0_all[0])\n",
    "#print(total) #(200*2)\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        if count_0_all[i][j] < 2: #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "#print(acc_0)\n",
    "avg_acc = acc_0/total\n",
    "print('avg_accuarcy for count_0:',avg_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
