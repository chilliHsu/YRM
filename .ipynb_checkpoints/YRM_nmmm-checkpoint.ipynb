{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data positive feedback dynamic (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ntu\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\ntu\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\ntu\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\ntu\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\ntu\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\ntu\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\ntu\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\pydot_ng\\__init__.py:30: UserWarning: Couldn't import _dotparser, loading of dot files will not be possible.\n",
      "  \"Couldn't import _dotparser, \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_following = np.load('../Data/npy/user_following_1489.npy')[:100]\n",
    "all_3374 = np.load('../Data/npy/all_3374D.npy')\n",
    "user_category = np.load('../Data/npy/user_category_1489.npy')[:100]\n",
    "YouTuber_category = np.load('../Data/npy/YouTuber_category_0.7.npy')\n",
    "active_users = np.load('../Data/npy/active_userID_1489.npy')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  0  2 24  1  0  0 11 47  7 25 32 10  1 16  1  6 24  3  2  1  6  4  0\n",
      " 39 39 10 16 27 22  0  2 17 15  0  2  0  8  2  1 28 16  3 33  9 24  0  3\n",
      "  0  9  0  0  0  2 28  2  2 18  0  3 23  0  2 39  0  1  1 65  0  2 18 21\n",
      "  7  1 47 14 19  1  3 66  3  0 28  3 20  0  6  0]\n",
      "The num of followers over 5: 42\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each YouTuber\n",
    "YouTuber_followers = np.sum(user_following, axis=0)\n",
    "print(YouTuber_followers)\n",
    "over5 = 0\n",
    "for num in YouTuber_followers:\n",
    "    if num >= 5:\n",
    "        over5+=1\n",
    "print('The num of followers over 5:',over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_following shape  (100, 88)\n",
      "all_3374 shape  (88, 3374)\n",
      "user_category shape  (100, 17)\n",
      "YouTuber_category shape  (88, 17)\n"
     ]
    }
   ],
   "source": [
    "print('user_following shape ',user_following.shape)\n",
    "print('all_3374 shape ',all_3374.shape)\n",
    "print('user_category shape ',user_category.shape)\n",
    "print('YouTuber_category shape ',YouTuber_category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_category after normalized by max...\n",
      "user_category_norm shape  (100, 17)\n"
     ]
    }
   ],
   "source": [
    "user_category_norm = np.zeros(user_category.shape)\n",
    "for i in range(len(user_category)):\n",
    "    user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "print('user_category after normalized by max...')\n",
    "print('user_category_norm shape ',user_category_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "following_true = [0]*len(user_following)\n",
    "for i in range(len(user_following)):\n",
    "    each_user = []\n",
    "    for j in range(len(user_following[i])):\n",
    "        if user_following[i][j] == 1:\n",
    "            each_user.append(j)\n",
    "    following_true[i] = each_user\n",
    "#print(following_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings  5\n",
      "Max number of followings  21\n"
     ]
    }
   ],
   "source": [
    "#最少跟最多的following \n",
    "minlen = 10000\n",
    "maxlen = 0\n",
    "num_of_follower = []\n",
    "for i in range(len(following_true)):\n",
    "    if len(following_true[i]) < minlen:\n",
    "        minlen = len(following_true[i])\n",
    "    if len(following_true[i]) > maxlen:\n",
    "        maxlen = len(following_true[i])\n",
    "    num_of_follower.append(len(following_true[i]))\n",
    "print('Min number of followings ',minlen)\n",
    "print('Max number of followings ',maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 8: 68\n",
      "over 10: 56\n",
      "over 12: 28\n",
      "avg 10: 12.517857142857142\n"
     ]
    }
   ],
   "source": [
    "over_10 = 0\n",
    "over_8 = 0\n",
    "over_12 = 0\n",
    "user_idx_over10 = []\n",
    "avg_over10 = 0\n",
    "for i in range(len(num_of_follower)):\n",
    "    num = num_of_follower[i]\n",
    "    if num >= 10:\n",
    "        over_10 += 1\n",
    "        user_idx_over10.append(i)\n",
    "        avg_over10+=num\n",
    "    if num >= 8:\n",
    "        over_8 += 1\n",
    "    if num >= 12:\n",
    "        over_12 += 1\n",
    "print('over 8:',over_8)\n",
    "print('over 10:',over_10)\n",
    "print('over 12:',over_12)\n",
    "print('avg 10:',avg_over10/len(user_idx_over10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amount = 10\n",
    "yt_test_amount = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = [i for i in range(len(user_following))]\n",
    "#user_idx = user_idx_over10\n",
    "#test_idx is the number of user for testing\n",
    "random.seed(3)\n",
    "test_idx = random.sample(user_idx,test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n",
      "2\n",
      "6 3\n",
      "3\n",
      "14 7\n",
      "7\n",
      "11 6\n",
      "5\n",
      "5 3\n",
      "2\n",
      "6 3\n",
      "3\n",
      "7 4\n",
      "3\n",
      "11 6\n",
      "5\n",
      "10 5\n",
      "5\n",
      "9 5\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Training  and Testing --New\n",
    "train_t = [0]*(len(user_following))\n",
    "train_f = [0]*(len(user_following))\n",
    "# Testing \n",
    "test_t = [0]*test_amount\n",
    "test_f = [0]*test_amount\n",
    "test_pos = -1\n",
    "\n",
    "for i in range(len(user_following)):\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train\n",
    "        \n",
    "    else: #if in test id, choose 2 true and other \n",
    "        test_pos += 1\n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        for j in range(88):\n",
    "            if user_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        print(len(temp_t),math.ceil(0.5*len(temp_t)))\n",
    "        t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t[test_pos] = t_for_test\n",
    "        test_f[test_pos] = f_for_test\n",
    "        \n",
    "        #other for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        print(len(t_for_train ))\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test/test_amount\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 100\n",
      "The length of train_f: 100\n",
      "The length of test_t: 10\n",
      "The length of test_f: 10\n"
     ]
    }
   ],
   "source": [
    "# train_t[i] 代表的是user i positive feedback\n",
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_t))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation  Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(user_following)\n",
    "m = 88  \n",
    "k = 128\n",
    "l = 3374\n",
    "#embedding_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "\n",
    "    #use_true=init_list_of_objects(136)\n",
    "    #use_test=init_list_of_objects(136)\n",
    "\n",
    "    #train_pair_t=[] #positive feedback\n",
    "    #train_pair_f=[] #negative feedback\n",
    "    train_yes_id=[] \n",
    "    for q in range(5):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "        \n",
    "            sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            train_yes_id.append(sample) #sample全部丟進去\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #change\n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "                #print('YouTuber_category ', YouTuber_category[sample[k]])\n",
    "                #print('User_category ',user_category_norm[z])\n",
    "            #print(yesr)\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #not_used_list = list(set(train_t[z]).difference(set(sample)))\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                #print(ta,'--> positive feedback')\n",
    "            \n",
    "                pos = sample.index(ta)\n",
    "                #new_sample = np.delete(sample,[pos])\n",
    "                #new_yes = np.delete(yes,[pos],axis=0)\n",
    "                #new_r_3 = np.delete(r_3,[pos])\n",
    "                new_sample = sample\n",
    "                new_yes = yes\n",
    "                new_r_3 = r_3\n",
    "                #print(len(yes),len(new_yes))\n",
    "                #print(yes)\n",
    "                #print(new_yes)\n",
    "            \n",
    "            \n",
    "            \n",
    "                #ta=random.choice(train_t[z]) #ta is true positve photo\n",
    "                #train_pair_t.append(ta)\n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                #print('Image_1 shape ',image_1.shape)\n",
    "                #train_f_sample = random.sample(train_f[z],20)\n",
    "                #print('True:',train_t_sample,'Now:',ta)\n",
    "                #print('False:',train_f_sample)\n",
    "                train_f_sample = random.sample(train_f[z],30)\n",
    "                for b in train_f_sample:\n",
    "                    #print('likes:',ta,';Not likes:',b)\n",
    "                    #b=random.choice(train_f[z])  #b is no feedback photo\n",
    "                    #train_pair_f.append(b)\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    #print('Image_2 shape',image_2.shape)\n",
    "            \n",
    "                    #use_test[z].append(b)\n",
    "                    _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: new_yes , l_id:new_sample, l_id_len:[len(new_sample)],r:new_r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                \n",
    "                    #print(XUIJ)\n",
    "                    #print('loss=',_loss)\n",
    "                    #print('auc=',_auc)\n",
    "                    #print('user positive negative',z,ta,b)\n",
    "                    #for each_par in _norm_par:\n",
    "                    #    print(each_par.shape)\n",
    "                    #    print(each_par)\n",
    "                    #print('sub,wu,wy,wa,wv',_last_be_relu)\n",
    "                    #print('sub before softmax:',_a_list)\n",
    "                    #print('sub alpha softmax:',r3)\n",
    "                    \n",
    "                    #print('---------------------------------------------------')\n",
    "        \n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "                #now1+=1\n",
    "        \n",
    "            #np.savez('../Data/latent_factor/YRM_up10/'+str(q)+'_'+str(z)+'.npz', User=U_history, YouTuber=Y_history)\n",
    "    \n",
    "        #print('mine:',xuij_auc/136)   \n",
    "        print('tanh ,wu,wy,wa,wv',_last_be_relu)\n",
    "        print('a_list:',_a_list)\n",
    "        print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        \n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    #print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])\n",
    "    #np.savez('../Data/grid_search_weight/E_dims/'+save_name+'.npz', \n",
    "    #                    U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    np.savez('../Data/grid_search_weight/Tan/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    return U, Y, A, E, Au, Ay, Aa, Av,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.01 1 0.01 150 :START!!---------------\n",
      "Iteraction: 0\n",
      "tanh ,wu,wy,wa,wv [array([[0.00701286]], dtype=float32), array([[1.420177e-33]], dtype=float32), array([[-3.8385268e-34]], dtype=float32), array([[-3.062703e-07]], dtype=float32)]\n",
      "a_list: [0.00253238 0.00253375 0.00253239 0.00701295 0.00701193 0.00077927\n",
      " 0.00701273]\n",
      "a_list_soft: [0.14261834 0.14261854 0.14261834 0.1432588  0.14325865 0.14236854\n",
      " 0.14325877]\n",
      "total_loss:----------------- [[0.38931274]]\n",
      "train_auc:------------------- 0.9338214783821478\n",
      "time: 131.17018055915833  sec\n",
      "Iteraction: 1\n",
      "tanh ,wu,wy,wa,wv [array([[-0.00183654]], dtype=float32), array([[2.3617425e-33]], dtype=float32), array([[-3.8385268e-34]], dtype=float32), array([[5.95852e-06]], dtype=float32)]\n",
      "a_list: [-0.00183253 -0.00183616 -0.0006634  -0.00066321 -0.00066642 -0.00020428\n",
      " -0.00184855]\n",
      "a_list_soft: [0.1427528  0.14275229 0.1429198  0.14291981 0.14291938 0.14298543\n",
      " 0.14275052]\n",
      "total_loss:----------------- [[0.28001574]]\n",
      "train_auc:------------------- 0.95278940027894\n",
      "time: 261.84759521484375  sec\n",
      "Iteraction: 2\n",
      "tanh ,wu,wy,wa,wv [array([[-5.024057e-09]], dtype=float32), array([[4.044404e-33]], dtype=float32), array([[3.7056655e-33]], dtype=float32), array([[9.5134163e-07]], dtype=float32)]\n",
      "a_list: [-1.5942932e-06  8.7055879e-10 -5.0195195e-09 -5.6015271e-08\n",
      "  3.2973222e-07 -3.1947525e-07 -1.7946890e-07]\n",
      "a_list_soft: [0.14285694 0.14285718 0.14285716 0.14285716 0.14285722 0.14285713\n",
      " 0.14285715]\n",
      "total_loss:----------------- [[0.25005946]]\n",
      "train_auc:------------------- 0.9635983263598327\n",
      "time: 393.06189727783203  sec\n",
      "Iteraction: 3\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [1,0.1,0.01]\n",
    "Embedding_weights = [1,0.1,0.01]\n",
    "#Embedding_dims = [5,10,15,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]\n",
    "Embedding_dims = [150]\n",
    "for paru_weight in par_weights:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print(paru_weight,pary_weight,beta_weight,Embedding_weight,embedding_dims,':START!!---------------')\n",
    "                    #clear_output()\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                          initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                          initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                          initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,k],  #n*m*kt\n",
    "                                                          initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [m,k],  #1*k \n",
    "                                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [m,k],  #1*k\n",
    "                                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [m,l],  #1*l\n",
    "                                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                                 # initializer=tf.contrib.layers.xavier_initializer())\n",
    "                                                         initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = Wy\n",
    "                    wa = Wa\n",
    "                    wv = Wv\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(wu,0)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.math.tanh( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui, xfi, transpose_b=True))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    a_list_soft=tf.nn.softmax(a_list)\n",
    "                    #a_list_soft = tf.divide(a_list,tf.reduce_sum(a_list, 0)) #without softmax\n",
    "                    \n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(wu,0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.expand_dims(xf[-1],0), transpose_b=True)\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                paru_weight * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                              ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(str(pary_weight)+'_'+str(pary_weight)+'_'+str(beta_weight)+'_'+str(Embedding_weight)+'_Edims'+str(embedding_dims))\n",
    "                    #np.savez('../Data/grid_search_weight/new_dims/'+str(pary_weight)+'_'+str(pary_weight)+'_'+str(beta_weight)+'_'+str(Embedding_weight)+'.npz', \n",
    "                    #         U=Ur, Y=Yr, A=Ar, E=Er, Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr,B=Br)\n",
    "                    print('Finish,',str(paru_weight)+'_'+str(pary_weight)+'_'+str(beta_weight)+'_'+str(Embedding_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(U, Y, A, E, A1, Au, Ay, Aa, Av,B):\n",
    "    #print(A1)\n",
    "    result=np.zeros((test_amount,88))\n",
    "    RS=np.zeros((test_amount,88))\n",
    "    #test_idx --> Test 的 index\n",
    "    max1 = 0\n",
    "    maxu = 0\n",
    "    maxy = 0\n",
    "    maxa = 0\n",
    "    maxv = 0\n",
    "    min1 = 100000000000000000\n",
    "    minu = 100000000000000000\n",
    "    miny = 100000000000000000\n",
    "    mina = 100000000000000000\n",
    "    minv = 100000000000000000\n",
    "    test_yes_id=[]\n",
    "    for s in range(test_amount):\n",
    "        print(s,test_idx[s])\n",
    "\n",
    "        yes=[]\n",
    "        sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "        #sample=result_yes_id[now]\n",
    "        test_yes_id.append(sample)\n",
    "        alpha=np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "            #print(test_idx[s])\n",
    "            #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "\n",
    "            #Observe each part in attention , below par are all (128,1)\n",
    "            testW1 = np.sum(A1[test_idx[s]])\n",
    "            #print(A1[test_idx[s]])\n",
    "            WuUu = np.sum(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T))\n",
    "            WyYy = np.sum(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T))\n",
    "            WaAa = np.sum(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T))\n",
    "            WvVy = np.sum(np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))\n",
    "            print('The sum of each par -->\\nw1:',testW1,'\\nWuU:',WuUu,'\\nwyY:',WyYy,'\\nWaA:',WaAa,'\\nWvV:',WvVy)\n",
    "            if testW1 > max1:\n",
    "                max1 = testW1\n",
    "            if testW1 < min1:\n",
    "                min1 = testW1\n",
    "            if WuUu > maxu:\n",
    "                maxu = WuUu\n",
    "            if WuUu < minu:\n",
    "                minu = WuUu\n",
    "            if WyYy > maxy:\n",
    "                maxy = WyYy\n",
    "            if WyYy < miny:\n",
    "                miny = WyYy\n",
    "            if WaAa > maxa:\n",
    "                maxa = WaAa\n",
    "            if WaAa < mina:\n",
    "                mina = WaAa\n",
    "            if WvVy > maxv:\n",
    "                mxv = WvVy\n",
    "            if WvVy < minv:\n",
    "                minv = WvVy\n",
    "            #Have w1\n",
    "            #alpha[a]=np.dot(A1[test_idx[s]],(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "            #                    np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "            #Without w1\n",
    "            alpha[a]=np.sum((relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                                np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "        mul=np.zeros((1,128))\n",
    "        print('alpha------------',alpha)\n",
    "        print('softmax alpha--------------',softmax(alpha))\n",
    "        for i in range(len(sample)):\n",
    "            mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "        new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "        for k in range(88):\n",
    "            result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]],np.dot(E, all_3374[k].T))\n",
    "    #print(max1,maxu,maxy,maxa,maxv)\n",
    "    #print(min1,minu,miny,mina,minv)\n",
    "    return RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "#從grid_search_weight中找尋不同的file \n",
    "path = '../Data/grid_search_weight/0105/'\n",
    "files = os.listdir(path)\n",
    "#U, Y, A, E, A1, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, W1, Wu, Wy, Wa, Wv,Beta])\n",
    "for file in files:\n",
    "    par_data = np.load(path+file)\n",
    "    U = par_data['U']\n",
    "    Y = par_data['Y']\n",
    "    A = par_data['A']\n",
    "    E = par_data['E']\n",
    "    W1 = par_data['W1']\n",
    "    Wu = par_data['Wu']\n",
    "    Wy = par_data['Wy']\n",
    "    Wa = par_data['Wa']\n",
    "    Wv = par_data['Wv']\n",
    "    B = par_data['B']\n",
    "    RS = testing(U, Y, A, E, W1, Wu, Wy, Wa, Wv,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Y, A, E, A1, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, W1, Wu, Wy, Wa, Wv,Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('W1 weight shape: ',A1.shape)\n",
    "print('Wu weight shape:',Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:',Aa.shape)\n",
    "print('Wv weight shape:',Av.shape)\n",
    "print('Beta shape:',B.shape)\n",
    "print('Embedding shape:',E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=np.zeros((test_amount,88))\n",
    "RS=np.zeros((test_amount,88))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id=[]\n",
    "for s in range(test_amount):\n",
    "    print(s,test_idx[s])\n",
    "\n",
    "    yes=[]\n",
    "    sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha=np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        #print(test_idx[s])\n",
    "        #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "        \n",
    "        #Observe each part in attention , below par are all (128,1)\n",
    "        testW1 = np.sum(np.multiply(A1[test_idx[s]],A1[test_idx[s]]))\n",
    "        WuUu = np.sum(np.multiply(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T),np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)))\n",
    "        WyYy = np.sum(np.multiply(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T),np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)))\n",
    "        WaAa = np.sum(np.multiply(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T),np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T)))\n",
    "        WvVy = np.sum(np.multiply(np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T),np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T)))\n",
    "        print('w1:',testW1,'\\nWuU:',WuUu,'\\nwyY:',WyYy,'\\nWaA:',WaAa,'\\nWvV:',WvVy)\n",
    "        \n",
    "        alpha[a]=np.dot(A1[test_idx[s]],(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                            np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "    mul=np.zeros((1,128))\n",
    "    print('alpha------------',alpha)\n",
    "    print('softmax alpha--------------',softmax(alpha))\n",
    "    for i in range(len(sample)):\n",
    "        mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    for k in range(88):\n",
    "        result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]],np.dot(E, all_3374[k].T))\n",
    "#print(RS[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取出test的資料\n",
    "testRS = np.zeros((test_amount,yt_test_amount)) #shape 150*20\n",
    "target = np.zeros((test_amount,yt_test_amount))\n",
    "#test_t 是true的\n",
    "#test_f 是false的\n",
    "        \n",
    "for z in range(test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    #positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    #not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "    #前兩個放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumtarget = 0\n",
    "for i in range(len(target)):\n",
    "    #print(np.sum(target[i]))\n",
    "    sumtarget += np.sum(target[i])\n",
    "print('num of positive data in testing:',sumtarget)\n",
    "print('total testing data:',test_amount*yt_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),int(np.sum(target[i])))\n",
    "    count_0_all.append(top_0)\n",
    "    print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = 0\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        #print(int(np.sum(target[i])))\n",
    "        total+=int(np.sum(target[i]))\n",
    "        if count_0_all[i][j] < int(np.sum(target[i])): #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "avg_acc = acc_0/100\n",
    "print('avg_accuarcy for count_0:',avg_acc)\n",
    "print(acc_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),1) #取一個\n",
    "    count_0_all.append(top_0)\n",
    "    print(top_0)\n",
    "    if top_0[0] < int(np.sum(target[i])):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_prec = correct/len(testRS)\n",
    "top1_recall = correct/(sumtarget)\n",
    "print('prec ',top1_prec,'recall ',top1_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top1_prec,top1_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_3 = topN(list(testRS[i]),3) #取一個\n",
    "    count_0_all.append(top_3)\n",
    "    #print(top_3)\n",
    "    for j in range(len(top_3)):\n",
    "        if top_3[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_prec = correct/(len(testRS)*3)\n",
    "top3_recall = correct/(sumtarget)\n",
    "print('prec ',top3_prec,'recall ',top3_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top3_prec,top3_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    count_0_all.append(top_5)\n",
    "    #print(top_5)\n",
    "    for j in range(len(top_5)):\n",
    "        if top_5[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_prec = correct/(len(testRS)*5)\n",
    "top5_recall = correct/(sumtarget)\n",
    "print('prec ',top5_prec,'recall ',top5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top5_prec,top5_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User_latent_factor = loaddata['User']\n",
    "#YouTuber_latent_factor = loaddata['YouTuber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('../Data/latent_factor/YRM_up10_ALL/Final/1226.npz', User=U, YouTuber=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax([-0.000000000000000000000000000000000000001,0.00000000000000000000000000000000000001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = tf.constant([1.1,2.7,6.1,4.7,5.5,6.5])\n",
    "B = tf.divide(C,tf.reduce_sum(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
