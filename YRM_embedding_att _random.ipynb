{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data positive feedback dynamic (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_split_path = 'D:/ChilliHsu/Data/Result_Matrix/Dims200_Text300/'\n",
    "with open(data_split_path+'test_t.json') as json_file:\n",
    "    test_t = json.load(json_file)\n",
    "with open(data_split_path+'test_f.json') as json_file:\n",
    "    test_f = json.load(json_file)\n",
    "with open(data_split_path+'train_t.json') as json_file:\n",
    "    train_t = json.load(json_file)\n",
    "with open(data_split_path+'train_f.json') as json_file:\n",
    "    train_f = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(train_t)):\n",
    "    if len(list(set(train_t[i]+train_f[i]))) == 44:\n",
    "        print(len(list(set(train_t[i]+train_f[i]+test_t[count]+test_f[count]))))\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_following = np.load('../Data/npy/user_following_1489.npy')\n",
    "all_3374 = np.load('../Data/npy/all_2939D_img0.5.npy')\n",
    "user_category = np.load('../Data/npy/user_category_1489.npy')\n",
    "YouTuber_category = np.load('../Data/npy/YouTuber_category_0.7.npy')\n",
    "active_users = np.load('../Data/npy/active_userID_1489.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test_amount = 150\n",
    "yt_test_amount = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random choice all 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all 0.2\n",
    "def generate_train_test(user_following,feature,user_category,item_category,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    print('user_category shape ',user_category.shape)\n",
    "    print('YouTuber_category shape ',item_category.shape)\n",
    "    user_category_norm = np.zeros(user_category.shape)\n",
    "    for i in range(len(user_category)):\n",
    "        user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "    print('user_category after normalized by max...')\n",
    "    print('user_category_norm shape ',user_category_norm.shape)\n",
    "    print('user cateogory norm',user_category_norm)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = item_category[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(33)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    test_pos = -1\n",
    "    for i in range(num_of_users):\n",
    "        t_for_train = []\n",
    "        f_for_train = []\n",
    "        if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    t_for_train.append(j)\n",
    "                else:\n",
    "                    f_for_train.append(j)\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "\n",
    "        else: #if in test id, choose 2 true and other \n",
    "            all_yts = [x for x in range(88)]\n",
    "            test_yt = random.sample(all_yts,math.ceil(0.2*len(all_yts)))\n",
    "            print(test_yt)\n",
    "            test_pos += 1\n",
    "            temp_t = []\n",
    "            temp_f = []\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    temp_t.append(j)\n",
    "                else:\n",
    "                    temp_f.append(j)\n",
    "            #print(temp_t)\n",
    "            #print(temp_f)\n",
    "            #t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "            #f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "            t_for_test = [item for item in test_yt if item in temp_t]\n",
    "            f_for_test = [item for item in test_yt if item in temp_f]\n",
    "            print(i,len(t_for_test)+len(f_for_test))\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,user_category_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive 0.2\n",
    "def generate_train_test(user_following,feature,user_category,item_category,active_users,user_test_amount,yt_test_amount):\n",
    "    print('Generate_train_test----')\n",
    "    #The shape of orignal data\n",
    "    print('user_following shape ',user_following.shape)\n",
    "    print('feature shape ',feature.shape)\n",
    "    print('user_category shape ',user_category.shape)\n",
    "    print('YouTuber_category shape ',item_category.shape)\n",
    "    user_category_norm = np.zeros(user_category.shape)\n",
    "    for i in range(len(user_category)):\n",
    "        user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "    print('user_category after normalized by max...')\n",
    "    print('user_category_norm shape ',user_category_norm.shape)\n",
    "    print('user cateogory norm',user_category_norm)\n",
    "    \n",
    "    #following youtuber for each user\n",
    "    following_true = []\n",
    "    for i in range(len(user_following)):\n",
    "        each_user = []\n",
    "        for j in range(len(user_following[i])):\n",
    "            if user_following[i][j] == 1:\n",
    "                each_user.append(j)\n",
    "        following_true.append(each_user)\n",
    "    #print(following_true)\n",
    "    #number of followings for each user\n",
    "    minlen = 10000\n",
    "    maxlen = 0\n",
    "    num_of_follower = []\n",
    "    for i in range(len(following_true)):\n",
    "        if len(following_true[i]) < minlen:\n",
    "            minlen = len(following_true[i])\n",
    "        if len(following_true[i]) > maxlen:\n",
    "            maxlen = len(following_true[i])\n",
    "        num_of_follower.append(len(following_true[i]))\n",
    "    print('Min number of followings ',minlen)\n",
    "    print('Max number of followings ',maxlen)\n",
    "    \n",
    "    num_of_users = user_following.shape[0]\n",
    "    num_of_youtubers = item_category[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Spliting training and testing data\n",
    "    \"\"\"\n",
    "    #testing user id random choice\n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(5)\n",
    "    #choose test_id and sorted from small to large\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    \n",
    "    print('test_idx',test_idx)\n",
    "    test_pos = -1\n",
    "    for i in range(num_of_users):\n",
    "        t_for_train = []\n",
    "        f_for_train = []\n",
    "        if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    t_for_train.append(j)\n",
    "                else:\n",
    "                    f_for_train.append(j)\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "\n",
    "        else: #if in test id, choose 2 true and other \n",
    "            test_pos += 1\n",
    "            temp_t = []\n",
    "            temp_f = []\n",
    "            for j in range(88):\n",
    "                if user_following[i][j] == 1:\n",
    "                    temp_t.append(j)\n",
    "                else:\n",
    "                    temp_f.append(j)\n",
    "                \n",
    "            t_for_test = random.sample(temp_t,math.ceil(0.2*len(temp_t)))\n",
    "            f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "            \n",
    "    # train_t[i] 代表的是user i positive feedback\n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,user_category_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate_train_test----\n",
      "user_following shape  (1489, 88)\n",
      "feature shape  (88, 2939)\n",
      "user_category shape  (1489, 17)\n",
      "YouTuber_category shape  (88, 17)\n",
      "user_category after normalized by max...\n",
      "user_category_norm shape  (1489, 17)\n",
      "user cateogory norm [[0.         1.         0.         ... 0.05714286 0.         0.        ]\n",
      " [0.24390244 0.         0.02439024 ... 0.         0.         0.09756098]\n",
      " [0.04210526 0.04210526 0.05263158 ... 0.02105263 0.         0.02105263]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01408451 0.01408451 0.04225352 ... 0.02816901 0.         0.02816901]\n",
      " [0.03703704 0.22222222 0.14814815 ... 0.         0.         0.        ]]\n",
      "Min number of followings  5\n",
      "Max number of followings  34\n",
      "test_idx [6, 13, 19, 26, 30, 42, 67, 68, 71, 107, 114, 117, 129, 135, 143, 144, 179, 188, 191, 216, 225, 227, 240, 245, 252, 253, 305, 334, 342, 343, 353, 379, 381, 394, 415, 436, 466, 477, 497, 499, 503, 533, 553, 558, 564, 567, 572, 574, 576, 577, 582, 586, 617, 623, 630, 633, 645, 648, 650, 653, 654, 655, 657, 658, 665, 685, 713, 724, 733, 740, 746, 749, 754, 776, 777, 794, 795, 796, 807, 813, 832, 839, 847, 864, 872, 876, 888, 908, 911, 930, 937, 947, 967, 969, 980, 984, 985, 986, 989, 995, 1013, 1016, 1030, 1038, 1047, 1050, 1061, 1074, 1080, 1081, 1090, 1103, 1110, 1136, 1137, 1146, 1168, 1201, 1207, 1224, 1232, 1244, 1253, 1263, 1273, 1274, 1277, 1280, 1284, 1294, 1299, 1301, 1323, 1334, 1335, 1338, 1345, 1348, 1367, 1378, 1386, 1395, 1398, 1407, 1414, 1437, 1457, 1477, 1480, 1483]\n",
      "[74, 10, 73, 15, 25, 24, 77, 9, 8, 87, 1, 55, 67, 47, 0, 19, 52, 2]\n",
      "6 18\n",
      "[11, 17, 58, 5, 2, 75, 26, 71, 29, 86, 14, 42, 46, 63, 39, 3, 85, 0]\n",
      "13 18\n",
      "[63, 37, 79, 42, 41, 67, 16, 12, 43, 49, 57, 34, 25, 10, 33, 22, 53, 17]\n",
      "19 18\n",
      "[61, 69, 59, 58, 41, 81, 30, 0, 2, 3, 10, 19, 43, 23, 46, 32, 39, 78]\n",
      "26 18\n",
      "[1, 63, 57, 76, 49, 73, 36, 34, 28, 5, 20, 50, 3, 44, 10, 14, 24, 82]\n",
      "30 18\n",
      "[49, 42, 30, 48, 5, 86, 16, 66, 20, 78, 9, 79, 63, 24, 25, 55, 36, 39]\n",
      "42 18\n",
      "[80, 55, 30, 75, 85, 48, 58, 56, 81, 51, 12, 47, 20, 35, 27, 4, 13, 23]\n",
      "67 18\n",
      "[49, 42, 61, 22, 54, 16, 30, 12, 17, 66, 53, 18, 72, 64, 11, 50, 67, 56]\n",
      "68 18\n",
      "[48, 0, 75, 12, 56, 63, 27, 64, 39, 23, 69, 55, 36, 76, 58, 86, 30, 53]\n",
      "71 18\n",
      "[32, 1, 38, 21, 61, 39, 24, 57, 5, 31, 77, 55, 64, 46, 44, 15, 25, 3]\n",
      "107 18\n",
      "[55, 40, 31, 70, 35, 80, 83, 79, 75, 42, 33, 44, 3, 37, 14, 23, 65, 26]\n",
      "114 18\n",
      "[3, 36, 7, 53, 66, 40, 41, 13, 55, 65, 85, 45, 32, 44, 31, 76, 1, 68]\n",
      "117 18\n",
      "[70, 60, 85, 11, 33, 57, 38, 10, 41, 55, 77, 42, 22, 49, 18, 39, 16, 59]\n",
      "129 18\n",
      "[47, 44, 38, 37, 36, 41, 85, 75, 0, 39, 20, 8, 53, 68, 74, 40, 23, 19]\n",
      "135 18\n",
      "[63, 84, 16, 52, 30, 21, 80, 36, 75, 55, 68, 25, 86, 81, 62, 15, 1, 78]\n",
      "143 18\n",
      "[80, 78, 24, 20, 43, 70, 35, 40, 60, 44, 21, 22, 85, 10, 14, 71, 8, 3]\n",
      "144 18\n",
      "[21, 18, 70, 85, 51, 69, 80, 15, 82, 60, 78, 25, 2, 79, 75, 71, 55, 48]\n",
      "179 18\n",
      "[3, 69, 46, 2, 57, 11, 24, 19, 63, 36, 81, 41, 45, 26, 66, 82, 18, 72]\n",
      "188 18\n",
      "[17, 26, 14, 41, 70, 31, 5, 12, 28, 69, 50, 58, 63, 9, 75, 46, 33, 48]\n",
      "191 18\n",
      "[43, 66, 64, 0, 8, 25, 44, 19, 84, 35, 31, 13, 15, 83, 46, 52, 17, 69]\n",
      "216 18\n",
      "[7, 71, 84, 78, 12, 3, 54, 70, 50, 43, 32, 46, 48, 44, 28, 64, 14, 66]\n",
      "225 18\n",
      "[42, 48, 21, 12, 2, 39, 70, 33, 3, 67, 22, 72, 61, 54, 56, 50, 35, 63]\n",
      "227 18\n",
      "[10, 25, 51, 76, 67, 55, 48, 19, 47, 6, 22, 68, 43, 41, 79, 38, 15, 39]\n",
      "240 18\n",
      "[39, 46, 47, 26, 23, 83, 75, 7, 50, 65, 28, 67, 87, 66, 10, 69, 21, 82]\n",
      "245 18\n",
      "[66, 71, 65, 35, 84, 37, 74, 28, 20, 2, 82, 50, 75, 44, 51, 78, 23, 79]\n",
      "252 18\n",
      "[54, 47, 0, 86, 57, 73, 17, 46, 8, 26, 84, 22, 56, 75, 40, 1, 79, 2]\n",
      "253 18\n",
      "[62, 42, 32, 17, 34, 71, 45, 16, 57, 55, 20, 7, 63, 2, 44, 8, 61, 10]\n",
      "305 18\n",
      "[3, 26, 74, 15, 20, 10, 13, 61, 12, 42, 22, 5, 58, 19, 76, 87, 7, 0]\n",
      "334 18\n",
      "[22, 79, 86, 17, 67, 50, 87, 75, 21, 8, 37, 65, 59, 20, 56, 72, 53, 47]\n",
      "342 18\n",
      "[71, 63, 50, 86, 9, 59, 16, 74, 84, 87, 24, 54, 47, 36, 66, 49, 25, 21]\n",
      "343 18\n",
      "[46, 69, 83, 42, 3, 51, 76, 1, 0, 11, 86, 43, 12, 33, 19, 52, 48, 62]\n",
      "353 18\n",
      "[35, 44, 25, 6, 47, 14, 46, 8, 24, 51, 43, 48, 13, 52, 11, 41, 39, 84]\n",
      "379 18\n",
      "[50, 21, 1, 47, 76, 85, 7, 36, 16, 54, 75, 48, 66, 4, 19, 80, 62, 5]\n",
      "381 18\n",
      "[52, 18, 45, 34, 82, 55, 30, 42, 85, 77, 23, 0, 6, 46, 63, 47, 24, 17]\n",
      "394 18\n",
      "[85, 75, 68, 28, 51, 36, 32, 35, 33, 46, 49, 10, 52, 31, 6, 76, 30, 12]\n",
      "415 18\n",
      "[59, 11, 35, 15, 23, 74, 61, 71, 25, 81, 30, 85, 47, 9, 80, 31, 1, 39]\n",
      "436 18\n",
      "[41, 38, 9, 74, 49, 67, 50, 8, 21, 0, 71, 22, 73, 7, 58, 32, 81, 15]\n",
      "466 18\n",
      "[81, 12, 58, 48, 11, 5, 50, 29, 54, 83, 2, 20, 56, 59, 43, 57, 15, 30]\n",
      "477 18\n",
      "[36, 39, 7, 20, 71, 24, 86, 26, 87, 30, 11, 5, 62, 2, 77, 21, 51, 28]\n",
      "497 18\n",
      "[77, 24, 1, 59, 19, 55, 73, 63, 36, 4, 50, 17, 51, 27, 22, 38, 40, 11]\n",
      "499 18\n",
      "[34, 40, 42, 39, 57, 4, 19, 35, 27, 2, 14, 48, 53, 3, 73, 75, 13, 22]\n",
      "503 18\n",
      "[26, 44, 68, 7, 16, 67, 38, 84, 33, 69, 20, 12, 73, 52, 77, 60, 11, 13]\n",
      "533 18\n",
      "[55, 31, 80, 50, 46, 87, 41, 59, 75, 58, 49, 71, 78, 13, 82, 70, 51, 35]\n",
      "553 18\n",
      "[21, 55, 63, 85, 34, 62, 11, 28, 66, 69, 44, 0, 38, 83, 84, 78, 73, 13]\n",
      "558 18\n",
      "[32, 64, 83, 57, 3, 47, 30, 21, 62, 53, 37, 63, 18, 20, 65, 38, 87, 55]\n",
      "564 18\n",
      "[59, 40, 38, 8, 15, 51, 63, 20, 2, 43, 73, 31, 0, 85, 68, 47, 76, 44]\n",
      "567 18\n",
      "[16, 36, 32, 0, 3, 19, 70, 43, 1, 47, 54, 69, 73, 57, 65, 81, 23, 26]\n",
      "572 18\n",
      "[85, 62, 67, 72, 43, 69, 52, 29, 84, 34, 24, 68, 35, 79, 58, 73, 19, 76]\n",
      "574 18\n",
      "[42, 2, 24, 44, 50, 43, 9, 35, 11, 18, 29, 10, 23, 80, 73, 61, 47, 87]\n",
      "576 18\n",
      "[60, 42, 38, 74, 87, 45, 73, 28, 25, 72, 0, 8, 80, 6, 64, 59, 71, 43]\n",
      "577 18\n",
      "[66, 6, 81, 39, 50, 79, 73, 3, 64, 75, 44, 60, 7, 65, 78, 15, 30, 9]\n",
      "582 18\n",
      "[33, 0, 19, 79, 78, 58, 74, 30, 65, 66, 75, 71, 46, 47, 44, 68, 18, 36]\n",
      "586 18\n",
      "[45, 82, 39, 87, 28, 8, 55, 34, 64, 37, 80, 40, 78, 51, 77, 15, 36, 65]\n",
      "617 18\n",
      "[74, 19, 43, 33, 27, 70, 28, 41, 44, 66, 50, 55, 52, 14, 78, 80, 83, 39]\n",
      "623 18\n",
      "[17, 4, 33, 23, 66, 79, 80, 7, 73, 20, 76, 1, 25, 8, 62, 3, 69, 47]\n",
      "630 18\n",
      "[9, 3, 83, 31, 22, 45, 79, 13, 20, 56, 8, 34, 23, 66, 72, 87, 58, 16]\n",
      "633 18\n",
      "[64, 15, 86, 12, 46, 74, 42, 11, 13, 48, 24, 7, 75, 82, 36, 77, 39, 35]\n",
      "645 18\n",
      "[9, 85, 19, 27, 83, 59, 66, 15, 86, 1, 26, 29, 47, 25, 48, 54, 84, 36]\n",
      "648 18\n",
      "[37, 65, 5, 18, 83, 63, 15, 33, 62, 67, 2, 3, 30, 76, 82, 84, 47, 31]\n",
      "650 18\n",
      "[3, 63, 44, 57, 12, 61, 77, 2, 84, 71, 19, 55, 5, 83, 1, 11, 6, 15]\n",
      "653 18\n",
      "[57, 80, 69, 79, 47, 37, 60, 58, 20, 42, 41, 65, 82, 77, 11, 21, 16, 29]\n",
      "654 18\n",
      "[59, 1, 24, 0, 8, 42, 50, 2, 10, 57, 77, 41, 65, 61, 16, 3, 66, 19]\n",
      "655 18\n",
      "[13, 25, 34, 81, 14, 82, 68, 84, 29, 9, 15, 75, 33, 73, 87, 85, 37, 8]\n",
      "657 18\n",
      "[26, 76, 31, 53, 21, 65, 23, 35, 34, 87, 52, 55, 54, 40, 80, 72, 85, 50]\n",
      "658 18\n",
      "[11, 37, 58, 43, 49, 54, 51, 60, 25, 42, 65, 3, 34, 28, 55, 7, 81, 44]\n",
      "665 18\n",
      "[63, 12, 74, 80, 43, 16, 49, 59, 77, 4, 85, 81, 28, 10, 39, 21, 6, 51]\n",
      "685 18\n",
      "[76, 3, 56, 38, 84, 64, 85, 67, 61, 48, 20, 59, 72, 65, 32, 54, 42, 1]\n",
      "713 18\n",
      "[64, 41, 18, 40, 44, 80, 52, 1, 42, 26, 72, 5, 17, 82, 75, 73, 37, 54]\n",
      "724 18\n",
      "[36, 81, 50, 24, 7, 83, 60, 32, 61, 43, 16, 62, 57, 25, 79, 44, 39, 71]\n",
      "733 18\n",
      "[31, 62, 56, 29, 14, 75, 63, 74, 54, 36, 52, 47, 33, 50, 49, 26, 8, 80]\n",
      "740 18\n",
      "[39, 20, 45, 16, 2, 63, 48, 42, 83, 82, 79, 1, 60, 27, 17, 14, 46, 55]\n",
      "746 18\n",
      "[74, 24, 7, 18, 34, 48, 61, 84, 4, 66, 21, 57, 45, 54, 20, 25, 75, 11]\n",
      "749 18\n",
      "[12, 56, 72, 62, 45, 63, 27, 51, 18, 71, 53, 42, 59, 86, 60, 54, 49, 17]\n",
      "754 18\n",
      "[38, 64, 51, 50, 25, 47, 44, 11, 27, 77, 15, 75, 28, 58, 40, 19, 3, 43]\n",
      "776 18\n",
      "[19, 0, 57, 31, 40, 27, 77, 4, 41, 49, 21, 11, 78, 13, 42, 48, 36, 56]\n",
      "777 18\n",
      "[68, 67, 11, 26, 51, 18, 28, 66, 3, 39, 52, 81, 34, 65, 43, 64, 13, 47]\n",
      "794 18\n",
      "[81, 14, 49, 48, 8, 82, 62, 5, 20, 28, 70, 18, 13, 41, 59, 68, 79, 80]\n",
      "795 18\n",
      "[51, 54, 84, 1, 83, 67, 45, 15, 7, 10, 40, 16, 14, 48, 61, 33, 46, 71]\n",
      "796 18\n",
      "[18, 4, 67, 63, 81, 64, 35, 62, 33, 56, 79, 53, 70, 15, 21, 85, 50, 52]\n",
      "807 18\n",
      "[6, 26, 81, 42, 85, 64, 78, 31, 24, 12, 19, 23, 35, 15, 30, 62, 54, 32]\n",
      "813 18\n",
      "[9, 41, 78, 34, 60, 68, 45, 1, 38, 13, 15, 10, 81, 57, 3, 77, 11, 87]\n",
      "832 18\n",
      "[36, 5, 17, 68, 33, 0, 41, 66, 8, 71, 48, 9, 86, 28, 46, 77, 15, 30]\n",
      "839 18\n",
      "[64, 41, 73, 68, 27, 51, 33, 57, 11, 4, 84, 45, 3, 26, 24, 48, 8, 46]\n",
      "847 18\n",
      "[6, 73, 41, 15, 60, 5, 80, 65, 32, 81, 76, 82, 58, 7, 87, 68, 21, 66]\n",
      "864 18\n",
      "[21, 33, 0, 59, 12, 18, 29, 25, 22, 13, 23, 11, 49, 30, 68, 4, 83, 26]\n",
      "872 18\n",
      "[10, 46, 16, 47, 28, 70, 78, 45, 72, 66, 68, 60, 85, 42, 37, 19, 2, 59]\n",
      "876 18\n",
      "[49, 59, 24, 52, 2, 70, 18, 87, 0, 33, 44, 31, 74, 25, 55, 61, 71, 81]\n",
      "888 18\n",
      "[26, 1, 63, 82, 67, 19, 83, 5, 15, 46, 58, 75, 24, 18, 22, 73, 16, 23]\n",
      "908 18\n",
      "[78, 0, 71, 17, 35, 74, 49, 43, 18, 32, 41, 68, 25, 45, 22, 7, 80, 58]\n",
      "911 18\n",
      "[17, 83, 30, 1, 45, 82, 15, 40, 18, 36, 34, 80, 77, 44, 35, 55, 26, 25]\n",
      "930 18\n",
      "[9, 53, 58, 68, 83, 25, 73, 17, 79, 48, 42, 64, 12, 4, 27, 50, 51, 86]\n",
      "937 18\n",
      "[56, 8, 27, 84, 45, 80, 44, 34, 42, 31, 32, 79, 25, 7, 36, 47, 63, 75]\n",
      "947 18\n",
      "[0, 46, 65, 72, 49, 82, 8, 71, 60, 27, 74, 9, 1, 37, 30, 51, 84, 36]\n",
      "967 18\n",
      "[66, 70, 64, 55, 13, 35, 47, 65, 43, 21, 81, 57, 46, 49, 56, 42, 83, 48]\n",
      "969 18\n",
      "[75, 72, 17, 47, 83, 23, 46, 24, 65, 36, 49, 58, 19, 42, 35, 63, 52, 26]\n",
      "980 18\n",
      "[14, 80, 43, 64, 68, 26, 34, 56, 10, 17, 9, 48, 20, 70, 73, 24, 33, 78]\n",
      "984 18\n",
      "[83, 75, 54, 13, 59, 30, 49, 36, 12, 84, 68, 78, 9, 63, 40, 2, 28, 25]\n",
      "985 18\n",
      "[11, 59, 56, 50, 30, 23, 1, 84, 35, 41, 25, 72, 61, 27, 28, 58, 8, 4]\n",
      "986 18\n",
      "[24, 43, 65, 12, 27, 6, 42, 61, 79, 1, 76, 44, 60, 34, 29, 47, 33, 85]\n",
      "989 18\n",
      "[12, 83, 40, 19, 23, 2, 85, 20, 87, 9, 66, 53, 4, 10, 28, 32, 33, 74]\n",
      "995 18\n",
      "[57, 14, 61, 38, 22, 77, 58, 84, 81, 78, 47, 66, 68, 9, 56, 15, 3, 29]\n",
      "1013 18\n",
      "[32, 84, 33, 73, 71, 54, 57, 30, 6, 81, 31, 47, 20, 67, 50, 77, 22, 9]\n",
      "1016 18\n",
      "[15, 65, 77, 75, 55, 52, 46, 42, 34, 73, 12, 78, 43, 54, 10, 76, 38, 79]\n",
      "1030 18\n",
      "[67, 59, 55, 7, 10, 19, 69, 45, 84, 5, 37, 8, 51, 27, 75, 6, 44, 35]\n",
      "1038 18\n",
      "[86, 45, 60, 51, 53, 12, 56, 57, 28, 1, 40, 83, 14, 11, 36, 67, 23, 37]\n",
      "1047 18\n",
      "[15, 79, 70, 37, 29, 52, 67, 60, 22, 14, 62, 42, 82, 64, 45, 36, 25, 78]\n",
      "1050 18\n",
      "[42, 52, 43, 30, 63, 40, 29, 4, 73, 87, 62, 45, 54, 80, 33, 9, 41, 17]\n",
      "1061 18\n",
      "[41, 63, 34, 43, 44, 10, 6, 65, 58, 67, 4, 62, 40, 81, 2, 75, 8, 54]\n",
      "1074 18\n",
      "[25, 19, 37, 66, 36, 32, 11, 26, 27, 62, 78, 63, 40, 81, 45, 1, 53, 83]\n",
      "1080 18\n",
      "[38, 54, 65, 16, 10, 83, 3, 59, 32, 11, 85, 86, 7, 58, 64, 31, 29, 2]\n",
      "1081 18\n",
      "[4, 83, 18, 52, 5, 30, 41, 45, 3, 60, 26, 42, 9, 58, 34, 78, 46, 35]\n",
      "1090 18\n",
      "[17, 12, 62, 28, 18, 25, 65, 77, 2, 32, 37, 23, 78, 46, 86, 56, 29, 31]\n",
      "1103 18\n",
      "[50, 33, 56, 68, 83, 36, 45, 1, 15, 48, 86, 8, 30, 60, 39, 13, 71, 55]\n",
      "1110 18\n",
      "[85, 58, 7, 0, 75, 49, 42, 28, 30, 77, 26, 39, 56, 50, 34, 84, 46, 2]\n",
      "1136 18\n",
      "[74, 44, 20, 37, 48, 43, 59, 62, 18, 52, 80, 22, 51, 15, 33, 56, 31, 87]\n",
      "1137 18\n",
      "[68, 67, 56, 58, 60, 40, 1, 29, 5, 53, 63, 44, 16, 3, 59, 47, 39, 65]\n",
      "1146 18\n",
      "[76, 14, 58, 87, 85, 6, 38, 31, 36, 62, 35, 30, 59, 83, 45, 86, 72, 39]\n",
      "1168 18\n",
      "[34, 31, 40, 42, 11, 23, 17, 20, 32, 86, 33, 18, 87, 59, 15, 57, 45, 76]\n",
      "1201 18\n",
      "[18, 22, 71, 51, 38, 72, 37, 34, 60, 75, 42, 4, 32, 2, 55, 66, 79, 35]\n",
      "1207 18\n",
      "[8, 19, 4, 61, 44, 10, 72, 58, 56, 45, 57, 49, 27, 82, 48, 85, 87, 37]\n",
      "1224 18\n",
      "[58, 35, 59, 14, 15, 51, 74, 27, 69, 86, 62, 29, 52, 82, 65, 77, 78, 72]\n",
      "1232 18\n",
      "[19, 67, 41, 8, 58, 80, 59, 76, 34, 53, 82, 57, 6, 50, 48, 60, 3, 75]\n",
      "1244 18\n",
      "[35, 61, 64, 32, 68, 46, 26, 10, 50, 28, 74, 67, 12, 17, 42, 11, 24, 38]\n",
      "1253 18\n",
      "[69, 85, 31, 59, 74, 20, 0, 38, 19, 43, 83, 26, 12, 32, 29, 33, 73, 71]\n",
      "1263 18\n",
      "[68, 56, 53, 47, 41, 58, 24, 5, 2, 1, 33, 50, 48, 38, 3, 14, 36, 78]\n",
      "1273 18\n",
      "[58, 48, 78, 80, 8, 42, 27, 19, 86, 53, 74, 20, 23, 44, 87, 73, 70, 32]\n",
      "1274 18\n",
      "[15, 86, 21, 73, 7, 5, 3, 38, 52, 66, 29, 16, 18, 9, 8, 79, 41, 50]\n",
      "1277 18\n",
      "[14, 71, 64, 23, 62, 24, 4, 33, 12, 87, 5, 75, 3, 66, 78, 61, 74, 68]\n",
      "1280 18\n",
      "[4, 31, 41, 23, 36, 13, 1, 80, 68, 79, 60, 15, 5, 20, 77, 64, 51, 34]\n",
      "1284 18\n",
      "[57, 0, 67, 77, 85, 79, 16, 3, 26, 24, 49, 47, 62, 66, 11, 55, 27, 73]\n",
      "1294 18\n",
      "[23, 29, 66, 7, 62, 83, 18, 85, 65, 50, 74, 34, 32, 41, 12, 87, 53, 13]\n",
      "1299 18\n",
      "[66, 19, 18, 30, 82, 11, 60, 78, 43, 61, 21, 52, 48, 59, 10, 2, 46, 16]\n",
      "1301 18\n",
      "[74, 27, 12, 26, 77, 63, 85, 36, 17, 32, 70, 48, 50, 49, 51, 82, 80, 8]\n",
      "1323 18\n",
      "[29, 86, 69, 35, 23, 26, 60, 10, 2, 44, 36, 58, 40, 43, 42, 66, 64, 28]\n",
      "1334 18\n",
      "[74, 71, 64, 23, 85, 60, 68, 79, 66, 84, 43, 4, 51, 34, 30, 10, 8, 54]\n",
      "1335 18\n",
      "[12, 11, 31, 8, 85, 1, 0, 62, 60, 39, 64, 87, 46, 68, 14, 43, 35, 4]\n",
      "1338 18\n",
      "[69, 79, 41, 8, 16, 44, 15, 39, 67, 53, 45, 56, 63, 13, 54, 25, 76, 50]\n",
      "1345 18\n",
      "[40, 73, 63, 49, 32, 62, 12, 72, 79, 53, 84, 69, 27, 25, 44, 11, 82, 47]\n",
      "1348 18\n",
      "[68, 31, 16, 86, 81, 58, 59, 85, 51, 54, 28, 74, 76, 75, 0, 6, 11, 44]\n",
      "1367 18\n",
      "[78, 22, 77, 25, 18, 80, 50, 86, 52, 15, 47, 49, 84, 61, 1, 36, 73, 60]\n",
      "1378 18\n",
      "[23, 8, 50, 3, 30, 19, 51, 67, 13, 35, 54, 45, 44, 16, 28, 22, 84, 82]\n",
      "1386 18\n",
      "[79, 4, 38, 5, 48, 47, 6, 0, 33, 73, 7, 82, 18, 14, 58, 20, 35, 71]\n",
      "1395 18\n",
      "[73, 71, 46, 77, 9, 87, 68, 72, 57, 51, 85, 7, 36, 43, 49, 24, 13, 42]\n",
      "1398 18\n",
      "[51, 42, 38, 5, 65, 78, 48, 63, 54, 26, 1, 35, 37, 12, 18, 50, 72, 77]\n",
      "1407 18\n",
      "[70, 5, 51, 2, 3, 71, 21, 49, 78, 20, 53, 16, 77, 29, 11, 82, 44, 42]\n",
      "1414 18\n",
      "[11, 75, 30, 21, 68, 2, 29, 66, 14, 40, 51, 74, 10, 34, 87, 37, 70, 77]\n",
      "1437 18\n",
      "[87, 36, 60, 84, 65, 11, 72, 81, 85, 13, 18, 45, 15, 62, 57, 73, 16, 3]\n",
      "1457 18\n",
      "[55, 35, 54, 48, 43, 47, 41, 15, 51, 46, 13, 17, 10, 66, 71, 62, 16, 68]\n",
      "1477 18\n",
      "[25, 43, 72, 30, 41, 63, 53, 8, 48, 75, 45, 22, 23, 40, 50, 15, 77, 83]\n",
      "1480 18\n",
      "[80, 62, 48, 83, 87, 79, 86, 10, 50, 54, 55, 9, 17, 41, 15, 71, 18, 76]\n",
      "1483 18\n",
      "The length of train_t: 1489\n",
      "The length of train_f: 1489\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "train_t,train_f,test_t,test_f,user_category_norm = generate_train_test(user_following,all_3374,user_category,YouTuber_category,active_users,user_test_amount,yt_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 10.076561450638012\n",
      "testing 2.3066666666666666\n",
      "total testing: 346\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train/len(user_following)\n",
    "print('training',avg)\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    #print(t)\n",
    "    total_test += len(t)\n",
    "avg = total_test/user_test_amount\n",
    "print('testing',avg)\n",
    "print('total testing:', total_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(88)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation  Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(user_following)\n",
    "m = 88  \n",
    "k = 64\n",
    "l = 2939\n",
    "#embedding_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.trainable_variables()])\n",
    "    print('trainable_vars:',sess.run(all_trainable_vars))\n",
    "    \"\"\"for v in tf.trainable_variables():\n",
    "        print(v)\"\"\"\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #user degree of category favor \n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                train_f_sample = random.sample(train_f[z],10)\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    \n",
    "                    _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "                    \n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "        print('Current time:',time.ctime())\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    print('Current time:',time.ctime())\n",
    "    U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])\n",
    "    #np.savez('../Data/grid_search_weight/new/'+save_name+'.npz', \n",
    "    #                    U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    #np.savez('../Data/npy/mask_feature/result300/'+save_name+'.npz', \n",
    "    #                    U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    \n",
    "    return U, Y, A, E, Au, Ay, Aa, Av,B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Dims Finding (Remind to use all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished []\n",
      "891\n",
      "Now Dims: 200\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-12-8c744787f13a>:201: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "trainable_vars: 51947168\n",
      "Iteraction: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8c744787f13a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[1;31m#save_name = 'ALL'+str(embedding_dims)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[0msave_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monly_feature\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                     \u001b[0mUr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAyr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAvr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                     np.savez('../Data/grid_search_weight/our/new/'+save_name+'.npz', \n\u001b[0;32m    206\u001b[0m                         U=Ur, Y=Yr, A=Ar, E=Er, Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr,B=Br)\n",
      "\u001b[1;32m<ipython-input-11-e465fa5f57fa>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(save_name)\u001b[0m\n\u001b[0;32m     68\u001b[0m                     _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n\u001b[0;32m     69\u001b[0m                                         \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myes\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0ml_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_id_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpositive_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpositive_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                                         image_i:image_1,image_j:image_2})\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[1;31m#print(_a_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[1;31m#print(r3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "#l = all_3374.shape[1]\n",
    "try_count = [str(try_i) for try_i in range(1)]\n",
    "only_features = ['no_300image','no_300following','no_300video','no_300text','no_300social']\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for only_feature in only_features:\n",
    "    all_3374 = np.load('D://ChilliHsu/Data/npy/mask_feature/'+only_feature+'.npy')\n",
    "    l = all_3374.shape[1]\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(only_feature)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    \n",
    "                    \n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.001,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.001,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.001,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    \"\"\"l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)), \n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\"\"\"\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(u, u)), \n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    #save_name = 'ALL'+str(embedding_dims)\n",
    "                    save_name = only_feature+str(embedding_dims)\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(save_name)\n",
    "                    np.savez('../Data/grid_search_weight/our/new/'+save_name+'.npz', \n",
    "                        U=Ur, Y=Yr, A=Ar, E=Er, Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr,B=Br)\n",
    "                    \n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished []\n",
      "2939\n",
      "Now Dims: 200\n",
      "trainable_vars: 52356768\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.45759818]]\n",
      "train_auc:------------------- 0.8529925353239136\n",
      "time: 4230.158174753189  sec\n",
      "Current time: Fri Jun 12 00:11:27 2020\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.29488563]]\n",
      "train_auc:------------------- 0.8882897893894961\n",
      "time: 8455.462852239609  sec\n",
      "Current time: Fri Jun 12 01:21:53 2020\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.26780304]]\n",
      "train_auc:------------------- 0.9005798453745668\n",
      "time: 12680.886924982071  sec\n",
      "Current time: Fri Jun 12 02:32:18 2020\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.25270024]]\n",
      "train_auc:------------------- 0.9075246600906425\n",
      "time: 16904.557923078537  sec\n",
      "Current time: Fri Jun 12 03:42:42 2020\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.24468907]]\n",
      "train_auc:------------------- 0.9102705945081312\n",
      "time: 21136.80283498764  sec\n",
      "Current time: Fri Jun 12 04:53:14 2020\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.23668803]]\n",
      "train_auc:------------------- 0.9140162623300453\n",
      "time: 25364.68063020706  sec\n",
      "Current time: Fri Jun 12 06:03:42 2020\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.23318063]]\n",
      "train_auc:------------------- 0.9154225539856038\n",
      "time: 29592.26921272278  sec\n",
      "Current time: Fri Jun 12 07:14:09 2020\n",
      "Iteraction: 7\n",
      "total_loss:----------------- [[0.22932426]]\n",
      "train_auc:------------------- 0.9171154358837643\n",
      "time: 33819.121436834335  sec\n",
      "Current time: Fri Jun 12 08:24:36 2020\n",
      "Total cost  33819.121436834335  sec\n",
      "Current time: Fri Jun 12 08:24:36 2020\n",
      "Finish dims:, 200\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "#l = all_3374.shape[1]\n",
    "try_count = [str(try_i) for try_i in range(1)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for tri in try_count:\n",
    "    #all_3374 = np.load('../Data/npy/mask_feature/'+only_feature+'.npy')\n",
    "    #l = all_3374.shape[1]\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(tri)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    \n",
    "                    \n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.001,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.001,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0.1,0.001,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)), \n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                    \"\"\"l2_norm = tf.add_n([\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(u, u)), \n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\"\"\"\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    save_name = 'ALL'+str(embedding_dims)\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(save_name)\n",
    "                    np.savez('../Data/grid_search_weight/our/new/'+save_name+'.npz', \n",
    "                        U=Ur, Y=Yr, A=Ar, E=Er, Wu=Aur, Wy=Ayr, Wa=Aar, Wv=Avr,B=Br)\n",
    "                    \n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "all_3374 = np.load('../Data/npy/mask_feature/no_300image.npy')\n",
    "l = all_3374.shape[1]\n",
    "only_features = ['no_300image']\n",
    "try_count = [str(try_i) for try_i in range(10)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for only_feature in only_features:\n",
    "    all_3374 = np.load('../Data/npy/mask_feature/'+only_feature+'.npy')\n",
    "    l = all_3374.shape[1]\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print('Finished',finish_list)\n",
    "                    finish_list.append(only_feature)\n",
    "                    print(l)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=4))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=6)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=8))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                    ])\n",
    "                                \n",
    "                        \n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "                    \n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(only_feature+'9_E'+str(embedding_dims))\n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('../Data/npy/Image_50_2048D.npy')\n",
    "b = np.load('../Data/npy/Video_512D_6650.npy')\n",
    "#c = np.load('../Data/npy/social_Norm15D.npy')\n",
    "\n",
    "all_3374 = np.concatenate((a, b), axis=1)\n",
    "#all_3374=np.concatenate((all_3374, c), axis=1)\n",
    "l =all_3374.shape[1]\n",
    "print('all 3374 shape',all_3374.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finish list:[image+video,image+text,video+text,social_text,social_image,social_video, following_video, following_social,following_image,following_text]\n",
    "#finish list:[following_image_text,following_image_video,following_image_social,social_image_video,social_image_text]\n",
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "#Embedding_dims = [150,160,170,180,190,210,220,230,240,250]\n",
    "Embedding_dims = [200]\n",
    "single_features = ['Video','Text','Social','Following','Image']\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for paru_weight in par_weights:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for single_feature  in single_features:\n",
    "                    if 'Video' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/Video_512D_6650.npy')\n",
    "                    elif 'Text' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/Text_300D.npy')\n",
    "                    elif 'Social' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/social_15D.npy')\n",
    "                    elif 'Following' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/Following_64D.npy')\n",
    "                    elif 'Image' in single_feature:\n",
    "                        all_3374 = np.load('../Data/npy/Image_50_2048D.npy')\n",
    "                        \n",
    "                    l = all_3374.shape[1]\n",
    "                    \n",
    "                    embedding_dims = 200\n",
    "                    clear_output()\n",
    "                    print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    print('Now feature:',single_feature,l)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    \n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                paru_weight * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(single_feature+'1_Edims200')\n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('../Data/npy/Image_50_2048D.npy')\n",
    "b = np.load('../Data/npy/Video_512D_6650.npy')\n",
    "c = np.load('../Data/npy/Text_300D.npy')\n",
    "d = np.load('../Data/npy/social_Norm15D.npy')\n",
    "temp1 = np.concatenate((a, b), axis=1)\n",
    "temp2=np.concatenate((c,d), axis=1)\n",
    "all_3374 = np.concatenate((temp1,temp2), axis=1)\n",
    "\n",
    "l =all_3374.shape[1]\n",
    "#finish list:[image+video,image+text,video+text,social_text,social_image,social_video, following_video, following_social,following_image,following_text]\n",
    "#finish list:[following_image_text,following_image_video,following_image_social,social_image_video,social_image_text,text_image_video,]\n",
    "#following_text_video,following_text_social, following_video_social\n",
    "#following,image,text,video,social\n",
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "#Embedding_dims = [150,160,170,180,190,210,220,230,240,250]\n",
    "Embedding_dims = [200]\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for paru_weight in par_weights:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    \n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                paru_weight * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training('Image_Video_Text_Social_Edims200')\n",
    "                    print('Finish dims:,',embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(U, Y, A, E, A1, Au, Ay, Aa, Av,B):\n",
    "    #print(A1)\n",
    "    result=np.zeros((test_amount,88))\n",
    "    RS=np.zeros((test_amount,88))\n",
    "    #test_idx --> Test 的 index\n",
    "    max1 = 0\n",
    "    maxu = 0\n",
    "    maxy = 0\n",
    "    maxa = 0\n",
    "    maxv = 0\n",
    "    min1 = 100000000000000000\n",
    "    minu = 100000000000000000\n",
    "    miny = 100000000000000000\n",
    "    mina = 100000000000000000\n",
    "    minv = 100000000000000000\n",
    "    test_yes_id=[]\n",
    "    for s in range(test_amount):\n",
    "        print(s,test_idx[s])\n",
    "\n",
    "        yes=[]\n",
    "        sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "        #sample=result_yes_id[now]\n",
    "        test_yes_id.append(sample)\n",
    "        alpha=np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "            #print(test_idx[s])\n",
    "            #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "\n",
    "            #Observe each part in attention , below par are all (128,1)\n",
    "            testW1 = np.sum(A1[test_idx[s]])\n",
    "            #print(A1[test_idx[s]])\n",
    "            WuUu = np.sum(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T))\n",
    "            WyYy = np.sum(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T))\n",
    "            WaAa = np.sum(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T))\n",
    "            WvVy = np.sum(np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))\n",
    "            print('The sum of each par -->\\nw1:',testW1,'\\nWuU:',WuUu,'\\nwyY:',WyYy,'\\nWaA:',WaAa,'\\nWvV:',WvVy)\n",
    "            if testW1 > max1:\n",
    "                max1 = testW1\n",
    "            if testW1 < min1:\n",
    "                min1 = testW1\n",
    "            if WuUu > maxu:\n",
    "                maxu = WuUu\n",
    "            if WuUu < minu:\n",
    "                minu = WuUu\n",
    "            if WyYy > maxy:\n",
    "                maxy = WyYy\n",
    "            if WyYy < miny:\n",
    "                miny = WyYy\n",
    "            if WaAa > maxa:\n",
    "                maxa = WaAa\n",
    "            if WaAa < mina:\n",
    "                mina = WaAa\n",
    "            if WvVy > maxv:\n",
    "                mxv = WvVy\n",
    "            if WvVy < minv:\n",
    "                minv = WvVy\n",
    "            #Have w1\n",
    "            #alpha[a]=np.dot(A1[test_idx[s]],(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "            #                    np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "            #Without w1\n",
    "            alpha[a]=np.sum((relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                                np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "        mul=np.zeros((1,128))\n",
    "        print('alpha------------',alpha)\n",
    "        print('softmax alpha--------------',softmax(alpha))\n",
    "        for i in range(len(sample)):\n",
    "            mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "        new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "        for k in range(88):\n",
    "            result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]],np.dot(E, all_3374[k].T))\n",
    "    #print(max1,maxu,maxy,maxa,maxv)\n",
    "    #print(min1,minu,miny,mina,minv)\n",
    "    return RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "#從grid_search_weight中找尋不同的file \n",
    "path = '../Data/grid_search_weight/0105/'\n",
    "files = os.listdir(path)\n",
    "#U, Y, A, E, A1, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, W1, Wu, Wy, Wa, Wv,Beta])\n",
    "for file in files:\n",
    "    par_data = np.load(path+file)\n",
    "    U = par_data['U']\n",
    "    Y = par_data['Y']\n",
    "    A = par_data['A']\n",
    "    E = par_data['E']\n",
    "    W1 = par_data['W1']\n",
    "    Wu = par_data['Wu']\n",
    "    Wy = par_data['Wy']\n",
    "    Wa = par_data['Wa']\n",
    "    Wv = par_data['Wv']\n",
    "    B = par_data['B']\n",
    "    RS = testing(U, Y, A, E, W1, Wu, Wy, Wa, Wv,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Y, A, E, A1, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, W1, Wu, Wy, Wa, Wv,Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('W1 weight shape: ',A1.shape)\n",
    "print('Wu weight shape:',Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:',Aa.shape)\n",
    "print('Wv weight shape:',Av.shape)\n",
    "print('Beta shape:',B.shape)\n",
    "print('Embedding shape:',E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=np.zeros((test_amount,88))\n",
    "RS=np.zeros((test_amount,88))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id=[]\n",
    "for s in range(test_amount):\n",
    "    print(s,test_idx[s])\n",
    "\n",
    "    yes=[]\n",
    "    sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha=np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r =np.max(YouTuber_category[sample[a]]*user_category_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        #print(test_idx[s])\n",
    "        #print(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0)))\n",
    "        \n",
    "        #Observe each part in attention , below par are all (128,1)\n",
    "        testW1 = np.sum(np.multiply(A1[test_idx[s]],A1[test_idx[s]]))\n",
    "        WuUu = np.sum(np.multiply(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T),np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)))\n",
    "        WyYy = np.sum(np.multiply(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T),np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)))\n",
    "        WaAa = np.sum(np.multiply(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T),np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T)))\n",
    "        WvVy = np.sum(np.multiply(np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T),np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T)))\n",
    "        print('w1:',testW1,'\\nWuU:',WuUu,'\\nwyY:',WyYy,'\\nWaA:',WaAa,'\\nWvV:',WvVy)\n",
    "        \n",
    "        alpha[a]=np.dot(A1[test_idx[s]],(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                            np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(all_3374[sample[a]],0).T))))*r\n",
    "    mul=np.zeros((1,128))\n",
    "    print('alpha------------',alpha)\n",
    "    print('softmax alpha--------------',softmax(alpha))\n",
    "    for i in range(len(sample)):\n",
    "        mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    for k in range(88):\n",
    "        result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]],np.dot(E, all_3374[k].T))\n",
    "#print(RS[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取出test的資料\n",
    "testRS = np.zeros((test_amount,yt_test_amount)) #shape 150*20\n",
    "target = np.zeros((test_amount,yt_test_amount))\n",
    "#test_t 是true的\n",
    "#test_f 是false的\n",
    "        \n",
    "for z in range(test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    #positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    #not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "    #前兩個放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumtarget = 0\n",
    "for i in range(len(target)):\n",
    "    #print(np.sum(target[i]))\n",
    "    sumtarget += np.sum(target[i])\n",
    "print('num of positive data in testing:',sumtarget)\n",
    "print('total testing data:',test_amount*yt_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),int(np.sum(target[i])))\n",
    "    count_0_all.append(top_0)\n",
    "    print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = 0\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        #print(int(np.sum(target[i])))\n",
    "        total+=int(np.sum(target[i]))\n",
    "        if count_0_all[i][j] < int(np.sum(target[i])): #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "avg_acc = acc_0/100\n",
    "print('avg_accuarcy for count_0:',avg_acc)\n",
    "print(acc_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),1) #取一個\n",
    "    count_0_all.append(top_0)\n",
    "    print(top_0)\n",
    "    if top_0[0] < int(np.sum(target[i])):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_prec = correct/len(testRS)\n",
    "top1_recall = correct/(sumtarget)\n",
    "print('prec ',top1_prec,'recall ',top1_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top1_prec,top1_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_3 = topN(list(testRS[i]),3) #取一個\n",
    "    count_0_all.append(top_3)\n",
    "    #print(top_3)\n",
    "    for j in range(len(top_3)):\n",
    "        if top_3[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_prec = correct/(len(testRS)*3)\n",
    "top3_recall = correct/(sumtarget)\n",
    "print('prec ',top3_prec,'recall ',top3_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top3_prec,top3_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(testRS)):\n",
    "    top_5 = topN(list(testRS[i]),5) #取一個\n",
    "    count_0_all.append(top_5)\n",
    "    #print(top_5)\n",
    "    for j in range(len(top_5)):\n",
    "        if top_5[j] < int(np.sum(target[i])):\n",
    "            correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_prec = correct/(len(testRS)*5)\n",
    "top5_recall = correct/(sumtarget)\n",
    "print('prec ',top5_prec,'recall ',top5_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "print('F1_score:',F1_score(top5_prec,top5_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User_latent_factor = loaddata['User']\n",
    "#YouTuber_latent_factor = loaddata['YouTuber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('../Data/latent_factor/YRM_up10_ALL/Final/1226.npz', User=U, YouTuber=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax([-0.000000000000000000000000000000000000001,0.00000000000000000000000000000000000001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = tf.constant([1.1,2.7,6.1,4.7,5.5,6.5])\n",
    "B = tf.divide(C,tf.reduce_sum(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryy = np.array([1,2,3,4,5])\n",
    "np.add(tryy,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.prod((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
